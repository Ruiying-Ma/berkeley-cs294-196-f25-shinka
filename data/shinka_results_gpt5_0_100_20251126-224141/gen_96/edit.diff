--- a/original.py
+++ b/original.py
@@ -1,427 +1,396 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
+"""Cache eviction algorithm: SLRU with TinyLFU admission-style victim ranking and adaptive protected ratio"""
 
 from collections import OrderedDict
-import math
-
-# LRU timestamp map kept for tie-breaking and fallback
+
+# -------------------------------
+# Global metadata (SLRU + TinyLFU)
+# -------------------------------
+
+# Segmented LRU
+slru_S0 = OrderedDict()  # probationary (recency-tested)
+slru_S1 = OrderedDict()  # protected (frequently-hit)
+ghost_S0 = OrderedDict()  # ghost history for S0 evictions
+ghost_S1 = OrderedDict()  # ghost history for S1 evictions
+
+# Target size for protected segment (adapts online)
+target_S1 = 0
+slru_capacity = None
+
+# Timestamps for tie-breaking
 m_key_timestamp = dict()
 
-# Adaptive Replacement Cache (ARC) metadata
-arc_T1 = OrderedDict()  # recent, resident
-arc_T2 = OrderedDict()  # frequent, resident
-arc_B1 = OrderedDict()  # ghost of T1 (recent history)
-arc_B2 = OrderedDict()  # ghost of T2 (frequent history)
-arc_p = 0               # target size of T1
-arc_capacity = None     # initialized from cache_snapshot
-
-# Idle tracking and scan handling
-last_ghost_hit_access = -1  # last access_count when B1/B2 was hit
-cold_streak = 0             # consecutive brand-new inserts (no ghost)
-scan_guard_until = -1       # guard window end
-cold_extra_applied = False  # one-time extra clamp during cold scans
-guard_demote_once = False   # one-shot demotion bias flag
-# Per-access flag indicating whether p was already adjusted on a ghost reference
-p_adjusted_this_access = False
-
-# Track which list the eviction candidate was chosen from to ensure correct ghosting
-last_replaced_from = None   # 'T1' or 'T2'
+# Cold streak detection for scan guard/pollution control
+cold_miss_streak = 0  # consecutive brand-new inserts (not ghost hits)
+
+# Last eviction source segment (for robust ghosting)
+last_evicted_from_segment = None  # 'S0' or 'S1' or None
+
+# -------------------------------
+# TinyLFU Count-Min Sketch (CMS)
+# -------------------------------
+cms_width = 0
+cms_counts = []
+cms_hash_seeds = (0x9e3779b1, 0x7f4a7c15, 0x94d049bb, 0x27d4eb2d)  # distinct odd constants
+cms_sampled = 0
+cms_age_period = 0  # how often to age
+cms_aged_at = 0     # access count at last aging
 
 
 def _ensure_capacity(cache_snapshot):
-    global arc_capacity
-    if arc_capacity is None:
-        arc_capacity = max(int(cache_snapshot.capacity), 1)
+    global slru_capacity, target_S1, cms_width, cms_counts, cms_age_period
+    if slru_capacity is None:
+        # Treat capacity as number of objects (the framework uses unit-sized objects)
+        slru_capacity = max(int(cache_snapshot.capacity), 1)
+        # Initialize protected target to half
+        target_S1 = slru_capacity // 2
+        # Init TinyLFU CMS: width ~ 4x capacity, power of two for fast masking
+        width = 1
+        desired = max(64, 4 * slru_capacity)
+        while width < desired:
+            width <<= 1
+        cms_width = width
+        cms_counts = [0] * cms_width
+        cms_age_period = max(512, 8 * slru_capacity)  # periodic aging
+        # Reset ghosts
+        ghost_S0.clear()
+        ghost_S1.clear()
+
+
+def _hash_index(key_str, seed):
+    # Stable within run; mask to width
+    h = hash((key_str, seed))
+    if h < 0:
+        h = -h
+    return h & (cms_width - 1)
+
+
+def _cms_add(key_str, delta=1):
+    if cms_width == 0:
+        return
+    for s in cms_hash_seeds:
+        idx = _hash_index(key_str, s)
+        val = cms_counts[idx] + delta
+        # Avoid unbounded growth; clamp counters
+        cms_counts[idx] = val if val < 0xFFFF else 0xFFFF
+
+
+def _cms_estimate(key_str):
+    if cms_width == 0:
+        return 0
+    mn = None
+    for s in cms_hash_seeds:
+        idx = _hash_index(key_str, s)
+        v = cms_counts[idx]
+        mn = v if mn is None else (v if v < mn else mn)
+    return mn if mn is not None else 0
+
+
+def _cms_maybe_age(access_count):
+    global cms_aged_at
+    if cms_width == 0:
+        return
+    if access_count - cms_aged_at >= cms_age_period:
+        # Age the sketch by halving counters
+        for i in range(cms_width):
+            cms_counts[i] >>= 1
+        cms_aged_at = access_count
 
 
 def _move_to_mru(od, key):
-    # Push key to MRU position of an OrderedDict
     if key in od:
         od.pop(key, None)
     od[key] = True
 
 
 def _insert_at_lru(od, key):
-    # Insert key at LRU position (probation)
     if key in od:
         od.pop(key, None)
     od[key] = True
     try:
-        # Move to beginning (LRU side)
         od.move_to_end(key, last=False)
     except Exception:
         pass
 
 
 def _pop_lru(od):
     if od:
         k, _ = od.popitem(last=False)
         return k
     return None
 
 
-def _guard_window(C):
-    # Short, gentle guard window length
-    return min(8, max(1, C // 16))
+def _rebalance_segments():
+    # Keep protected segment near target by demoting from its LRU if too large.
+    # We do not force S0 size; eviction will primarily reduce S0.
+    while len(slru_S1) > target_S1:
+        k = _pop_lru(slru_S1)
+        if k is None:
+            break
+        _move_to_mru(slru_S0, k)
 
 
 def _trim_ghosts():
-    # Keep ghosts total size within capacity with p-aware hysteresis
-    total = len(arc_B1) + len(arc_B2)
-    C = arc_capacity if arc_capacity is not None else 1
-    target_B1 = min(C, max(0, arc_p))
-    target_B2 = max(0, C - target_B1)
-    h = max(1, C // 32)  # hysteresis to reduce oscillation
+    # Keep total ghosts bounded by capacity to maintain a meaningful history signal
+    total = len(ghost_S0) + len(ghost_S1)
+    C = slru_capacity if slru_capacity else 1
     while total > C:
-        over_B1 = len(arc_B1) - target_B1
-        over_B2 = len(arc_B2) - target_B2
-        if over_B1 > h and arc_B1:
-            _pop_lru(arc_B1)
-        elif over_B2 > h and arc_B2:
-            _pop_lru(arc_B2)
+        # Prefer trimming the larger ghost side
+        if len(ghost_S0) >= len(ghost_S1):
+            _pop_lru(ghost_S0)
         else:
-            # Otherwise trim from the larger side
-            if len(arc_B1) >= len(arc_B2):
-                _pop_lru(arc_B1)
-            else:
-                _pop_lru(arc_B2)
-        total = len(arc_B1) + len(arc_B2)
+            _pop_lru(ghost_S1)
+        total = len(ghost_S0) + len(ghost_S1)
 
 
 def _resync(cache_snapshot):
-    # Ensure resident metadata tracks actual cache content
+    # Synchronize SLRU sets with actual cache content
     cache_keys = set(cache_snapshot.cache.keys())
-    for k in list(arc_T1.keys()):
+    for k in list(slru_S0.keys()):
         if k not in cache_keys:
-            arc_T1.pop(k, None)
-    for k in list(arc_T2.keys()):
+            slru_S0.pop(k, None)
+    for k in list(slru_S1.keys()):
         if k not in cache_keys:
-            arc_T2.pop(k, None)
-    # Any cached keys not tracked: seed using ghost hints for better accuracy
+            slru_S1.pop(k, None)
+    # Any cached key not in our structures: place into S0 (probationary)
     for k in cache_keys:
-        if k not in arc_T1 and k not in arc_T2:
-            if k in arc_B2:
-                _move_to_mru(arc_T2, k)
-                arc_B2.pop(k, None)
-            elif k in arc_B1:
-                _move_to_mru(arc_T1, k)
-                arc_B1.pop(k, None)
-            else:
-                _move_to_mru(arc_T1, k)
-    # Keep ghosts disjoint from residents (robustness)
-    for k in list(arc_B1.keys()):
-        if k in arc_T1 or k in arc_T2:
-            arc_B1.pop(k, None)
-    for k in list(arc_B2.keys()):
-        if k in arc_T1 or k in arc_T2:
-            arc_B2.pop(k, None)
+        if k not in slru_S0 and k not in slru_S1:
+            _move_to_mru(slru_S0, k)
+    # Ensure ghosts don't contain resident keys
+    for k in list(ghost_S0.keys()):
+        if k in cache_keys:
+            ghost_S0.pop(k, None)
+    for k in list(ghost_S1.keys()):
+        if k in cache_keys:
+            ghost_S1.pop(k, None)
+    _rebalance_segments()
     _trim_ghosts()
 
 
-def _decay_p_if_idle(cache_snapshot):
-    # Proportional, bounded decay of p when no ghost hits for a while; plus one-shot cold clamp
-    global arc_p, cold_extra_applied
-    C = arc_capacity if arc_capacity else 1
-    if last_ghost_hit_access >= 0 and arc_p > 0:
-        idle = cache_snapshot.access_count - last_ghost_hit_access
-        if idle > 0:
-            cap_step = max(1, C // 8)
-            dyn_step = max(1, idle // max(1, C // 4))
-            step = min(cap_step, dyn_step)
-            arc_p = max(0, arc_p - step)
-    # One-time extra clamp during prolonged cold streaks (scan-like) to accelerate recovery
-    if cold_streak >= max(1, C // 2) and not cold_extra_applied:
-        extra = min(max(1, C // 4), max(1, cold_streak // max(1, C // 8)))
-        arc_p = max(0, arc_p - extra)
-        cold_extra_applied = True
+def _adjust_target_on_ghost(key):
+    # Adjust protected target based on which ghost list contains the key
+    global target_S1, cold_miss_streak
+    C = slru_capacity if slru_capacity else 1
+    if key in ghost_S0:
+        # We evicted from S0 before; recency-only miss suggests protected too small
+        step = max(1, len(ghost_S1) // max(1, len(ghost_S0)))
+        target_S1 = min(C, target_S1 + step)
+        # Ghost consumed
+        ghost_S0.pop(key, None)
+        cold_miss_streak = 0
+        return True
+    if key in ghost_S1:
+        # We evicted from S1 before; protected likely too large
+        step = max(1, len(ghost_S0) // max(1, len(ghost_S1)))
+        target_S1 = max(0, target_S1 - step)
+        ghost_S1.pop(key, None)
+        cold_miss_streak = 0
+        return True
+    return False
+
+
+def _select_victim(C):
+    # Prefer evicting from S0; if empty, fall back to S1.
+    # Use sampled TinyLFU over oldest candidates; tie-break by timestamp (older first).
+    kS0 = min(8, max(1, C // 16))
+    kS1 = min(2, max(1, C // 32))
+    candidates = []
+
+    # Collect S0 candidates (oldest first)
+    cnt = 0
+    for k in slru_S0.keys():
+        candidates.append((k, 'S0'))
+        cnt += 1
+        if cnt >= kS0:
+            break
+
+    # If S0 empty, allow a few S1 candidates
+    if not candidates:
+        cnt = 0
+        for k in slru_S1.keys():
+            candidates.append((k, 'S1'))
+            cnt += 1
+            if cnt >= max(kS0, kS1):
+                break
+    else:
+        # Also consider a tiny set from S1 in case of severely cold S0
+        cnt = 0
+        for k in slru_S1.keys():
+            candidates.append((k, 'S1'))
+            cnt += 1
+            if cnt >= kS1:
+                break
+
+    # Score: frequency; prefer S0 on ties; break ties by oldest timestamp
+    best = None
+    best_score = None
+    best_ts = None
+    for k, seg in candidates:
+        freq = _cms_estimate(k)
+        # Penalize evicting protected segment slightly to prefer S0 unless significantly colder
+        if seg == 'S1':
+            freq += 1  # small bias
+        ts = m_key_timestamp.get(k, 0)
+        if best is None or freq < best_score or (freq == best_score and (seg == 'S0' and best[1] == 'S1')) or (freq == best_score and seg == best[1] and ts < best_ts):
+            best = (k, seg)
+            best_score = freq
+            best_ts = ts
+
+    return best  # (key, segment) or None
 
 
 def evict(cache_snapshot, obj):
     '''
     Choose the eviction victim.
     - Return: candid_obj_key
     '''
-    global arc_p, last_ghost_hit_access, cold_streak, scan_guard_until, cold_extra_applied, last_replaced_from, guard_demote_once, p_adjusted_this_access
+    global last_evicted_from_segment
     _ensure_capacity(cache_snapshot)
     _resync(cache_snapshot)
-    _decay_p_if_idle(cache_snapshot)
-    p_adjusted_this_access = False
+    _cms_maybe_age(cache_snapshot.access_count)
+
+    C = slru_capacity if slru_capacity else 1
+
+    # Choose victim using sampled TinyLFU preferences
+    choice = _select_victim(C)
+    if choice is None:
+        # Fallback: any cached key (should not happen often)
+        if cache_snapshot.cache:
+            k = next(iter(cache_snapshot.cache.keys()))
+            last_evicted_from_segment = 'S0' if k in slru_S0 else ('S1' if k in slru_S1 else None)
+            return k
+        return None
+
+    k, seg = choice
+    last_evicted_from_segment = seg
+    return k
+
+
+def update_after_hit(cache_snapshot, obj):
+    '''
+    Update metadata immediately after a cache hit.
+    '''
+    _ensure_capacity(cache_snapshot)
+    _cms_maybe_age(cache_snapshot.access_count)
 
     key = obj.key
-    C = arc_capacity if arc_capacity else 1
-    in_B1 = key in arc_B1
-    in_B2 = key in arc_B2
-
-    # Canonical ghost-driven p updates BEFORE REPLACE (ARC)
-    if in_B1:
-        # step_up = ceil(|B2|/|B1|); clamp by C//8
-        denom = max(1, len(arc_B1))
-        step_up = (len(arc_B2) + denom - 1) // denom
-        arc_p = min(C, arc_p + min(step_up, max(1, C // 8)))
-        last_ghost_hit_access = cache_snapshot.access_count
-        cold_streak = 0
-        scan_guard_until = -1
-        guard_demote_once = False
-        cold_extra_applied = False
-        p_adjusted_this_access = True
-    elif in_B2:
-        # step_down = ceil(|B1|/|B2|); clamp by C//8 (or C//4 under prolonged cold streaks)
-        denom = max(1, len(arc_B2))
-        step_down = (len(arc_B1) + denom - 1) // denom
-        dec_cap = max(1, (C // 4) if cold_streak >= max(1, C // 2) else (C // 8))
-        arc_p = max(0, arc_p - min(step_down, dec_cap))
-        last_ghost_hit_access = cache_snapshot.access_count
-        cold_streak = 0
-        scan_guard_until = -1
-        guard_demote_once = False
-        cold_extra_applied = False
-        p_adjusted_this_access = True
+
+    # Record frequency and timestamp
+    _cms_add(key, 1)
+    m_key_timestamp[key] = cache_snapshot.access_count
+
+    # Promotion/refresh
+    if key in slru_S0:
+        # Promote to protected MRU
+        slru_S0.pop(key, None)
+        _move_to_mru(slru_S1, key)
     else:
-        # Brand-new: do NOT change p here; optionally open a short guard window on long cold streaks
-        if cold_streak >= max(1, C // 8):
-            scan_guard_until = max(scan_guard_until, cache_snapshot.access_count + _guard_window(C))
-
-    # ARC REPLACE with guard-adjusted effective p
-    t1_sz = len(arc_T1)
-    guard_active = (scan_guard_until != -1 and cache_snapshot.access_count < scan_guard_until)
-    # Gentle effective_p drop under guard
-    drop_cap = max(1, C // 16)
-    extra = 0
-    if guard_active:
-        extra = min(drop_cap, 1 + max(0, (cold_streak - max(1, C // 8))) // max(1, C // 16))
-    p_eff = max(0, arc_p - extra)
-    # One-shot demotion bias when scans likely and no freq history (B2 empty)
-    if guard_active and len(arc_B2) == 0 and len(arc_T2) > len(arc_T1) and not guard_demote_once:
-        p_eff = 0
-        guard_demote_once = True
-
-    candidate = None
-    last_replaced_from = None
-    if t1_sz >= 1 and (t1_sz > p_eff or (in_B2 and t1_sz == p_eff)):
-        # Evict LRU from T1
-        candidate = next(iter(arc_T1)) if arc_T1 else None
-        if candidate is not None:
-            last_replaced_from = 'T1'
+        # Refresh in protected; if absent due to drift, insert to protected
+        _move_to_mru(slru_S1, key)
+
+    # Recent hits imply frequency; gently bias towards larger protected segment
+    global target_S1, cold_miss_streak
+    target_S1 = min(slru_capacity, target_S1 + 1)
+    cold_miss_streak = 0
+
+    # Keep segments balanced
+    _rebalance_segments()
+
+
+def update_after_insert(cache_snapshot, obj):
+    '''
+    Update metadata immediately after inserting a new object into the cache.
+    '''
+    _ensure_capacity(cache_snapshot)
+    _cms_maybe_age(cache_snapshot.access_count)
+
+    key = obj.key
+    C = slru_capacity if slru_capacity else 1
+
+    # Frequency and timestamp for the accessed key (miss)
+    _cms_add(key, 1)
+    m_key_timestamp[key] = cache_snapshot.access_count
+
+    ghost_hit = _adjust_target_on_ghost(key)
+
+    # Insert new key into S0 normally; on ghost_S1 hit, allow direct placement to S1
+    global cold_miss_streak
+    if ghost_hit and key not in slru_S0 and key not in slru_S1:
+        # If the key was in protected ghost, it likely deserves protected insertion
+        # Otherwise, it goes to probationary
+        if key in slru_S0 or key in slru_S1:
+            pass  # already handled
+        if key not in ghost_S0 and key not in ghost_S1:
+            # No longer in ghost after _adjust_target_on_ghost
+            pass
+        # Heuristic: if S1 is not over target, place directly into S1
+        if len(slru_S1) < max(1, target_S1):
+            _move_to_mru(slru_S1, key)
+        else:
+            _move_to_mru(slru_S0, key)
+        cold_miss_streak = 0
     else:
-        # Evict LRU from T2
-        candidate = next(iter(arc_T2)) if arc_T2 else None
-        if candidate is not None:
-            last_replaced_from = 'T2'
-
-    # Deterministic, depth-limited fallbacks with ghost hints
-    if candidate is None:
-        # Try to avoid removing B2-hinted keys from T1
-        for k in list(arc_T1.keys()):
-            if k not in arc_B2:
-                candidate = k
-                last_replaced_from = 'T1'
-                break
-    if candidate is None:
-        # Prefer T2 keys that appear in B1 (recency-only hint)
-        for k in list(arc_T2.keys()):
-            if k in arc_B1:
-                candidate = k
-                last_replaced_from = 'T2'
-                break
-    if candidate is None:
-        # Depth-limited peek
-        budget = min(8, max(1, C // 16))
-        cnt = 0
-        for k in arc_T1.keys():
-            if k not in arc_B2:
-                candidate = k
-                last_replaced_from = 'T1'
-                break
-            cnt += 1
-            if cnt >= budget:
-                break
-        if candidate is None:
-            cnt = 0
-            for k in arc_T2.keys():
-                if k in arc_B1:
-                    candidate = k
-                    last_replaced_from = 'T2'
-                    break
-                cnt += 1
-                if cnt >= budget:
-                    break
-    if candidate is None:
-        # Timestamp tie-breaker restricted to T1 keys first
-        min_ts = float('inf')
-        min_k = None
-        for k in arc_T1.keys():
-            ts = m_key_timestamp.get(k, float('inf'))
-            if ts < min_ts:
-                min_ts = ts
-                min_k = k
-        if min_k is not None:
-            candidate = min_k
-            last_replaced_from = 'T1'
-    if candidate is None and m_key_timestamp:
-        # Fallback timestamp across all cached keys
-        min_ts = float('inf')
-        min_k = None
-        for k in cache_snapshot.cache.keys():
-            ts = m_key_timestamp.get(k, float('inf'))
-            if ts < min_ts:
-                min_ts = ts
-                min_k = k
-        candidate = min_k
-        # Set source if we can infer it
-        if candidate in arc_T1:
-            last_replaced_from = 'T1'
-        elif candidate in arc_T2:
-            last_replaced_from = 'T2'
-    if candidate is None and cache_snapshot.cache:
-        # Last resort: arbitrary
-        candidate = next(iter(cache_snapshot.cache.keys()))
-        if candidate in arc_T1:
-            last_replaced_from = 'T1'
-        elif candidate in arc_T2:
-            last_replaced_from = 'T2'
-    return candidate
-
-
-def update_after_hit(cache_snapshot, obj):
-    '''
-    Update metadata immediately after a cache hit.
-    '''
-    global m_key_timestamp, cold_streak, scan_guard_until, guard_demote_once, p_adjusted_this_access
-    _ensure_capacity(cache_snapshot)
-    _decay_p_if_idle(cache_snapshot)
-
-    # ARC: on hit, move to T2 MRU
-    key = obj.key
-    if key in arc_T1:
-        arc_T1.pop(key, None)
-        _move_to_mru(arc_T2, key)
+        # Brand-new miss: insert into S0; if many consecutive brand-new misses, insert at LRU to reduce pollution
+        cold_miss_streak += 1
+        guard_threshold = max(2, C // 4)
+        if cold_miss_streak >= guard_threshold:
+            _insert_at_lru(slru_S0, key)
+            # During cold scans, bias target_S1 downward a bit
+            global target_S1
+            target_S1 = max(0, target_S1 - 1)
+        else:
+            _move_to_mru(slru_S0, key)
+
+    _rebalance_segments()
+    _trim_ghosts()
+
+
+def update_after_evict(cache_snapshot, obj, evicted_obj):
+    '''
+    Update metadata immediately after evicting the victim.
+    '''
+    k = evicted_obj.key
+
+    # Remove from segments and send to matching ghost
+    if k in slru_S0:
+        slru_S0.pop(k, None)
+        _move_to_mru(ghost_S0, k)
+        ghost_S1.pop(k, None)
+    elif k in slru_S1:
+        slru_S1.pop(k, None)
+        _move_to_mru(ghost_S1, k)
+        ghost_S0.pop(k, None)
     else:
-        # If already in T2, refresh; if not present due to drift, place in T2
-        _move_to_mru(arc_T2, key)
-
-    # Resident keys must not exist in ghosts
-    arc_B1.pop(key, None)
-    arc_B2.pop(key, None)
-
-    # Any hit breaks a cold streak and cancels scan guard and one-shot bias
-    cold_streak = 0
-    scan_guard_until = -1
-    guard_demote_once = False
-
-    # Update timestamp for tie-breaking/fallback
-    m_key_timestamp[key] = cache_snapshot.access_count
-    # Reset per-access p-adjustment flag
-    p_adjusted_this_access = False
-
-
-def update_after_insert(cache_snapshot, obj):
-    '''
-    Update metadata immediately after inserting a new object into the cache.
-    '''
-    global m_key_timestamp, cold_streak, scan_guard_until, guard_demote_once, last_ghost_hit_access, cold_extra_applied, arc_p, p_adjusted_this_access
-    _ensure_capacity(cache_snapshot)
-    _decay_p_if_idle(cache_snapshot)
-
-    key = obj.key
-    C = arc_capacity if arc_capacity else 1
-    guard_active = (scan_guard_until != -1 and cache_snapshot.access_count < scan_guard_until)
-
-    # ARC admission policy: ghost hits go to T2 (p already adjusted in evict)
-    if key in arc_B1 or key in arc_B2:
-        # Canonical ghost-driven p updates if evict didn't do it
-        if not p_adjusted_this_access:
-            if key in arc_B1:
-                denom = max(1, len(arc_B1))
-                step_up = (len(arc_B2) + denom - 1) // denom
-                arc_p = min(C, arc_p + min(step_up, max(1, C // 8)))
-            else:
-                denom = max(1, len(arc_B2))
-                step_down = (len(arc_B1) + denom - 1) // denom
-                dec_cap = max(1, (C // 4) if cold_streak >= max(1, C // 2) else (C // 8))
-                arc_p = max(0, arc_p - min(step_down, dec_cap))
-            last_ghost_hit_access = cache_snapshot.access_count
-            guard_demote_once = False
-            cold_extra_applied = False
-        cold_streak = 0
-        scan_guard_until = -1
-        # keep ghosts disjoint
-        arc_B1.pop(key, None)
-        arc_B2.pop(key, None)
-        _move_to_mru(arc_T2, key)
-    else:
-        # Brand new: insert into T1; during guard, insert at LRU to reduce pollution
-        cold_streak += 1
-        if guard_active:
-            _insert_at_lru(arc_T1, key)
-        else:
-            _move_to_mru(arc_T1, key)
-        # If long cold streak and no active guard, open a short guard window
-        if cold_streak >= max(1, C // 8) and not guard_active:
-            scan_guard_until = max(scan_guard_until, cache_snapshot.access_count + _guard_window(C))
-        # Ensure ghosts are disjoint from residents
-        arc_B1.pop(key, None)
-        arc_B2.pop(key, None)
-
+        # If not tracked, assume it was probationary
+        _move_to_mru(ghost_S0, k)
+        ghost_S1.pop(k, None)
+
+    # Cleanup timestamp for evicted key
+    m_key_timestamp.pop(k, None)
+
+    # Keep ghost history in check
     _trim_ghosts()
-    m_key_timestamp[key] = cache_snapshot.access_count
-    # Reset per-access p-adjustment flag
-    p_adjusted_this_access = False
-
-
-def update_after_evict(cache_snapshot, obj, evicted_obj):
-    '''
-    Update metadata immediately after evicting the victim.
-    '''
-    global m_key_timestamp, last_replaced_from
-    _ensure_capacity(cache_snapshot)
-    k = evicted_obj.key
-
-    # Place evicted resident into corresponding ghost list using remembered source
-    if last_replaced_from == 'T1':
-        arc_T1.pop(k, None)
-        _move_to_mru(arc_B1, k)
-        arc_B2.pop(k, None)
-    elif last_replaced_from == 'T2':
-        arc_T2.pop(k, None)
-        _move_to_mru(arc_B2, k)
-        arc_B1.pop(k, None)
-    else:
-        # Fallback by checking membership (robustness)
-        if k in arc_T1:
-            arc_T1.pop(k, None)
-            _move_to_mru(arc_B1, k)
-            arc_B2.pop(k, None)
-        elif k in arc_T2:
-            arc_T2.pop(k, None)
-            _move_to_mru(arc_B2, k)
-            arc_B1.pop(k, None)
-        else:
-            _move_to_mru(arc_B1, k)
-            arc_B2.pop(k, None)
-
-    # Clean up
-    last_replaced_from = None
-    m_key_timestamp.pop(k, None)
-    _trim_ghosts()
+    # Rebalance after eviction
+    _rebalance_segments()
 
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate