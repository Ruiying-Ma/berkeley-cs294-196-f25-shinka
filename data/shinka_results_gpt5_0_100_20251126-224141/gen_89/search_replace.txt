<NAME>
arc_2q_pend_lfu_guard
</NAME>

<DESCRIPTION>
I propose three complementary improvements to reduce miss rate across mixed workloads:

1) Delayed promotion (2Q-style) from T1 to T2 using a small pending window:
   - Instead of immediately promoting a T1 hit to T2, we first set a "pending" marker and require a second hit within a short window to promote. This avoids polluting T2 with one-timers and improves scan resistance while still promoting genuinely reusable items quickly. I implement this in update_after_hit with a tunable window _pending_window() reduced to a small, dynamic bound.

2) More stable ARC adaptation and better scan handling:
   - Use ceil-style updates for ARC’s p on ghost hits to align with canonical ARC behavior and make adaptation more responsive without being over-aggressive.
   - Refine the scan guard to a shorter window and reset it on the first reuse, reducing overreaction.
   - Add a one-eviction demotion bias during suspected scans when there’s no frequency signal (B2 empty) and T2 > T1 to prevent T2 from ballooning on uniform/scan-like traces.

3) Lightweight LFU carryover with cleanup:
   - Stop dropping the per-key tiny frequency on eviction; instead, decay it by one and keep it while the key resides in ghosts. This helps immediate re-admissions make better choices.
   - To prevent unbounded growth, tie ghost trimming to frequency cleanup by removing frequency entries for keys trimmed from B1/B2.

Additionally, the victim selection already samples a few LRU candidates and uses a recency-aged frequency score; the above changes make that signal more robust across phase shifts. These adjustments preserve correctness, keep logic simple, and target improvements where prior metrics showed vulnerability (scan-like traces and over-promotion to T2).

</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _pending_window():
    cap = arc_capacity if arc_capacity is not None else 1
    return max(1, cap // 4)
=======
def _pending_window():
    cap = arc_capacity if arc_capacity is not None else 1
    # Use a short dynamic window to require quick re-reference before promotion
    return min(8, max(1, cap // 8))
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def _trim_ghosts():
    # Keep ghosts total size within 2x capacity with proportional trimming to p
    cap_base = (arc_capacity if arc_capacity is not None else 1)
    total_cap = cap_base * 2
    # Targets proportional to current p (approximately 2*p and 2C-2*p)
    target_B1 = min(total_cap, max(0, int(total_cap * (float(arc_p) / max(1, cap_base)))))
    target_B2 = max(0, total_cap - target_B1)

    def _over_target():
        return (len(arc_B1) + len(arc_B2)) - total_cap

    # Trim until under the total budget, favoring lists above their targets
    while (len(arc_B1) + len(arc_B2)) > total_cap:
        if len(arc_B1) > target_B1:
            _pop_lru(arc_B1)
        elif len(arc_B2) > target_B2:
            _pop_lru(arc_B2)
        else:
            # If neither is over its proportional target, drop from the larger one
            if len(arc_B1) >= len(arc_B2):
                _pop_lru(arc_B1)
            else:
                _pop_lru(arc_B2)
=======
def _trim_ghosts():
    global m_key_freq
    # Keep ghosts total size within 2x capacity with proportional trimming to p
    cap_base = (arc_capacity if arc_capacity is not None else 1)
    total_cap = cap_base * 2
    # Targets proportional to current p (approximately 2*p and 2C-2*p)
    target_B1 = min(total_cap, max(0, int(total_cap * (float(arc_p) / max(1, cap_base)))))
    target_B2 = max(0, total_cap - target_B1)

    def _over_target():
        return (len(arc_B1) + len(arc_B2)) - total_cap

    # Trim until under the total budget, favoring lists above their targets
    while (len(arc_B1) + len(arc_B2)) > total_cap:
        if len(arc_B1) > target_B1:
            k = _pop_lru(arc_B1)
            if k is not None:
                m_key_freq.pop(k, None)
        elif len(arc_B2) > target_B2:
            k = _pop_lru(arc_B2)
            if k is not None:
                m_key_freq.pop(k, None)
        else:
            # If neither is over its proportional target, drop from the larger one
            if len(arc_B1) >= len(arc_B2):
                k = _pop_lru(arc_B1)
            else:
                k = _pop_lru(arc_B2)
            if k is not None:
                m_key_freq.pop(k, None)
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_capacity(cache_snapshot)
    _resync(cache_snapshot)
    # Apply ghost-driven adaptation before replacement (canonical ARC behavior)
    global arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until
    now = cache_snapshot.access_count
    step_cap = max(1, (arc_capacity if arc_capacity is not None else 1) // 8)
    if obj.key in arc_B1:
        # Favor recency: increase p
        ratio = max(1, len(arc_B2) // max(1, len(arc_B1)))
        inc = min(ratio, step_cap, max(0, (arc_capacity if arc_capacity is not None else 0) - arc_p))
        arc_p = min((arc_capacity if arc_capacity is not None else arc_p), arc_p + inc)
        arc_last_ghost_hit_access = now
        cold_streak = 0
    elif obj.key in arc_B2:
        # Favor frequency: decrease p
        ratio = max(1, len(arc_B1) // max(1, len(arc_B2)))
        dec = min(ratio, step_cap, arc_p)
        arc_p = max(0, arc_p - dec)
        arc_last_ghost_hit_access = now
        cold_streak = 0

    # ARC replacement: choose between T1 and T2 depending on p and whether obj is in B2
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)
    choose_from_T1 = False
    if t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)):
        choose_from_T1 = True

    # Scan-guard bias: during suspected scans, prefer evicting from T1 if possible
    if now <= scan_guard_until and t1_sz > 0:
        choose_from_T1 = True

    # Depth-limited, frequency- and age-informed victim selection within chosen list
    victim = None
    depth = min(8, max(1, (arc_capacity if arc_capacity is not None else 1) // 16))
    if choose_from_T1:
        if arc_T1:
            victim = _pick_victim_from_od(arc_T1, now, depth)
    else:
        if arc_T2:
            victim = _pick_victim_from_od(arc_T2, now, depth)

    # Try the other list with sampling if the chosen one is empty
    if victim is None:
        if choose_from_T1 and arc_T2:
            victim = _pick_victim_from_od(arc_T2, now, depth)
        elif (not choose_from_T1) and arc_T1:
            victim = _pick_victim_from_od(arc_T1, now, depth)

    # Fallbacks: if still none, use classic ARC fallbacks
    if victim is None:
        if t1_sz > 0 and arc_T1:
            victim = next(iter(arc_T1))
        elif len(arc_T2) > 0 and arc_T2:
            victim = next(iter(arc_T2))
        else:
            # Rare drift: resync once and retry small bounded scan, then deterministic fallback
            _resync(cache_snapshot)
            scan_lim = max(1, (arc_capacity if arc_capacity is not None else 1) // 16)
            if arc_T1:
                i = 0
                for k in arc_T1.keys():
                    victim = k
                    i += 1
                    if i >= scan_lim:
                        break
            elif arc_T2:
                i = 0
                for k in arc_T2.keys():
                    victim = k
                    i += 1
                    if i >= scan_lim:
                        break
            elif cache_snapshot.cache:
                # Deterministic final fallback: first key iteration
                victim = next(iter(cache_snapshot.cache.keys()))
    return victim
=======
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_capacity(cache_snapshot)
    _resync(cache_snapshot)
    # Apply ghost-driven adaptation before replacement (canonical ARC behavior)
    global arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until
    now = cache_snapshot.access_count
    cap = (arc_capacity if arc_capacity is not None else 0)
    step_cap = max(1, (arc_capacity if arc_capacity is not None else 1) // 8)
    if obj.key in arc_B1:
        # Favor recency: increase p (use ceil-style ratio)
        b1 = len(arc_B1)
        b2 = len(arc_B2)
        ratio = max(1, (b2 + max(1, b1) - 1) // max(1, b1))
        inc = min(ratio, step_cap, max(0, cap - arc_p))
        arc_p = min(cap if cap else arc_p, arc_p + inc)
        arc_last_ghost_hit_access = now
        cold_streak = 0
    elif obj.key in arc_B2:
        # Favor frequency: decrease p (use ceil-style ratio)
        b1 = len(arc_B1)
        b2 = len(arc_B2)
        ratio = max(1, (b1 + max(1, b2) - 1) // max(1, b2))
        dec = min(ratio, step_cap, arc_p)
        arc_p = max(0, arc_p - dec)
        arc_last_ghost_hit_access = now
        cold_streak = 0

    # ARC replacement: choose between T1 and T2 depending on p and whether obj is in B2
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)
    choose_from_T1 = False
    if t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)):
        choose_from_T1 = True

    # Scan-guard bias: during suspected scans, prefer evicting from T1 if possible
    if now <= scan_guard_until and t1_sz > 0:
        choose_from_T1 = True
        # But when there is no frequency signal (B2 empty) and T2 already larger, prefer one demotion from T2
        if len(arc_B2) == 0 and len(arc_T2) > len(arc_T1):
            choose_from_T1 = False

    # Depth-limited, frequency- and age-informed victim selection within chosen list
    victim = None
    depth = min(8, max(1, (arc_capacity if arc_capacity is not None else 1) // 16))
    if choose_from_T1:
        if arc_T1:
            victim = _pick_victim_from_od(arc_T1, now, depth)
    else:
        if arc_T2:
            victim = _pick_victim_from_od(arc_T2, now, depth)

    # Try the other list with sampling if the chosen one is empty
    if victim is None:
        if choose_from_T1 and arc_T2:
            victim = _pick_victim_from_od(arc_T2, now, depth)
        elif (not choose_from_T1) and arc_T1:
            victim = _pick_victim_from_od(arc_T1, now, depth)

    # Fallbacks: if still none, use classic ARC fallbacks
    if victim is None:
        if t1_sz > 0 and arc_T1:
            victim = next(iter(arc_T1))
        elif len(arc_T2) > 0 and arc_T2:
            victim = next(iter(arc_T2))
        else:
            # Rare drift: resync once and retry small bounded scan, then deterministic fallback
            _resync(cache_snapshot)
            scan_lim = max(1, (arc_capacity if arc_capacity is not None else 1) // 16)
            if arc_T1:
                i = 0
                for k in arc_T1.keys():
                    victim = k
                    i += 1
                    if i >= scan_lim:
                        break
            elif arc_T2:
                i = 0
                for k in arc_T2.keys():
                    victim = k
                    i += 1
                    if i >= scan_lim:
                        break
            elif cache_snapshot.cache:
                # Deterministic final fallback: first key iteration
                victim = next(iter(cache_snapshot.cache.keys()))
    return victim
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, cold_streak, m_key_freq
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    # Any hit breaks cold streaks
    cold_streak = 0

    # Keep resident metadata consistent with actual cache
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    key = obj.key
    if key in arc_T1:
        # Canonical ARC: on a hit in T1, move to T2 (become frequent)
        arc_T1.pop(key, None)
        _move_to_mru(arc_T2, key)
        t1_pending.pop(key, None)
    elif key in arc_T2:
        # Refresh recency within T2
        _move_to_mru(arc_T2, key)
    else:
        # Metadata drift: conservatively place into T1 as recent
        _move_to_mru(arc_T1, key)

    # Increment lightweight frequency counter (saturate)
    m_key_freq[key] = min(7, m_key_freq.get(key, 0) + 1)

    # Maintain disjointness: resident keys must not appear in ghosts
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)

    _trim_ghosts()
    # Update timestamp for tie-breaking/fallback
    m_key_timestamp[key] = now
=======
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, cold_streak, m_key_freq, scan_guard_until
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    # Any hit breaks cold streaks and cancels scan guard
    cold_streak = 0
    scan_guard_until = 0

    # Keep resident metadata consistent with actual cache
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    key = obj.key
    if key in arc_T1:
        # Delayed promotion: require a quick second hit before moving to T2
        last = t1_pending.get(key)
        if last is not None and (now - last) <= _pending_window():
            arc_T1.pop(key, None)
            _move_to_mru(arc_T2, key)
            t1_pending.pop(key, None)
        else:
            t1_pending[key] = now
            _move_to_mru(arc_T1, key)
    elif key in arc_T2:
        # Refresh recency within T2
        _move_to_mru(arc_T2, key)
    else:
        # Metadata drift: conservatively place into T1 as recent and set pending
        _move_to_mru(arc_T1, key)
        t1_pending[key] = now

    # Increment lightweight frequency counter (saturate)
    m_key_freq[key] = min(7, m_key_freq.get(key, 0) + 1)

    # Maintain disjointness: resident keys must not appear in ghosts
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)

    _trim_ghosts()
    # Update timestamp for tie-breaking/fallback
    m_key_timestamp[key] = now
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until, m_key_freq
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    key = obj.key

    # Keep resident metadata consistent with actual cache
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    # ARC admission: p was already adapted in evict on ghost hits. Place accordingly.
    if key in arc_B1 or key in arc_B2:
        # On a ghost hit, item becomes frequent
        _move_to_mru(arc_T2, key)
        arc_last_ghost_hit_access = now
        cold_streak = 0
        # Initialize/boost frequency on ghost re-admission
        m_key_freq[key] = max(m_key_freq.get(key, 0), 2)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
        cold_streak += 1
        # Initialize frequency low for brand new items
        m_key_freq[key] = max(m_key_freq.get(key, 0), 1)
        # Streaming-aware behavior: early guard and progressive clamp
        if arc_capacity is not None:
            cap = arc_capacity
            # Bias REPLACE to draw from T1 for a short window during scans
            if cold_streak >= max(1, cap // 2):
                scan_guard_until = now + max(1, cap // 8)
            if cold_streak >= cap:
                dec = max(1, cap // 8)
                arc_p = max(0, arc_p - dec)
                # Keep pending hints conservative after clamp
                t1_pending.clear()
                # Prevent unbounded growth of the streak counter to keep adaptation responsive
                if cold_streak >= 2 * cap:
                    arc_p = max(0, arc_p - dec)
                    cold_streak = cap

    # Maintain disjointness: resident keys must not appear in ghosts
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)

    _trim_ghosts()
    m_key_timestamp[key] = now
=======
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until, m_key_freq
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    key = obj.key

    # Keep resident metadata consistent with actual cache
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    # ARC admission: p was already adapted in evict on ghost hits. Place accordingly.
    if key in arc_B1 or key in arc_B2:
        # On a ghost hit, item becomes frequent
        _move_to_mru(arc_T2, key)
        arc_last_ghost_hit_access = now
        cold_streak = 0
        # Initialize/boost frequency on ghost re-admission
        m_key_freq[key] = max(m_key_freq.get(key, 0), 2)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
        cold_streak += 1
        # Initialize frequency low for brand new items
        m_key_freq[key] = max(m_key_freq.get(key, 0), 1)
        # Streaming-aware behavior: early guard and progressive clamp
        if arc_capacity is not None:
            cap = arc_capacity
            # Bias REPLACE to draw from T1 briefly during suspected scans
            if cold_streak >= max(1, cap // 3):
                scan_guard_until = now + min(8, max(1, cap // 16))
            if cold_streak >= cap:
                dec = max(1, cap // 8)
                arc_p = max(0, arc_p - dec)
                # Keep pending hints conservative after clamp
                t1_pending.clear()
                # Prevent unbounded growth of the streak counter to keep adaptation responsive
                if cold_streak >= 2 * cap:
                    arc_p = max(0, arc_p - dec)
                    cold_streak = cap

    # Maintain disjointness: resident keys must not appear in ghosts
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)

    _trim_ghosts()
    m_key_timestamp[key] = now
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_freq
    _ensure_capacity(cache_snapshot)
    k = evicted_obj.key
    # Move evicted resident to corresponding ghost list, keeping ghosts disjoint
    if k in arc_T1:
        arc_T1.pop(k, None)
        _move_to_mru(arc_B1, k)
        arc_B2.pop(k, None)
    elif k in arc_T2:
        arc_T2.pop(k, None)
        _move_to_mru(arc_B2, k)
        arc_B1.pop(k, None)
    else:
        # Unknown membership: prefer B2 if it already exists there, otherwise B1
        if k in arc_B2:
            _move_to_mru(arc_B2, k)
            arc_B1.pop(k, None)
        else:
            _move_to_mru(arc_B1, k)
            arc_B2.pop(k, None)
    # Clean up metadata for evicted item
    m_key_timestamp.pop(k, None)
    m_key_freq.pop(k, None)
    t1_pending.pop(k, None)
    _trim_ghosts()
=======
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_freq
    _ensure_capacity(cache_snapshot)
    k = evicted_obj.key
    # Move evicted resident to corresponding ghost list, keeping ghosts disjoint
    if k in arc_T1:
        arc_T1.pop(k, None)
        _move_to_mru(arc_B1, k)
        arc_B2.pop(k, None)
    elif k in arc_T2:
        arc_T2.pop(k, None)
        _move_to_mru(arc_B2, k)
        arc_B1.pop(k, None)
    else:
        # Unknown membership: prefer B2 if it already exists there, otherwise B1
        if k in arc_B2:
            _move_to_mru(arc_B2, k)
            arc_B1.pop(k, None)
        else:
            _move_to_mru(arc_B1, k)
            arc_B2.pop(k, None)
    # Clean up metadata for evicted item
    m_key_timestamp.pop(k, None)
    # Keep a small decayed frequency to aid re-admission; cleanup happens on ghost trim
    m_key_freq[k] = max(0, m_key_freq.get(k, 0) - 1)
    t1_pending.pop(k, None)
    _trim_ghosts()
>>>>>>> REPLACE

</DIFF>