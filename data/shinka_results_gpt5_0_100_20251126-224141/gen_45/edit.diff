--- a/original.py
+++ b/original.py
@@ -1,349 +1,369 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm for optimizing hit rates across multiple workloads
-Simplified ARC with enhancements:
-- Resident sets: T1 (recent), T2 (frequent).
-- Ghost sets: B1 (recently evicted from T1), B2 (recently evicted from T2).
-- Adaptive target p controls desired size of T1 with damping, decay and scan clamp.
-- Delayed promotion to T2 to reduce over-protection.
+"""S3Q Scan-Guarded SLRU with TinyLFU and Ghost-driven window adaptation
+
+Resident segments:
+- 'w': small window FIFO (recency burst absorption)
+- 'p': probation (testing area)
+- 'q': protected (frequent)
+
+Ghost sets:
+- Gp (m_ghost_b1_ts): keys evicted from recency side (w or p)
+- Gq (m_ghost_b2_ts): keys evicted from protected side (q)
+
+Adaptation:
+- m_target_p is repurposed as the target window size (|W|), adaptively tuned via ghost hits.
+- Asymmetric caps and ceiling ratios keep control responsive but stable.
+- Scan guarding inserts brand-new keys into probation during cold streaks, protecting Q.
 """
 
-# Resident metadata
-# - m_key_timestamp: last access time for each resident key
-# - m_key_segment: 't1' (recent) or 't2' (frequent) for resident keys
-m_key_timestamp = dict()
-m_key_segment = dict()
-
-# Ghost metadata (key -> last timestamp when it entered ghost)
-m_ghost_b1_ts = dict()
-m_ghost_b2_ts = dict()
-
-# Adaptive target for T1 size
-m_target_p = None
-
-# Additional controls
+# Per-key resident metadata
+m_key_timestamp = dict()  # key -> last access time
+m_key_segment = dict()    # key -> 'w' | 'p' | 'q'
+
+# Ghost metadata (key -> last ghost timestamp)
+m_ghost_b1_ts = dict()  # Gp: evicted from recency side (w or p)
+m_ghost_b2_ts = dict()  # Gq: evicted from protected (q)
+
+# Adaptive window target (repurposed from previous name p)
+m_target_p = None  # desired size of |W|
+
+# Scan and ghost tracking
 m_last_ghost_hit_access = None
-m_cold_streak = 0  # count of consecutive cold misses (not in ghosts)
-m_t1_pending_hits = dict()  # key -> last hit access_count while in T1
-
-# Lightweight frequency sketch with periodic decay to avoid long-term bias
-m_freq = dict()  # key -> decaying frequency count (applies to recent activity)
-m_next_decay_access = None  # next access_count at which to decay frequencies
+m_last_ghost_hit_side = None  # 'Gp' or 'Gq'
+m_cold_streak = 0  # consecutive cold misses (not in ghosts)
+
+# TinyLFU-like frequency sketch with periodic decay
+m_freq = dict()              # key -> decaying frequency
+m_next_decay_access = None   # access threshold for next decay
 
 
 def _cap(cache_snapshot):
     try:
         return int(cache_snapshot.capacity)
     except Exception:
+        # Fallback if capacity unknown; match number of cache entries
         return max(1, len(cache_snapshot.cache))
 
 
 def _ensure_init(cache_snapshot):
     global m_target_p, m_last_ghost_hit_access, m_cold_streak, m_next_decay_access
     if m_target_p is None:
-        m_target_p = max(1, _cap(cache_snapshot) // 2)
+        # Start with a modest window (20% of capacity)
+        m_target_p = max(1, _cap(cache_snapshot) // 5)
     if m_last_ghost_hit_access is None:
         m_last_ghost_hit_access = cache_snapshot.access_count
     if m_cold_streak is None:
         m_cold_streak = 0
     if m_next_decay_access is None:
-        # Schedule frequency decay roughly once per capacity accesses
         m_next_decay_access = cache_snapshot.access_count + max(8, _cap(cache_snapshot))
 
 
-def _resident_sets(cache_snapshot):
-    """Return (t1_keys, t2_keys) among current cache keys, using metadata."""
-    cache_keys = set(cache_snapshot.cache.keys())
-    t1_keys = []
-    t2_keys = []
-    for k in cache_keys:
-        seg = m_key_segment.get(k, 't1')
-        if seg == 't2':
-            t2_keys.append(k)
-        else:
-            # default everything unknown to t1
-            if seg not in ('t1', 't2'):
-                m_key_segment[k] = 't1'
-            t1_keys.append(k)
-    return t1_keys, t2_keys
-
-
-def _lru_key(keys):
-    """Return LRU key among `keys` using m_key_timestamp; None if empty."""
-    if not keys:
-        return None
-    return min(keys, key=lambda k: m_key_timestamp.get(k, float('inf')))
-
-
-def _prune_ghosts(cache_snapshot):
-    """Keep total ghost size <= 2*capacity; evict oldest from the larger ghost list first."""
-    total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
-    cap = _cap(cache_snapshot)
-    limit = max(1, 2 * cap)
-    while total > limit:
-        # Prefer trimming the larger ghost list to maintain balance
-        if len(m_ghost_b1_ts) >= len(m_ghost_b2_ts):
-            if m_ghost_b1_ts:
-                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
-                m_ghost_b1_ts.pop(b1_old_key, None)
-            elif m_ghost_b2_ts:
-                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
-                m_ghost_b2_ts.pop(b2_old_key, None)
-        else:
-            if m_ghost_b2_ts:
-                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
-                m_ghost_b2_ts.pop(b2_old_key, None)
-            elif m_ghost_b1_ts:
-                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
-                m_ghost_b1_ts.pop(b1_old_key, None)
-        total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
-
-
 def _maybe_decay_freq(cache_snapshot):
-    """Periodically decay frequency counts to bound memory and track recent popularity."""
     global m_freq, m_next_decay_access
     _ensure_init(cache_snapshot)
-    if m_next_decay_access is None:
-        m_next_decay_access = cache_snapshot.access_count + max(8, _cap(cache_snapshot))
-        return
-    if cache_snapshot.access_count >= m_next_decay_access:
+    if cache_snapshot.access_count >= (m_next_decay_access or 0):
         if m_freq:
             for k in list(m_freq.keys()):
-                newc = m_freq.get(k, 0) >> 1  # halve counts
+                newc = m_freq.get(k, 0) >> 1
                 if newc:
                     m_freq[k] = newc
                 else:
                     m_freq.pop(k, None)
         m_next_decay_access = cache_snapshot.access_count + max(8, _cap(cache_snapshot))
 
 
-def _bump_freq(key, weight=1):
-    """Increase frequency count with small integer weight."""
+def _bump_freq(key, w=1):
     try:
-        inc = max(1, int(weight))
+        inc = max(1, int(w))
     except Exception:
         inc = 1
     m_freq[key] = m_freq.get(key, 0) + inc
 
 
-def _choose_victim(keys, now):
-    """Pick victim by lowest frequency then oldest timestamp among provided keys."""
+def _segment_keys(cache_snapshot):
+    """Partition current residents by segment label."""
+    w_keys, p_keys, q_keys = [], [], []
+    for k in cache_snapshot.cache.keys():
+        seg = m_key_segment.get(k)
+        if seg == 'w':
+            w_keys.append(k)
+        elif seg == 'q':
+            q_keys.append(k)
+        else:
+            # default unknown to probation for safety
+            m_key_segment[k] = 'p'
+            p_keys.append(k)
+    return w_keys, p_keys, q_keys
+
+
+def _lru_key(keys):
+    if not keys:
+        return None
+    return min(keys, key=lambda k: m_key_timestamp.get(k, float('inf')))
+
+
+def _choose_victim(keys):
+    """Hybrid: prefer lowest freq, then oldest."""
     if not keys:
         return None
     return min(keys, key=lambda k: (m_freq.get(k, 0), m_key_timestamp.get(k, float('inf'))))
 
 
+def _clamp_window_target(cap):
+    """Clamp window size to a reasonable band."""
+    global m_target_p
+    min_w = max(1, cap // 64)
+    max_w = max(1, cap // 2)
+    if m_target_p < min_w:
+        m_target_p = min_w
+    elif m_target_p > max_w:
+        m_target_p = max_w
+
+
+def _prune_ghosts(cache_snapshot):
+    """Keep total ghost size <= 2*cap; bias trimming opposite to last hit side."""
+    global m_ghost_b1_ts, m_ghost_b2_ts
+    cap = _cap(cache_snapshot)
+    limit = max(1, 2 * cap)
+    total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
+    while total > limit:
+        # Prefer trimming the side opposite the last-hit side to retain the signal longer.
+        if m_last_ghost_hit_side == 'Gp' and m_ghost_b2_ts:
+            # trim oldest from Gq
+            v = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
+            m_ghost_b2_ts.pop(v, None)
+        elif m_last_ghost_hit_side == 'Gq' and m_ghost_b1_ts:
+            # trim oldest from Gp
+            v = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
+            m_ghost_b1_ts.pop(v, None)
+        else:
+            # Otherwise trim from the larger
+            if len(m_ghost_b1_ts) >= len(m_ghost_b2_ts):
+                if m_ghost_b1_ts:
+                    v = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
+                    m_ghost_b1_ts.pop(v, None)
+                elif m_ghost_b2_ts:
+                    v = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
+                    m_ghost_b2_ts.pop(v, None)
+            else:
+                if m_ghost_b2_ts:
+                    v = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
+                    m_ghost_b2_ts.pop(v, None)
+                elif m_ghost_b1_ts:
+                    v = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
+                    m_ghost_b1_ts.pop(v, None)
+        total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
+
+
 def evict(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm chooses the eviction victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The new object that needs to be inserted into the cache.
-    - Return:
-        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
-    '''
-    _ensure_init(cache_snapshot)
-    t1_keys, t2_keys = _resident_sets(cache_snapshot)
-
-    # Keep ghosts disjoint from current residents to ensure consistent ARC signals
+    """
+    Choose the eviction victim.
+    Policy:
+    - If window exceeds target, evict from W (FIFO-like via LRU on timestamp).
+    - Otherwise default SLRU: evict from probation P if non-empty; else W; else Q.
+    - Ghost bias: if obj in Gq, favor evicting from recency side (W then P).
+                 if obj in Gp, favor evicting from Q.
+    Within any segment, choose by lowest frequency then LRU.
+    """
+    _ensure_init(cache_snapshot)
+    cap = _cap(cache_snapshot)
+    _clamp_window_target(cap)
+    w_keys, p_keys, q_keys = _segment_keys(cache_snapshot)
+
+    # Keep ghosts disjoint from current residents
     for k in cache_snapshot.cache.keys():
         m_ghost_b1_ts.pop(k, None)
         m_ghost_b2_ts.pop(k, None)
 
-    # Strict ARC REPLACE decision with hybrid LFU-LRU victim choice within a segment:
-    cap = _cap(cache_snapshot)
-    p = min(max(0, m_target_p), cap)
-    t1_len = len(t1_keys)
-    now = cache_snapshot.access_count
-
-    evict_from_t1 = (t1_len > p) or (obj.key in m_ghost_b2_ts and t1_len >= max(1, p))
-
-    if evict_from_t1:
-        victim = _choose_victim(t1_keys, now)
+    # If window is oversized vs target, shed from W first
+    if len(w_keys) > max(1, m_target_p):
+        victim = _choose_victim(w_keys) or _choose_victim(p_keys) or _choose_victim(q_keys)
+        return victim
+
+    in_gp = obj.key in m_ghost_b1_ts
+    in_gq = obj.key in m_ghost_b2_ts
+
+    # Ghost-biased replacement choice
+    if in_gq:
+        # Favor evicting from recency side first
+        victim = _choose_victim(w_keys) or _choose_victim(p_keys) or _choose_victim(q_keys)
         if victim is not None:
             return victim
-        # If T1 is empty, fall back to T2 with hybrid choice
-        victim = _choose_victim(t2_keys, now)
+    elif in_gp:
+        # Favor evicting from protected if present
+        victim = _choose_victim(q_keys) or _choose_victim(p_keys) or _choose_victim(w_keys)
         if victim is not None:
             return victim
-    else:
-        victim = _choose_victim(t2_keys, now)
-        if victim is not None:
-            return victim
-        # If T2 is empty, fall back to T1
-        victim = _choose_victim(t1_keys, now)
-        if victim is not None:
-            return victim
-
-    # Last resort: global lowest-frequency then oldest
-    all_keys = list(cache_snapshot.cache.keys())
-    if not all_keys:
-        return None
-    return _choose_victim(all_keys, now)
+
+    # Default SLRU: evict from probation if possible
+    victim = _choose_victim(p_keys)
+    if victim is not None:
+        return victim
+
+    # Then from W, else Q
+    victim = _choose_victim(w_keys) or _choose_victim(q_keys)
+    return victim
 
 
 def update_after_hit(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object accessed during the cache hit.
-    - Return: `None`
-    '''
-    global m_key_timestamp, m_key_segment, m_t1_pending_hits, m_cold_streak, m_target_p, m_last_ghost_hit_access, m_freq, m_next_decay_access
-    _ensure_init(cache_snapshot)
-
-    # Periodic decay to keep frequency sketch recent
+    """
+    On hit:
+    - Refresh timestamp and bump frequency.
+    - Move from W or P to Q (promotion).
+    - Light idle drift: if no ghost hits for ~cap accesses, nudge window toward baseline.
+    - Keep ghosts disjoint.
+    """
+    global m_cold_streak, m_target_p, m_last_ghost_hit_access
+    _ensure_init(cache_snapshot)
     _maybe_decay_freq(cache_snapshot)
 
-    # Refresh recency
-    m_key_timestamp[obj.key] = cache_snapshot.access_count
-
-    # Reset cold streak on any hit and bump frequency
+    now = cache_snapshot.access_count
+    cap = _cap(cache_snapshot)
+    base_w = max(1, cap // 5)
+
+    # Reset cold streak and update stats
     m_cold_streak = 0
-    _bump_freq(obj.key, 1)
-
-    # Slow decay of p if no ghost hits for a while (scan recovery)
-    cap = _cap(cache_snapshot)
-    if cache_snapshot.access_count - m_last_ghost_hit_access > cap:
-        m_target_p = max(0, m_target_p - 1)
-
-    # ARC-style immediate promotion: on a hit in T1, move to T2
-    seg = m_key_segment.get(obj.key, 't1')
-    if seg != 't2':
-        m_key_segment[obj.key] = 't2'
-    # Clear any pending two-hit state (no longer used)
-    m_t1_pending_hits.pop(obj.key, None)
-
-    # Ensure ghosts remain disjoint from residents
+    m_key_timestamp[obj.key] = now
+    _bump_freq(obj.key, 2)
+
+    # Promotion to protected
+    seg = m_key_segment.get(obj.key, 'p')
+    if seg in ('w', 'p'):
+        m_key_segment[obj.key] = 'q'
+
+    # Idle drift for window target (slowly home toward baseline without ghost signals)
+    if now - m_last_ghost_hit_access > cap:
+        if m_target_p > base_w:
+            m_target_p -= 1
+        elif m_target_p < base_w:
+            m_target_p += 1
+        _clamp_window_target(cap)
+
+    # Keep ghosts disjoint
     m_ghost_b1_ts.pop(obj.key, None)
     m_ghost_b2_ts.pop(obj.key, None)
 
 
 def update_after_insert(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object that was just inserted into the cache.
-    - Return: `None`
-    '''
-    global m_key_timestamp, m_key_segment, m_target_p, m_last_ghost_hit_access, m_cold_streak, m_t1_pending_hits, m_freq, m_next_decay_access
-    _ensure_init(cache_snapshot)
-
-    # Periodic decay to keep frequency sketch recent
+    """
+    On insert (miss path):
+    - Adapt window size via ghost hits:
+        * If key ∈ Gp: increase window target (recency needs more room).
+        * If key ∈ Gq: decrease window target (protect frequency).
+      Use ceiling ratios and asymmetric caps; stronger decrease during cold streaks.
+    - Scan-guard insertion: during sustained cold streaks, insert into probation P
+      instead of window W.
+    - On ghost hits, promote to Q immediately.
+    """
+    global m_target_p, m_last_ghost_hit_access, m_last_ghost_hit_side, m_cold_streak
+    _ensure_init(cache_snapshot)
     _maybe_decay_freq(cache_snapshot)
 
     cap = _cap(cache_snapshot)
-    in_b1 = obj.key in m_ghost_b1_ts
-    in_b2 = obj.key in m_ghost_b2_ts
-
-    # Adaptive step caps with asymmetric responsiveness (scan-aware)
+    _clamp_window_target(cap)
+
+    in_gp = obj.key in m_ghost_b1_ts
+    in_gq = obj.key in m_ghost_b2_ts
+
     inc_cap = max(1, cap // 8)
     dec_cap = max(1, (cap // 4) if m_cold_streak >= max(1, cap // 2) else (cap // 8))
 
-    seg = 't1'
-    if in_b1:
-        # Increase p (give more room to recency), with ceiling ratio and cap
+    seg = 'w'  # default insertion into window
+
+    if in_gp:
+        # Enlarge window (recency pressure)
         denom = max(1, len(m_ghost_b1_ts))
         numer = len(m_ghost_b2_ts)
-        raw_inc = max(1, (numer + denom - 1) // denom)  # ceil(|B2|/|B1|)
-        inc = min(inc_cap, raw_inc)
-        m_target_p = min(cap, m_target_p + inc)
+        raw_inc = max(1, (numer + denom - 1) // denom)  # ceil(|Gq|/|Gp|)
+        m_target_p = min(cap, m_target_p + min(inc_cap, raw_inc))
+        _clamp_window_target(cap)
+        seg = 'q'  # strong signal ⇒ direct protect
         # Ghost bookkeeping
         m_ghost_b1_ts.pop(obj.key, None)
         m_ghost_b2_ts.pop(obj.key, None)
-        # Promote on ghost hit
-        seg = 't2'
         m_last_ghost_hit_access = cache_snapshot.access_count
+        m_last_ghost_hit_side = 'Gp'
         m_cold_streak = 0
-        _bump_freq(obj.key, 2)
-    elif in_b2:
-        # Decrease p (favor frequency), with ceiling ratio and stronger cap during cold streaks
+        _bump_freq(obj.key, 3)
+    elif in_gq:
+        # Shrink window (favor frequency)
         denom = max(1, len(m_ghost_b2_ts))
         numer = len(m_ghost_b1_ts)
-        raw_dec = max(1, (numer + denom - 1) // denom)  # ceil(|B1|/|B2|)
-        dec = min(dec_cap, raw_dec)
-        m_target_p = max(0, m_target_p - dec)
+        raw_dec = max(1, (numer + denom - 1) // denom)  # ceil(|Gp|/|Gq|)
+        m_target_p = max(0, m_target_p - min(dec_cap, raw_dec))
+        _clamp_window_target(cap)
+        seg = 'q'  # strong signal ⇒ direct protect
         # Ghost bookkeeping
         m_ghost_b2_ts.pop(obj.key, None)
         m_ghost_b1_ts.pop(obj.key, None)
-        # Promote on ghost hit
-        seg = 't2'
         m_last_ghost_hit_access = cache_snapshot.access_count
+        m_last_ghost_hit_side = 'Gq'
         m_cold_streak = 0
-        _bump_freq(obj.key, 3)
+        _bump_freq(obj.key, 4)
     else:
-        # Cold miss - track streak and apply gentle scan clamp to reduce T1 pressure
+        # Cold miss: scan guard and gentle clamp
         m_cold_streak += 1
-        # Gradual clamp during sustained cold streaks
+        # During sustained cold streaks, divert brand-new keys to probation
+        if m_cold_streak >= max(1, cap // 2):
+            seg = 'p'
+        else:
+            seg = 'w'
+        # Gentle clamp to reduce window if streak persists
         if m_cold_streak % max(1, cap // 2) == 0:
             m_target_p = max(0, m_target_p - max(1, cap // 16))
-        if m_cold_streak >= cap:
-            m_target_p = max(0, m_target_p - max(1, cap // 8))
-            m_cold_streak = 0
-        # Ensure no stale ghost entries remain for this resident
-        m_ghost_b1_ts.pop(obj.key, None)
-        m_ghost_b2_ts.pop(obj.key, None)
+            _clamp_window_target(cap)
         _bump_freq(obj.key, 1)
 
-    # Insert into resident set
+    # Install resident metadata
     m_key_segment[obj.key] = seg
     m_key_timestamp[obj.key] = cache_snapshot.access_count
-    # Clear any stale pending state for this key
-    m_t1_pending_hits.pop(obj.key, None)
-
-    # Control ghost size (expanded history)
+
+    # Ensure ghosts remain disjoint
+    m_ghost_b1_ts.pop(obj.key, None)
+    m_ghost_b2_ts.pop(obj.key, None)
+
+    # Control ghost history size
     _prune_ghosts(cache_snapshot)
 
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
-    '''
-    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object to be inserted into the cache.
-        - `evicted_obj`: The object that was just evicted from the cache.
-    - Return: `None`
-    '''
-    global m_key_timestamp, m_key_segment, m_ghost_b1_ts, m_ghost_b2_ts, m_t1_pending_hits
-    _ensure_init(cache_snapshot)
-
-    # Remove resident metadata for evicted key
-    seg = m_key_segment.pop(evicted_obj.key, 't1')
+    """
+    After eviction, move evicted resident to appropriate ghost:
+    - If evicted from Q → Gq
+    - Else (from W or P) → Gp
+    """
+    _ensure_init(cache_snapshot)
+
+    seg = m_key_segment.pop(evicted_obj.key, 'p')
     m_key_timestamp.pop(evicted_obj.key, None)
-    m_t1_pending_hits.pop(evicted_obj.key, None)
-
-    # Add to corresponding ghost list with current time to maintain LRU.
-    # Ensure ghosts remain disjoint.
+
     ts = cache_snapshot.access_count
-    if seg == 't2':
-        m_ghost_b2_ts[evicted_obj.key] = ts
+    if seg == 'q':
+        m_ghost_b2_ts[evicted_obj.key] = ts  # Gq
         m_ghost_b1_ts.pop(evicted_obj.key, None)
     else:
-        m_ghost_b1_ts[evicted_obj.key] = ts
+        m_ghost_b1_ts[evicted_obj.key] = ts  # Gp
         m_ghost_b2_ts.pop(evicted_obj.key, None)
 
-    # Control ghost size (expanded history)
     _prune_ghosts(cache_snapshot)
-
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate