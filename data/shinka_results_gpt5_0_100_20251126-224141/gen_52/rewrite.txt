# EVOLVE-BLOCK-START
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

from collections import OrderedDict

# LRU timestamp map kept for compatibility and as a tie-breaker
m_key_timestamp = dict()

# Lightweight LFU counter with periodic aging
m_freq = dict()
last_age_access = 0
AGE_INTERVAL_FACTOR = 4  # age every ~4×capacity accesses (at least 500)

# Scan/idle adaptation state
last_ghost_hit_access = -1
cold_streak = 0
scan_guard_until = 0  # time until which we bias REPLACE toward T1

# Adaptive Replacement Cache (ARC) metadata
arc_T1 = OrderedDict()  # recent, resident
arc_T2 = OrderedDict()  # frequent, resident
arc_B1 = OrderedDict()  # ghost of T1
arc_B2 = OrderedDict()  # ghost of T2
arc_p = 0               # target size of T1
arc_capacity = None     # will be initialized from cache_snapshot


def _ensure_capacity(cache_snapshot):
    global arc_capacity, arc_p
    if arc_capacity is None:
        arc_capacity = max(int(cache_snapshot.capacity), 1)
    # Bound p within [0, C]
    if arc_capacity is not None:
        arc_p = min(max(arc_p, 0), arc_capacity)


def _maybe_age(cache_snapshot):
    global last_age_access, m_freq
    if arc_capacity is None:
        return
    interval = max(500, arc_capacity * AGE_INTERVAL_FACTOR)
    now = cache_snapshot.access_count
    if now - last_age_access >= interval:
        # Halve all frequencies to age out stale popularity
        for k in list(m_freq.keys()):
            newv = m_freq.get(k, 0) >> 1
            if newv <= 0:
                m_freq.pop(k, None)
            else:
                m_freq[k] = newv
        last_age_access = now


def _decay_p_if_idle(cache_snapshot):
    # Decay p when there have been no ghost hits for a while
    global arc_p
    if arc_capacity is None:
        return
    if last_ghost_hit_access < 0:
        return
    idle = cache_snapshot.access_count - last_ghost_hit_access
    if idle <= 0:
        return
    # Proportional, bounded decay
    dec = min(arc_capacity // 8 if arc_capacity else 1, max(1, idle // max(1, arc_capacity // 4)))
    arc_p = max(0, arc_p - dec)


def _move_to_mru(od, key):
    # Push key to MRU position of an OrderedDict
    if key in od:
        od.pop(key, None)
    od[key] = True


def _pop_lru(od):
    if od:
        k, _ = od.popitem(last=False)
        return k
    return None


def _trim_ghosts():
    # Keep ghosts within 2x capacity and bias trimming to track p split:
    # target |B1| ≈ p, |B2| ≈ C - p
    cap = arc_capacity if arc_capacity is not None else 1
    # Bound p
    global arc_p
    arc_p = min(max(arc_p, 0), cap)
    total_cap = 2 * cap
    while (len(arc_B1) + len(arc_B2)) > total_cap:
        target_B1 = min(cap, max(0, arc_p))
        target_B2 = max(0, cap - target_B1)
        excess_B1 = max(0, len(arc_B1) - target_B1)
        excess_B2 = max(0, len(arc_B2) - target_B2)
        if excess_B1 >= excess_B2 and arc_B1:
            _pop_lru(arc_B1)
        elif arc_B2:
            _pop_lru(arc_B2)
        else:
            # If both within target but total still exceeds (due to rounding), trim larger
            if len(arc_B1) >= len(arc_B2) and arc_B1:
                _pop_lru(arc_B1)
            elif arc_B2:
                _pop_lru(arc_B2)
            else:
                break


def _resync(cache_snapshot):
    # Ensure resident metadata tracks actual cache content and ghosts remain disjoint
    cache_keys = set(cache_snapshot.cache.keys())
    for k in list(arc_T1.keys()):
        if k not in cache_keys:
            arc_T1.pop(k, None)
    for k in list(arc_T2.keys()):
        if k not in cache_keys:
            arc_T2.pop(k, None)
    # Add any cached keys we missed to T1 as recent
    for k in cache_keys:
        if k not in arc_T1 and k not in arc_T2:
            arc_T1[k] = True
    # Ghosts must not contain residents
    for k in list(arc_B1.keys()):
        if k in arc_T1 or k in arc_T2:
            arc_B1.pop(k, None)
    for k in list(arc_B2.keys()):
        if k in arc_T1 or k in arc_T2:
            arc_B2.pop(k, None)
    _trim_ghosts()


def _pick_lfu_among_lru(od, sample_k):
    # Among the k oldest in od, pick the key with the lowest frequency.
    # Tie-break by oldest timestamp to better approximate true LRU for equals.
    if not od:
        return None
    k = max(1, sample_k)
    best_key = None
    best_freq = None
    best_ts = None
    count = 0
    for key in od.keys():
        f = m_freq.get(key, 0)
        ts = m_key_timestamp.get(key, float('inf'))
        if (best_freq is None or
            f < best_freq or
            (f == best_freq and ts < best_ts)):
            best_freq = f
            best_ts = ts
            best_key = key
        count += 1
        if count >= k:
            break
    return best_key if best_key is not None else next(iter(od))


def _demote_t2_to_t1(n_items):
    # Demote up to n_items from T2 LRU to T1 MRU to protect frequency during scans
    moved = 0
    while moved < n_items and arc_T2:
        k = _pop_lru(arc_T2)
        if k is None:
            break
        _move_to_mru(arc_T1, k)
        moved += 1
    return moved


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global arc_p, last_ghost_hit_access, cold_streak, scan_guard_until
    _ensure_capacity(cache_snapshot)
    _maybe_age(cache_snapshot)
    _decay_p_if_idle(cache_snapshot)

    # Keep metadata consistent
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    cap = arc_capacity if arc_capacity is not None else 1
    step_cap = max(1, cap // 8)

    # Ghost-driven p updates before REPLACE (canonical ARC)
    now = cache_snapshot.access_count
    if obj.key in arc_B1:
        # Recent reuse: increase p
        inc = max(1, len(arc_B2) // max(1, len(arc_B1)))
        arc_p = min(cap, arc_p + min(step_cap, inc))
        last_ghost_hit_access = now
        cold_streak = 0
    elif obj.key in arc_B2:
        # Frequent reuse: decrease p
        dec = max(1, len(arc_B1) // max(1, len(arc_B2)))
        arc_p = max(0, arc_p - min(step_cap, dec))
        last_ghost_hit_access = now
        cold_streak = 0
    else:
        # Cold miss: track streak and possibly activate scan guard
        cold_streak += 1
        if cold_streak >= max(1, cap // 2) and now >= scan_guard_until:
            # Activate a short guard window to bias REPLACE toward T1
            scan_guard_until = now + max(1, cap // 8)
            # Nudge p down gently to protect frequency during scans
            arc_p = max(0, arc_p - min(cap // 8, max(1, cold_streak // max(1, cap // 8))))
            # Optionally demote a couple of T2 LRU to T1 MRU
            _demote_t2_to_t1(min(2, max(1, cap // 16)))

    # ARC replacement: choose between T1 and T2 depending on updated arc_p and ghost hit type
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)
    choose_T1 = (t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)))

    # If scan guard active, bias toward T1 when possible
    if now < scan_guard_until and len(arc_T1) > 0:
        choose_T1 = True

    # Frequency-aware sampling among the k oldest of the chosen list
    sample_k = min(16, max(2, (arc_capacity if arc_capacity else 1) // 8))
    candidate = None
    if choose_T1 and arc_T1:
        candidate = _pick_lfu_among_lru(arc_T1, sample_k)
    elif (not choose_T1) and arc_T2:
        candidate = _pick_lfu_among_lru(arc_T2, sample_k)

    # If preferred list empty, try the other resident list
    if candidate is None:
        if arc_T1:
            candidate = _pick_lfu_among_lru(arc_T1, sample_k)
        elif arc_T2:
            candidate = _pick_lfu_among_lru(arc_T2, sample_k)

    # Strengthened fallback sequence
    if candidate is None:
        # (a) T1 LRU not in B2
        for k in arc_T1.keys():
            if k not in arc_B2:
                candidate = k
                break
    if candidate is None:
        # (b) T2 LRU that appears in B1
        for k in arc_T2.keys():
            if k in arc_B1:
                candidate = k
                break
    if candidate is None:
        # (c) scan up to C//16 from T1 then T2 to find a key not in B2
        limit = max(1, cap // 16)
        cnt = 0
        for k in arc_T1.keys():
            if k not in arc_B2:
                candidate = k
                break
            cnt += 1
            if cnt >= limit:
                break
        if candidate is None:
            cnt = 0
            for k in arc_T2.keys():
                if k in arc_B1:
                    candidate = k
                    break
                cnt += 1
                if cnt >= limit:
                    break

    if candidate is None:
        # Fallback: choose the oldest by timestamp if available, else any key
        if m_key_timestamp and cache_snapshot.cache:
            min_ts = float('inf')
            best = None
            for k in cache_snapshot.cache.keys():
                ts = m_key_timestamp.get(k, float('inf'))
                if ts < min_ts:
                    min_ts = ts
                    best = k
            candidate = best
        if candidate is None and cache_snapshot.cache:
            candidate = next(iter(cache_snapshot.cache.keys()))
    return candidate


def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, cold_streak
    _ensure_capacity(cache_snapshot)
    _maybe_age(cache_snapshot)

    # Reset cold streak on any hit
    cold_streak = 0

    # ARC: on hit, move to T2 MRU
    key = obj.key
    if key in arc_T1:
        arc_T1.pop(key, None)
        _move_to_mru(arc_T2, key)
    else:
        # If already in T2, refresh; if not present due to drift, place in T2
        if key in arc_T2:
            _move_to_mru(arc_T2, key)
        else:
            _move_to_mru(arc_T2, key)
    # Keep ghosts disjoint with residents
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)
    # Update timestamp for tie-breaking/fallback and bump frequency
    m_key_timestamp[key] = cache_snapshot.access_count
    m_freq[key] = m_freq.get(key, 0) + 1

    # Defensive: repair metadata drift if any
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_freq, last_ghost_hit_access, cold_streak
    _ensure_capacity(cache_snapshot)
    _maybe_age(cache_snapshot)
    key = obj.key
    now = cache_snapshot.access_count

    # ARC admission policy (p was already adjusted in evict on ghost hits)
    was_ghost = False
    if key in arc_B1:
        arc_B1.pop(key, None)
        _move_to_mru(arc_T2, key)
        last_ghost_hit_access = now
        cold_streak = 0
        was_ghost = True
    elif key in arc_B2:
        arc_B2.pop(key, None)
        _move_to_mru(arc_T2, key)
        last_ghost_hit_access = now
        cold_streak = 0
        was_ghost = True
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)

    # Keep ghosts disjoint with residents and trimmed
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)
    _trim_ghosts()
    # Track access time; bump frequency only for ghost re-admissions (reuse signal)
    m_key_timestamp[key] = now
    if was_ghost:
        m_freq[key] = m_freq.get(key, 0) + 1

    # Defensive: repair metadata drift if any
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp
    _ensure_capacity(cache_snapshot)
    _maybe_age(cache_snapshot)
    k = evicted_obj.key
    # Move evicted resident to corresponding ghost list and keep ghosts disjoint
    if k in arc_T1:
        arc_T1.pop(k, None)
        arc_B2.pop(k, None)
        _move_to_mru(arc_B1, k)
    elif k in arc_T2:
        arc_T2.pop(k, None)
        arc_B1.pop(k, None)
        _move_to_mru(arc_B2, k)
    else:
        # Unknown membership: prefer consistency with existing ghost presence
        if k in arc_B2:
            arc_B1.pop(k, None)
            _move_to_mru(arc_B2, k)
        else:
            arc_B2.pop(k, None)
            _move_to_mru(arc_B1, k)
    # Remove timestamp entry for evicted item to avoid growth (keep freq as history)
    m_key_timestamp.pop(k, None)
    _trim_ghosts()

# EVOLVE-BLOCK-END