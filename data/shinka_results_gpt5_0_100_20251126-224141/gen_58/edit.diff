--- a/original.py
+++ b/original.py
@@ -1,333 +1,278 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
+"""Cache eviction algorithm: SLRU + TinyLFU-guided victim selection"""
 
 from collections import OrderedDict
 
-# LRU timestamp map used as a tie-breaker and fallback
+# Global timestamps for tie-breaking/fallback
 m_key_timestamp = dict()
 
-# Adaptive Replacement Cache (ARC) metadata
-arc_T1 = OrderedDict()  # recent, resident
-arc_T2 = OrderedDict()  # frequent, resident
-arc_B1 = OrderedDict()  # ghost of T1
-arc_B2 = OrderedDict()  # ghost of T2
-arc_p = 0               # target size of T1
-arc_capacity = None     # will be initialized from cache_snapshot
-
-# Delayed promotion and adaptation state
-t1_pending = dict()             # key -> last hit access_count while in T1
-arc_last_ghost_hit_access = 0   # last access_count when a ghost hit occurred
-arc_last_decay_access = 0       # throttle decay operations
-cold_streak = 0                 # consecutive cold admissions without ghost/hit signal
-scan_guard_until = 0            # bias REPLACE toward T1 during suspected scans
-
-def _pending_window():
-    cap = arc_capacity if arc_capacity is not None else 1
-    return max(1, cap // 4)
+# SLRU segments
+slru_probation = OrderedDict()  # probationary segment (new/cold)
+slru_protected = OrderedDict()  # protected segment (hot)
+slru_capacity = None
+slru_protected_max = None  # target size for protected segment
+
+# TinyLFU Count-Min Sketch
+cms_depth = 4
+cms_width = 2048
+cms_mask = cms_width - 1
+cms_tables = None
+cms_additions = 0
+cms_decay_interval = 4096  # number of increments between decays
+
+
+def _next_pow2(x: int) -> int:
+    v = 1
+    while v < x:
+        v <<= 1
+    return max(1, v)
+
+
+def _cms_init():
+    global cms_tables, cms_width, cms_mask, cms_decay_interval
+    if cms_tables is None:
+        cms_tables = [[0] * cms_width for _ in range(cms_depth)]
+        cms_mask = cms_width - 1
+        # decay interval tuned to capacity scale (done in _ensure_capacity)
+        # already set by _ensure_capacity
+
+
+def _cms_reinit_for_capacity(cap: int):
+    """Reinitialize CMS parameters when capacity is first known."""
+    global cms_tables, cms_width, cms_mask, cms_decay_interval
+    # Width proportional to capacity; power of two for fast masking
+    cms_width = min(16384, max(1024, _next_pow2(cap * 2)))
+    cms_mask = cms_width - 1
+    cms_tables = [[0] * cms_width for _ in range(cms_depth)]
+    # Decay about every few caches of operations to bound counters
+    cms_decay_interval = max(2 * cms_width, cap * 8)
+
+
+def _cms_idx(i, key):
+    # Mix hashes per depth; use mask for modulo
+    h1 = hash(key)
+    h2 = hash((i + 0x9e3779b1, key))
+    return (h1 ^ (h2 << 1) ^ (h1 >> (i + 1))) & cms_mask
+
+
+def _cms_inc(key):
+    global cms_additions
+    if cms_tables is None:
+        _cms_init()
+    for i in range(cms_depth):
+        idx = _cms_idx(i, key)
+        # Saturate at 2^31-1 to avoid overflow
+        v = cms_tables[i][idx] + 1
+        cms_tables[i][idx] = v if v < 0x7FFFFFFF else 0x7FFFFFFF
+    cms_additions += 1
+    if cms_additions % cms_decay_interval == 0:
+        # Periodic halving decay
+        for i in range(cms_depth):
+            row = cms_tables[i]
+            for j in range(cms_width):
+                row[j] >>= 1
+
+
+def _cms_estimate(key) -> int:
+    if cms_tables is None:
+        _cms_init()
+    est = None
+    for i in range(cms_depth):
+        idx = _cms_idx(i, key)
+        v = cms_tables[i][idx]
+        est = v if est is None else (v if v < est else est)
+    return est if est is not None else 0
 
 
 def _ensure_capacity(cache_snapshot):
-    global arc_capacity
-    if arc_capacity is None:
-        arc_capacity = max(int(cache_snapshot.capacity), 1)
+    """Initialize capacity-dependent parameters once."""
+    global slru_capacity, slru_protected_max
+    if slru_capacity is None:
+        slru_capacity = max(int(cache_snapshot.capacity), 1)
+        # Protected gets 80%, probation gets 20% by default
+        slru_protected_max = max(1, int(slru_capacity * 0.8))
+        _cms_reinit_for_capacity(slru_capacity)
 
 
 def _move_to_mru(od, key):
-    # Push key to MRU position of an OrderedDict
     if key in od:
         od.pop(key, None)
     od[key] = True
 
 
 def _pop_lru(od):
-    if od:
-        k, _ = od.popitem(last=False)
-        return k
-    return None
-
-
-def _trim_ghosts():
-    # Keep ghosts total size within 2x capacity with proportional trimming to p
-    cap_base = (arc_capacity if arc_capacity is not None else 1)
-    total_cap = cap_base * 2
-    # Targets proportional to current p (approximately 2*p and 2C-2*p)
-    target_B1 = min(total_cap, max(0, int(total_cap * (float(arc_p) / max(1, cap_base)))))
-    target_B2 = max(0, total_cap - target_B1)
-
-    def _over_target():
-        return (len(arc_B1) + len(arc_B2)) - total_cap
-
-    # Trim until under the total budget, favoring lists above their targets
-    while (len(arc_B1) + len(arc_B2)) > total_cap:
-        if len(arc_B1) > target_B1:
-            _pop_lru(arc_B1)
-        elif len(arc_B2) > target_B2:
-            _pop_lru(arc_B2)
+    if not od:
+        return None
+    k, _ = od.popitem(last=False)
+    return k
+
+
+def _resync(cache_snapshot):
+    """Ensure SLRU metadata matches actual cache contents."""
+    cache_keys = set(cache_snapshot.cache.keys())
+    # Remove non-resident keys from segments
+    for k in list(slru_probation.keys()):
+        if k not in cache_keys:
+            slru_probation.pop(k, None)
+    for k in list(slru_protected.keys()):
+        if k not in cache_keys:
+            slru_protected.pop(k, None)
+    # Any missing resident keys become probationary
+    for k in cache_keys:
+        if k not in slru_probation and k not in slru_protected:
+            slru_probation[k] = True
+    # Enforce protected max by demoting LRU protected if oversized
+    while len(slru_protected) > (slru_protected_max if slru_protected_max is not None else len(slru_protected)):
+        demote = _pop_lru(slru_protected)
+        if demote is not None:
+            _move_to_mru(slru_probation, demote)
+
+
+def _choose_victim_from_probation(sample_k: int, now: int):
+    """Pick the lowest estimated frequency among the k-elder probation entries."""
+    if not slru_probation:
+        return None
+    # Gather up to sample_k from probation LRU side
+    victims = []
+    i = 0
+    for k in slru_probation.keys():
+        victims.append(k)
+        i += 1
+        if i >= sample_k:
+            break
+    # Choose by minimum TinyLFU estimate, tie-break by oldest timestamp
+    best = None
+    best_score = None
+    best_time = None
+    for k in victims:
+        s = _cms_estimate(k)
+        ts = m_key_timestamp.get(k, 0)
+        if best is None or s < best_score or (s == best_score and ts < best_time):
+            best = k
+            best_score = s
+            best_time = ts
+    return best
+
+
+def evict(cache_snapshot, obj):
+    """
+    Choose eviction victim using SLRU + TinyLFU:
+    - Prefer evicting from probationary LRU set; sample a few LRU candidates and evict the one with lowest estimated frequency.
+    - If probation is empty, demote one protected LRU to probation and then evict from probation.
+    """
+    _ensure_capacity(cache_snapshot)
+    _resync(cache_snapshot)
+    now = cache_snapshot.access_count
+
+    # Try sampling from probation
+    sample_k = max(2, min(8, slru_capacity // 16))  # small, bounded sample
+    victim = _choose_victim_from_probation(sample_k, now)
+
+    # If probation empty, demote from protected then retry
+    if victim is None:
+        demote = _pop_lru(slru_protected)
+        if demote is not None:
+            _move_to_mru(slru_probation, demote)
+            victim = _choose_victim_from_probation(sample_k, now)
+
+    # Fallbacks if metadata empty or drifted
+    if victim is None:
+        _resync(cache_snapshot)
+        # Try again
+        victim = _choose_victim_from_probation(sample_k, now)
+    if victim is None:
+        # As last resort, evict oldest in cache by timestamp
+        if cache_snapshot.cache:
+            # Find key with minimum timestamp
+            victim = None
+            oldest_ts = None
+            for k in cache_snapshot.cache.keys():
+                ts = m_key_timestamp.get(k, 0)
+                if victim is None or ts < oldest_ts:
+                    victim = k
+                    oldest_ts = ts
         else:
-            # If neither is over its proportional target, drop from the larger one
-            if len(arc_B1) >= len(arc_B2):
-                _pop_lru(arc_B1)
-            else:
-                _pop_lru(arc_B2)
-
-# Decay controller: if no ghost hits for a while, bias toward recency (smaller p) with bounded proportional decay
-def _decay_arc_p_if_idle(now):
-    global arc_p, arc_last_decay_access, cold_streak
-    if arc_capacity is None:
-        return
-    cap = arc_capacity
-    idle = now - arc_last_ghost_hit_access
-    step_interval = max(1, cap // 8)
-    if idle >= cap and (now - arc_last_decay_access) >= step_interval:
-        step = min(max(1, cap // 8), max(1, idle // max(1, cap // 4)))
-        arc_p = max(0, arc_p - step)
-        arc_last_decay_access = now
-    # If we've accumulated a cold streak, apply a one-time clamp to speed recovery from scans
-    if cold_streak >= max(1, cap // 2):
-        clamp = min(max(1, cap // 4), max(1, cold_streak // max(1, cap // 8)))
-        arc_p = max(0, arc_p - clamp)
-
-
-def _resync(cache_snapshot):
-    # Ensure resident metadata tracks actual cache content
-    cache_keys = set(cache_snapshot.cache.keys())
-    for k in list(arc_T1.keys()):
-        if k not in cache_keys:
-            arc_T1.pop(k, None)
-    for k in list(arc_T2.keys()):
-        if k not in cache_keys:
-            arc_T2.pop(k, None)
-    # Add any cached keys we missed to T1 as recent
-    for k in cache_keys:
-        if k not in arc_T1 and k not in arc_T2:
-            arc_T1[k] = True
-    _trim_ghosts()
-
-
-def evict(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm chooses the eviction victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The new object that needs to be inserted into the cache.
-    - Return:
-        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
-    '''
+            victim = None
+    return victim
+
+
+def update_after_hit(cache_snapshot, obj):
+    """
+    On cache hit:
+    - Increment TinyLFU counter.
+    - If in probation: promote to protected MRU; if protected exceeds its target, demote its LRU to probation MRU.
+    - If in protected: refresh to MRU.
+    - If not tracked (drift): add to probation MRU.
+    """
     _ensure_capacity(cache_snapshot)
     _resync(cache_snapshot)
-    # Apply ghost-driven adaptation before replacement (canonical ARC behavior)
-    global arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until
     now = cache_snapshot.access_count
-    step_cap = max(1, (arc_capacity if arc_capacity is not None else 1) // 8)
-    if obj.key in arc_B1:
-        # Favor recency: increase p
-        ratio = max(1, len(arc_B2) // max(1, len(arc_B1)))
-        inc = min(ratio, step_cap, max(0, (arc_capacity if arc_capacity is not None else 0) - arc_p))
-        arc_p = min((arc_capacity if arc_capacity is not None else arc_p), arc_p + inc)
-        arc_last_ghost_hit_access = now
-        cold_streak = 0
-    elif obj.key in arc_B2:
-        # Favor frequency: decrease p
-        ratio = max(1, len(arc_B1) // max(1, len(arc_B2)))
-        dec = min(ratio, step_cap, arc_p)
-        arc_p = max(0, arc_p - dec)
-        arc_last_ghost_hit_access = now
-        cold_streak = 0
-
-    # ARC replacement: choose between T1 and T2 depending on p and whether obj is in B2
-    x_in_B2 = obj.key in arc_B2
-    t1_sz = len(arc_T1)
-    choose_from_T1 = False
-    if t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)):
-        choose_from_T1 = True
-
-    # Scan-guard bias: during suspected scans, prefer evicting from T1 if possible
-    if now <= scan_guard_until and t1_sz > 0:
-        choose_from_T1 = True
-
-    victim = None
-    if choose_from_T1:
-        # Evict LRU from T1
-        victim = next(iter(arc_T1)) if arc_T1 else None
+    key = obj.key
+
+    _cms_inc(key)
+
+    if key in slru_protected:
+        _move_to_mru(slru_protected, key)
+    elif key in slru_probation:
+        # Promote to protected
+        slru_probation.pop(key, None)
+        _move_to_mru(slru_protected, key)
+        # Keep protected within cap via demotion of its LRU
+        if slru_protected_max is not None and len(slru_protected) > slru_protected_max:
+            demote = _pop_lru(slru_protected)
+            if demote is not None:
+                _move_to_mru(slru_probation, demote)
     else:
-        # Evict LRU from T2
-        victim = next(iter(arc_T2)) if arc_T2 else None
-
-    # Fallbacks: if chosen list is empty, try the other; otherwise bounded LRU scan
-    if victim is None:
-        if t1_sz > 0 and arc_T1:
-            victim = next(iter(arc_T1))
-        elif len(arc_T2) > 0 and arc_T2:
-            victim = next(iter(arc_T2))
-        else:
-            # Rare drift: resync once and retry small bounded scan, then deterministic fallback
-            _resync(cache_snapshot)
-            scan_lim = max(1, (arc_capacity if arc_capacity is not None else 1) // 16)
-            if arc_T1:
-                i = 0
-                for k in arc_T1.keys():
-                    victim = k
-                    i += 1
-                    if i >= scan_lim:
-                        break
-            elif arc_T2:
-                i = 0
-                for k in arc_T2.keys():
-                    victim = k
-                    i += 1
-                    if i >= scan_lim:
-                        break
-            elif cache_snapshot.cache:
-                # Deterministic final fallback: first key iteration
-                victim = next(iter(cache_snapshot.cache.keys()))
-    return victim
-
-
-def update_after_hit(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object accessed during the cache hit.
-    - Return: `None`
-    '''
-    global m_key_timestamp, cold_streak
-    _ensure_capacity(cache_snapshot)
+        # Drift: place as probationary
+        _move_to_mru(slru_probation, key)
+
+    m_key_timestamp[key] = now
+
+
+def update_after_insert(cache_snapshot, obj):
+    """
+    After a miss and insertion:
+    - Increment TinyLFU counter.
+    - Place the new key into probation MRU.
+    """
+    _ensure_capacity(cache_snapshot)
+    _resync(cache_snapshot)
     now = cache_snapshot.access_count
-    _decay_arc_p_if_idle(now)
-    # Any hit breaks cold streaks
-    cold_streak = 0
-
-    # Keep resident metadata consistent with actual cache
-    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
-        _resync(cache_snapshot)
-
     key = obj.key
-    if key in arc_T1:
-        # Canonical ARC: on a hit in T1, move to T2 (become frequent)
-        arc_T1.pop(key, None)
-        _move_to_mru(arc_T2, key)
-        t1_pending.pop(key, None)
-    elif key in arc_T2:
-        # Refresh recency within T2
-        _move_to_mru(arc_T2, key)
-    else:
-        # Metadata drift: conservatively place into T1 as recent
-        _move_to_mru(arc_T1, key)
-
-    # Maintain disjointness: resident keys must not appear in ghosts
-    arc_B1.pop(key, None)
-    arc_B2.pop(key, None)
-
-    _trim_ghosts()
-    # Update timestamp for tie-breaking/fallback
+
+    _cms_inc(key)
+    _move_to_mru(slru_probation, key)
     m_key_timestamp[key] = now
 
 
-def update_after_insert(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object that was just inserted into the cache.
-    - Return: `None`
-    '''
-    global m_key_timestamp, arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until
-    _ensure_capacity(cache_snapshot)
-    now = cache_snapshot.access_count
-    _decay_arc_p_if_idle(now)
-    key = obj.key
-
-    # Keep resident metadata consistent with actual cache
-    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
-        _resync(cache_snapshot)
-
-    # ARC admission: p was already adapted in evict on ghost hits. Place accordingly.
-    if key in arc_B1 or key in arc_B2:
-        # On a ghost hit, item becomes frequent
-        _move_to_mru(arc_T2, key)
-        arc_last_ghost_hit_access = now
-        cold_streak = 0
-    else:
-        # Brand new: insert into T1 (recent)
-        _move_to_mru(arc_T1, key)
-        cold_streak += 1
-        # Streaming-aware behavior: early guard and progressive clamp
-        if arc_capacity is not None:
-            cap = arc_capacity
-            # Bias REPLACE to draw from T1 for a short window during scans
-            if cold_streak >= max(1, cap // 2):
-                scan_guard_until = now + max(1, cap // 8)
-            if cold_streak >= cap:
-                dec = max(1, cap // 8)
-                arc_p = max(0, arc_p - dec)
-                # Keep pending hints conservative after clamp
-                t1_pending.clear()
-                # Prevent unbounded growth of the streak counter to keep adaptation responsive
-                if cold_streak >= 2 * cap:
-                    arc_p = max(0, arc_p - dec)
-                    cold_streak = cap
-
-    # Maintain disjointness: resident keys must not appear in ghosts
-    arc_B1.pop(key, None)
-    arc_B2.pop(key, None)
-
-    _trim_ghosts()
-    m_key_timestamp[key] = now
-
-
 def update_after_evict(cache_snapshot, obj, evicted_obj):
-    '''
-    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object to be inserted into the cache.
-        - `evicted_obj`: The object that was just evicted from the cache.
-    - Return: `None`
-    '''
-    global m_key_timestamp
+    """
+    After eviction:
+    - Remove the evicted key from SLRU segments and timestamp map.
+    """
     _ensure_capacity(cache_snapshot)
     k = evicted_obj.key
-    # Move evicted resident to corresponding ghost list, keeping ghosts disjoint
-    if k in arc_T1:
-        arc_T1.pop(k, None)
-        _move_to_mru(arc_B1, k)
-        arc_B2.pop(k, None)
-    elif k in arc_T2:
-        arc_T2.pop(k, None)
-        _move_to_mru(arc_B2, k)
-        arc_B1.pop(k, None)
-    else:
-        # Unknown membership: prefer B2 if it already exists there, otherwise B1
-        if k in arc_B2:
-            _move_to_mru(arc_B2, k)
-            arc_B1.pop(k, None)
-        else:
-            _move_to_mru(arc_B1, k)
-            arc_B2.pop(k, None)
-    # Clean up metadata for evicted item
+    slru_probation.pop(k, None)
+    slru_protected.pop(k, None)
     m_key_timestamp.pop(k, None)
-    t1_pending.pop(k, None)
-    _trim_ghosts()
-
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate