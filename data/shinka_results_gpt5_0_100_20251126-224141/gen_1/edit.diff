--- a/original.py
+++ b/original.py
@@ -1,79 +1,194 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
+"""Adaptive Replacement Cache (ARC) to optimize hit rates across diverse workloads"""
+from collections import OrderedDict
 
-m_key_timestamp = dict()
+# In-cache lists (LRU order: left=LRU, right=MRU)
+_T1 = OrderedDict()  # Recent (seen once)
+_T2 = OrderedDict()  # Frequent (seen >= twice)
+
+# Ghost lists (track recently evicted keys from T1/T2; no data, just keys)
+_B1 = OrderedDict()
+_B2 = OrderedDict()
+
+# Target size for T1 (recency portion). Adapted online.
+_p = 0.0
+
+# Observed capacity (number of items). We align with cache_snapshot.capacity.
+_g_capacity = 0
+
+
+def _ensure_capacity(cache_snapshot):
+    """Initialize/refresh capacity-aware parameters and trim ghost lists."""
+    global _g_capacity, _p
+    if cache_snapshot is None:
+        return
+    cap = cache_snapshot.capacity
+    # Fallback to at least 1; also ensure not below current cache length
+    cap = max(cap, len(cache_snapshot.cache), 1)
+    if _g_capacity != cap:
+        _g_capacity = cap
+        # Initialize p to half capacity on first run; clamp otherwise
+        if _p == 0.0:
+            _p = float(max(1, int(round(0.5 * _g_capacity))))
+        else:
+            _p = min(max(_p, 0.0), float(_g_capacity))
+
+    # Bound ghost list sizes to capacity (standard ARC behavior)
+    while len(_B1) > _g_capacity:
+        _B1.popitem(last=False)
+    while len(_B2) > _g_capacity:
+        _B2.popitem(last=False)
+
+
+def _move_to_mru(od: OrderedDict, key: str):
+    """Place key at MRU position of the ordered dict."""
+    if key in od:
+        od.pop(key, None)
+    od[key] = None
+
+
+def _lru_key(od: OrderedDict):
+    """Get LRU key without removing; None if empty."""
+    try:
+        return next(iter(od))
+    except StopIteration:
+        return None
+
 
 def evict(cache_snapshot, obj):
     '''
-    This function defines how the algorithm chooses the eviction victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The new object that needs to be inserted into the cache.
-    - Return:
-        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
+    Choose eviction victim per ARC replace() rule.
+    Prefer evicting from T1 when T1 is above target p (or on B2-ghost arrivals),
+    otherwise evict from T2. Fallback: any cached key if metadata desync occurs.
     '''
-    candid_obj_key = None
-    min_ts = min(m_key_timestamp.values())
-    candid_obj_keys = list(key for key in cache_snapshot.cache if m_key_timestamp[key] == min_ts)
-    candid_obj_key = candid_obj_keys[0]
-    return candid_obj_key
+    _ensure_capacity(cache_snapshot)
+
+    t1_sz = len(_T1)
+    t2_sz = len(_T2)
+
+    # ARC replace() heuristic
+    # If incoming key is in B2 and T1 is at target p, evict from T1 to protect frequency.
+    # Else if T1 is larger than p, evict from T1; otherwise from T2.
+    victim = None
+    if t1_sz > 0 and ((obj.key in _B2 and t1_sz >= int(round(_p))) or (t1_sz > int(round(_p)))):
+        victim = _lru_key(_T1)
+    elif t2_sz > 0:
+        victim = _lru_key(_T2)
+    elif t1_sz > 0:
+        victim = _lru_key(_T1)
+
+    # Fallback if metadata out-of-sync: pick any current cache key
+    if victim is None or victim not in cache_snapshot.cache:
+        for k in cache_snapshot.cache:
+            victim = k
+            break
+
+    return victim
+
 
 def update_after_hit(cache_snapshot, obj):
     '''
-    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object accessed during the cache hit.
-    - Return: `None`
+    Update ARC state after a cache hit.
+    - If hit in T1: promote to T2 MRU.
+    - If hit in T2: move to T2 MRU.
+    - If not tracked (desync), consider it frequent and add to T2 MRU.
     '''
-    global m_key_timestamp
-    assert obj.key in m_key_timestamp
-    m_key_timestamp[obj.key] = cache_snapshot.access_count
+    _ensure_capacity(cache_snapshot)
+    key = obj.key
+
+    if key in _T1:
+        _T1.pop(key, None)
+        _move_to_mru(_T2, key)
+    elif key in _T2:
+        _move_to_mru(_T2, key)
+    else:
+        # Metadata desync: treat as frequent since it hit
+        _move_to_mru(_T2, key)
+
 
 def update_after_insert(cache_snapshot, obj):
     '''
-    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object that was just inserted into the cache.
-    - Return: `None`
+    Update ARC state after inserting a new object (on miss).
+    - Default: insert into T1 MRU (recency).
+    - If the key is found in B1 (ghost of T1): increase p and insert into T2 MRU.
+    - If the key is found in B2 (ghost of T2): decrease p and insert into T2 MRU.
+    Ghost lists are trimmed to capacity.
     '''
-    global m_key_timestamp
-    assert obj.key not in m_key_timestamp
-    m_key_timestamp[obj.key] = cache_snapshot.access_count
+    _ensure_capacity(cache_snapshot)
+    key = obj.key
+    global _p
+
+    # If metadata already had it in cache lists, treat as hit
+    if key in _T1:
+        _T1.pop(key, None)
+        _move_to_mru(_T2, key)
+        return
+    if key in _T2:
+        _move_to_mru(_T2, key)
+        return
+
+    # Ghost hits drive adaptation of p
+    if key in _B1:
+        # Favor recency: raise p
+        inc = max(1, len(_B2) // max(1, len(_B1)))
+        _p = min(float(_g_capacity), _p + inc)
+        _B1.pop(key, None)
+        _move_to_mru(_T2, key)
+    elif key in _B2:
+        # Favor frequency: lower p
+        dec = max(1, len(_B1) // max(1, len(_B2)))
+        _p = max(0.0, _p - dec)
+        _B2.pop(key, None)
+        _move_to_mru(_T2, key)
+    else:
+        # First-time insertion: recency path
+        _move_to_mru(_T1, key)
+
+    # Keep ghost lists bounded
+    _ensure_capacity(cache_snapshot)
+
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     '''
-    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object to be inserted into the cache.
-        - `evicted_obj`: The object that was just evicted from the cache.
-    - Return: `None`
+    After evicting a victim from the physical cache, move it into the appropriate ghost list.
+    - If victim was in T1: move to B1.
+    - If victim was in T2: move to B2.
+    - If unknown (desync): place in B1 as conservative default.
+    Ghost lists are trimmed to capacity.
     '''
-    global m_key_timestamp
-    assert obj.key not in m_key_timestamp
-    assert evicted_obj.key in m_key_timestamp
-    m_key_timestamp.pop(evicted_obj.key)
+    _ensure_capacity(cache_snapshot)
+    k = evicted_obj.key
 
+    if k in _T1:
+        _T1.pop(k, None)
+        _move_to_mru(_B1, k)
+    elif k in _T2:
+        _T2.pop(k, None)
+        _move_to_mru(_B2, k)
+    else:
+        # Conservative fallback
+        _move_to_mru(_B1, k)
+
+    # Trim ghosts
+    _ensure_capacity(cache_snapshot)
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate