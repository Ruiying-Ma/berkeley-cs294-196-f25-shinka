<NAME>
arc_tiny_lfu_hybrid
</NAME>

<DESCRIPTION>
I introduce a hybrid ARC + TinyLFU improvement to reduce misses across diverse workloads.

Key ideas:
- Adjust ARC’s target parameter p at the time of eviction decision based on ghost membership (B1/B2), not only at insert time. This makes the replacement decision immediately responsive to workload shifts, aligning with the original ARC algorithm.
- Overlay lightweight TinyLFU frequency estimates to avoid evicting high-frequency items. When both T1 and T2 have candidates, prefer evicting the one with lower estimated frequency; break ties by LRU timestamp. This protects hot objects even under recency pressure.
- Maintain a compact frequency sketch (defaultdict) with periodic exponential decay to keep memory and counts bounded. Counts persist across evictions (as in TinyLFU) to improve admission/retention decisions later.
- Prevent double p-adjustment within a single access by tracking the last adjustment access_count.

Changes:
- Added m_freq and decay logic.
- Adjust arc_p during evict (and still adjust on insert when cache isn’t full) with a guard to avoid double-adjustments.
- Frequency-aware victim selection while preserving ARC’s structure and fallbacks.
- Update frequency counts on hits/inserts.

This yields better adaptability (ARC) plus frequency-aware protection (TinyLFU), improving hit rates on mixed recency/frequency traces.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
from collections import OrderedDict

# LRU timestamp map kept for compatibility and as a tie-breaker
m_key_timestamp = dict()

# Adaptive Replacement Cache (ARC) metadata
arc_T1 = OrderedDict()  # recent, resident
arc_T2 = OrderedDict()  # frequent, resident
arc_B1 = OrderedDict()  # ghost of T1
arc_B2 = OrderedDict()  # ghost of T2
arc_p = 0               # target size of T1
arc_capacity = None     # will be initialized from cache_snapshot
=======
from collections import OrderedDict, defaultdict

# LRU timestamp map kept for compatibility and as a tie-breaker
m_key_timestamp = dict()

# Lightweight TinyLFU-style frequency sketch
m_freq = defaultdict(int)
m_decay_last = 0  # last access_count when decay happened

# Adaptive Replacement Cache (ARC) metadata
arc_T1 = OrderedDict()  # recent, resident
arc_T2 = OrderedDict()  # frequent, resident
arc_B1 = OrderedDict()  # ghost of T1
arc_B2 = OrderedDict()  # ghost of T2
arc_p = 0               # target size of T1
arc_capacity = None     # will be initialized from cache_snapshot
arc_last_adj = -1       # prevent double p-adjustment within same access
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
def _ensure_capacity(cache_snapshot):
    global arc_capacity
    if arc_capacity is None:
        arc_capacity = max(int(cache_snapshot.capacity), 1)
=======
def _ensure_capacity(cache_snapshot):
    global arc_capacity
    if arc_capacity is None:
        arc_capacity = max(int(cache_snapshot.capacity), 1)

def _maybe_decay(cache_snapshot):
    # Periodically decay frequencies to keep counts bounded
    global m_decay_last
    # decay every ~10k accesses
    if cache_snapshot.access_count - m_decay_last >= 10000:
        for k in list(m_freq.keys()):
            cnt = m_freq[k] >> 1
            if cnt <= 0:
                # keep small footprint
                m_freq.pop(k, None)
            else:
                m_freq[k] = cnt
        m_decay_last = cache_snapshot.access_count
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_capacity(cache_snapshot)
    _resync(cache_snapshot)
    # ARC replacement: choose between T1 and T2 depending on arc_p and ghost hit type
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)
    candidate = None
    if t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)):
        # Evict LRU from T1
        candidate = next(iter(arc_T1)) if arc_T1 else None
    else:
        # Evict LRU from T2
        candidate = next(iter(arc_T2)) if arc_T2 else None
    if candidate is None:
        # Fallback: choose the oldest by timestamp if available, else any key
        if m_key_timestamp:
            min_ts = min(m_key_timestamp.get(k, float('inf')) for k in cache_snapshot.cache.keys())
            for k in cache_snapshot.cache.keys():
                if m_key_timestamp.get(k, float('inf')) == min_ts:
                    candidate = k
                    break
        if candidate is None:
            candidate = next(iter(cache_snapshot.cache.keys()))
    return candidate
=======
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global arc_p, arc_last_adj
    _ensure_capacity(cache_snapshot)
    _resync(cache_snapshot)
    _maybe_decay(cache_snapshot)

    # Adjust ARC target p once per access based on ghost membership
    if arc_last_adj != cache_snapshot.access_count:
        if obj.key in arc_B1:
            inc = max(1, len(arc_B2) // max(1, len(arc_B1)))
            arc_p = min(arc_capacity, arc_p + inc)
            arc_last_adj = cache_snapshot.access_count
        elif obj.key in arc_B2:
            dec = max(1, len(arc_B1) // max(1, len(arc_B2)))
            arc_p = max(0, arc_p - dec)
            arc_last_adj = cache_snapshot.access_count

    # ARC replacement: choose between T1 and T2 depending on arc_p and ghost hit type
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)

    # LRU candidates from segments
    cand_t1 = next(iter(arc_T1)) if arc_T1 else None
    cand_t2 = next(iter(arc_T2)) if arc_T2 else None

    # ARC preferred segment
    prefer_t1 = (t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)))

    # Frequency-aware choice: avoid evicting frequent items when both exist
    def _freq(k):
        return m_freq.get(k, 0)

    candidate = None
    if cand_t1 is None and cand_t2 is None:
        # Fallback: choose the oldest by timestamp if available, else any key
        if m_key_timestamp:
            min_ts = min(m_key_timestamp.get(k, float('inf')) for k in cache_snapshot.cache.keys())
            for k in cache_snapshot.cache.keys():
                if m_key_timestamp.get(k, float('inf')) == min_ts:
                    candidate = k
                    break
        if candidate is None:
            candidate = next(iter(cache_snapshot.cache.keys()))
        return candidate

    if cand_t1 is None:
        candidate = cand_t2
    elif cand_t2 is None:
        candidate = cand_t1
    else:
        # Both exist: choose lower frequency; break ties with older timestamp (LRU timestamp map)
        if prefer_t1:
            primary, secondary = cand_t1, cand_t2
        else:
            primary, secondary = cand_t2, cand_t1
        # Compare frequencies
        f_primary = _freq(primary)
        f_secondary = _freq(secondary)
        if f_primary < f_secondary:
            candidate = primary
        elif f_secondary < f_primary:
            candidate = secondary
        else:
            # Tie on frequency: use older timestamp if available, else ARC preference
            ts_primary = m_key_timestamp.get(primary, float('inf'))
            ts_secondary = m_key_timestamp.get(secondary, float('inf'))
            if ts_primary <= ts_secondary:
                candidate = primary
            else:
                candidate = secondary

    return candidate
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp
    _ensure_capacity(cache_snapshot)
    # ARC: on hit, move to T2 MRU
    key = obj.key
    if key in arc_T1:
        arc_T1.pop(key, None)
        _move_to_mru(arc_T2, key)
    else:
        # If already in T2, refresh; if not present due to drift, place in T2
        if key in arc_T2:
            _move_to_mru(arc_T2, key)
        else:
            _move_to_mru(arc_T2, key)
    # Update timestamp for tie-breaking/fallback
    m_key_timestamp[key] = cache_snapshot.access_count
=======
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp
    _ensure_capacity(cache_snapshot)
    _maybe_decay(cache_snapshot)
    # ARC: on hit, move to T2 MRU
    key = obj.key
    if key in arc_T1:
        arc_T1.pop(key, None)
        _move_to_mru(arc_T2, key)
    else:
        # If already in T2, refresh; if not present due to drift, place in T2
        if key in arc_T2:
            _move_to_mru(arc_T2, key)
        else:
            _move_to_mru(arc_T2, key)
    # Update frequency and timestamp for tie-breaking/fallback
    m_freq[key] += 1
    m_key_timestamp[key] = cache_snapshot.access_count
>>>>>>> REPLACE
</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p
    _ensure_capacity(cache_snapshot)
    key = obj.key
    # ARC admission policy
    if key in arc_B1:
        # Previously evicted from T1: favor recency by increasing p
        inc = max(1, len(arc_B2) // max(1, len(arc_B1)))
        arc_p = min(arc_capacity, arc_p + inc)
        arc_B1.pop(key, None)
        _move_to_mru(arc_T2, key)
    elif key in arc_B2:
        # Previously frequent: favor frequency by decreasing p
        dec = max(1, len(arc_B1) // max(1, len(arc_B2)))
        arc_p = max(0, arc_p - dec)
        arc_B2.pop(key, None)
        _move_to_mru(arc_T2, key)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
    _trim_ghosts()
    m_key_timestamp[key] = cache_snapshot.access_count
=======
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p, arc_last_adj
    _ensure_capacity(cache_snapshot)
    _maybe_decay(cache_snapshot)
    key = obj.key
    # ARC admission policy (also handles non-full cache case).
    # Avoid double adjustment within the same access if already done in evict.
    if arc_last_adj != cache_snapshot.access_count and key in arc_B1:
        # Previously evicted from T1: favor recency by increasing p
        inc = max(1, len(arc_B2) // max(1, len(arc_B1)))
        arc_p = min(arc_capacity, arc_p + inc)
        arc_last_adj = cache_snapshot.access_count
        arc_B1.pop(key, None)
        _move_to_mru(arc_T2, key)
    elif arc_last_adj != cache_snapshot.access_count and key in arc_B2:
        # Previously frequent: favor frequency by decreasing p
        dec = max(1, len(arc_B1) // max(1, len(arc_B2)))
        arc_p = max(0, arc_p - dec)
        arc_last_adj = cache_snapshot.access_count
        arc_B2.pop(key, None)
        _move_to_mru(arc_T2, key)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
    _trim_ghosts()
    # Seed a small frequency for new/inserted item
    m_freq[key] += 1
    m_key_timestamp[key] = cache_snapshot.access_count
>>>>>>> REPLACE
</DIFF>