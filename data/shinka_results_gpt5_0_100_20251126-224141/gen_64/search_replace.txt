<NAME>
arc_pre_replace_p_update_and_2c_ghosts
</NAME>

<DESCRIPTION>
I implement two complementary improvements grounded in canonical ARC behavior and practical robustness:

1) Move ghost-driven p adaptation into evict (pre-REPLACE) and guard against double updates. In ARC, when a miss hits B1 or B2, the target size p must be updated before choosing which list to evict from. Previously, p was updated only in update_after_insert, meaning REPLACE decisions were made with a stale p—hurting adaptation to workload changes. I now adjust p inside evict when obj.key ∈ B1 or B2, clamp it, record the access as a ghost hit, and then run REPLACE. To avoid double-stepping when the cache isn't full (evict not called), update_after_insert still performs p updates, but only if evict didn't already do it in the same access (tracked by last_p_update_access).

2) Improve ghost list policy and idle recovery:
- Expand ghost capacity to |B1| + |B2| ≤ 2C and trim proportionally toward targets |B1|≈p and |B2|≈C−p. This keeps stronger, more informative ghost signals and aligns better with ARC’s split.
- Make idle p-decay proportional and bounded to recover faster from scans without oscillation: decay step scales with idle time but is capped to C/8 per call.

Together, these changes reduce scan pollution, improve quick adaptation on phase changes, and retain strong performance on mixed workloads. All updates are localized to the EVOLVE block and consistent with existing metadata.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# Idle tracking for gentle scan recovery
last_ghost_hit_access = -1  # last access_count when B1/B2 was hit
=======
# Idle tracking for gentle scan recovery
last_ghost_hit_access = -1  # last access_count when B1/B2 was hit
# Track when p was last updated to avoid double updates (evict + insert in same access)
last_p_update_access = -1
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def _ensure_capacity(cache_snapshot):
    global arc_capacity
    if arc_capacity is None:
        arc_capacity = max(int(cache_snapshot.capacity), 1)
=======
def _ensure_capacity(cache_snapshot):
    global arc_capacity, arc_p
    if arc_capacity is None:
        arc_capacity = max(int(cache_snapshot.capacity), 1)
    # Clamp p into [0, capacity]
    if arc_capacity is not None:
        arc_p = min(max(arc_p, 0), arc_capacity)
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def _trim_ghosts():
    # Keep ghosts total size within capacity (matches best baseline)
    total = len(arc_B1) + len(arc_B2)
    cap = arc_capacity if arc_capacity is not None else 1
    while total > cap:
        # Evict from the larger ghost list first
        if len(arc_B1) >= len(arc_B2):
            _pop_lru(arc_B1)
        else:
            _pop_lru(arc_B2)
        total = len(arc_B1) + len(arc_B2)
=======
def _trim_ghosts():
    # Keep ghosts total size within 2x capacity and bias trimming toward p split
    cap = arc_capacity if arc_capacity is not None else 1
    # Bound p
    global arc_p
    arc_p = min(max(arc_p, 0), cap)
    total_cap = 2 * cap

    # Proportional targets guided by p within [0, C]
    target_B1 = min(cap, max(0, arc_p))
    target_B2 = max(0, cap - target_B1)

    # Enforce total size first
    while (len(arc_B1) + len(arc_B2)) > total_cap:
        excess_B1 = max(0, len(arc_B1) - target_B1)
        excess_B2 = max(0, len(arc_B2) - target_B2)
        if excess_B1 >= excess_B2 and arc_B1:
            _pop_lru(arc_B1)
        elif arc_B2:
            _pop_lru(arc_B2)
        else:
            # If both within target but total still exceeds (due to rounding), trim larger
            if len(arc_B1) >= len(arc_B2) and arc_B1:
                _pop_lru(arc_B1)
            elif arc_B2:
                _pop_lru(arc_B2)
            else:
                break
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def _decay_p_if_idle(cache_snapshot):
    # If no ghost hits for a while, gently decay p toward 0 to recover from scans
    global arc_p
    if last_ghost_hit_access >= 0:
        idle = cache_snapshot.access_count - last_ghost_hit_access
        if idle > (arc_capacity if arc_capacity else 1) and arc_p > 0:
            arc_p = max(0, arc_p - 1)
=======
def _decay_p_if_idle(cache_snapshot):
    # If no ghost hits for a while, gently and proportionally decay p toward 0
    global arc_p
    if last_ghost_hit_access >= 0:
        idle = cache_snapshot.access_count - last_ghost_hit_access
        if idle > 0 and arc_p > 0:
            cap = arc_capacity if arc_capacity else 1
            # Proportional, bounded decay to hasten recovery after long idle
            step = max(1, idle // max(1, cap // 4))
            arc_p = max(0, arc_p - min(cap // 8, step))
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_capacity(cache_snapshot)
    # Keep metadata consistent first and decay p if idle
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)
    _decay_p_if_idle(cache_snapshot)

    # ARC replacement: choose between T1 and T2 depending on arc_p and ghost hit type
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)
    from_t1 = (t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)))

    # Primary choice
    if from_t1 and arc_T1:
        return next(iter(arc_T1))
    if (not from_t1) and arc_T2:
        return next(iter(arc_T2))
    # If preferred list empty, try the other
    if arc_T1:
        return next(iter(arc_T1))
    if arc_T2:
        return next(iter(arc_T2))

    # Try to repair metadata and retry ARC replacement before falling back
    _resync(cache_snapshot)
    t1_sz = len(arc_T1)
    from_t1 = (t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)))
    if from_t1 and arc_T1:
        return next(iter(arc_T1))
    if (not from_t1) and arc_T2:
        return next(iter(arc_T2))
    if arc_T1:
        return next(iter(arc_T1))
    if arc_T2:
        return next(iter(arc_T2))

    # Ghost-informed fallback: prefer evicting something present in B1 (recency-only)
    for k in cache_snapshot.cache.keys():
        if k in arc_B1:
            return k
    # Next: prefer any key not in B2 (avoid evicting likely frequent)
    for k in cache_snapshot.cache.keys():
        if k not in arc_B2:
            return k
    # Otherwise, timestamp tie-breaker
    if m_key_timestamp:
        min_ts = min(m_key_timestamp.get(k, float('inf')) for k in cache_snapshot.cache.keys())
        for k in cache_snapshot.cache.keys():
            if m_key_timestamp.get(k, float('inf')) == min_ts:
                return k
    # Last resort: arbitrary
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache.keys()))
    return None
=======
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global arc_p, last_ghost_hit_access, last_p_update_access
    _ensure_capacity(cache_snapshot)
    # Keep metadata consistent first and decay p if idle
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)
    _decay_p_if_idle(cache_snapshot)

    # If the request is a ghost hit, update p before REPLACE (canonical ARC)
    step_cap = max(1, arc_capacity // 8)
    key = obj.key
    if key in arc_B1:
        # Favor recency: increase p
        inc = max(1, len(arc_B2) // max(1, len(arc_B1)))
        inc = min(inc, step_cap, max(0, arc_capacity - arc_p))
        arc_p = min(arc_capacity, arc_p + inc)
        last_ghost_hit_access = cache_snapshot.access_count
        last_p_update_access = cache_snapshot.access_count
    elif key in arc_B2:
        # Favor frequency: decrease p
        dec = max(1, len(arc_B1) // max(1, len(arc_B2)))
        dec = min(dec, step_cap, arc_p)
        arc_p = max(0, arc_p - dec)
        last_ghost_hit_access = cache_snapshot.access_count
        last_p_update_access = cache_snapshot.access_count

    # ARC replacement: choose between T1 and T2 depending on arc_p and ghost hit type
    x_in_B2 = key in arc_B2
    t1_sz = len(arc_T1)
    from_t1 = (t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)))

    # Primary choice
    if from_t1 and arc_T1:
        candidate = next(iter(arc_T1))
    elif (not from_t1) and arc_T2:
        candidate = next(iter(arc_T2))
    elif arc_T1:
        candidate = next(iter(arc_T1))
    elif arc_T2:
        candidate = next(iter(arc_T2))
    else:
        candidate = None

    # Try to repair metadata and retry ARC replacement before falling back
    if candidate is None or candidate not in cache_snapshot.cache:
        _resync(cache_snapshot)
        t1_sz = len(arc_T1)
        from_t1 = (t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)))
        if from_t1 and arc_T1:
            candidate = next(iter(arc_T1))
        elif arc_T2:
            candidate = next(iter(arc_T2))

    # Ghost-informed fallback: prefer evicting something present in B1 (recency-only)
    if candidate is None or candidate not in cache_snapshot.cache:
        for k in cache_snapshot.cache.keys():
            if k in arc_B1:
                candidate = k
                break
    # Next: prefer any key not in B2 (avoid evicting likely frequent)
    if (candidate is None or candidate not in cache_snapshot.cache) and cache_snapshot.cache:
        for k in cache_snapshot.cache.keys():
            if k not in arc_B2:
                candidate = k
                break
    # Otherwise, timestamp tie-breaker
    if (candidate is None or candidate not in cache_snapshot.cache) and cache_snapshot.cache:
        if m_key_timestamp:
            min_ts = min(m_key_timestamp.get(k, float('inf')) for k in cache_snapshot.cache.keys())
            for k in cache_snapshot.cache.keys():
                if m_key_timestamp.get(k, float('inf')) == min_ts:
                    candidate = k
                    break
    # Last resort: arbitrary
    if (candidate is None or candidate not in cache_snapshot.cache) and cache_snapshot.cache:
        candidate = next(iter(cache_snapshot.cache.keys()))
    return candidate
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p, last_ghost_hit_access
    _ensure_capacity(cache_snapshot)
    _decay_p_if_idle(cache_snapshot)

    key = obj.key
    # ARC admission policy with bounded p updates
    step_cap = max(1, arc_capacity // 8)
    if key in arc_B1:
        # Previously evicted from T1: favor recency by increasing p
        inc = max(1, len(arc_B2) // max(1, len(arc_B1)))
        inc = min(inc, step_cap, max(0, arc_capacity - arc_p))
        arc_p = min(arc_capacity, arc_p + inc)
        last_ghost_hit_access = cache_snapshot.access_count
        arc_B1.pop(key, None)
        arc_B2.pop(key, None)  # keep ghosts disjoint
        _move_to_mru(arc_T2, key)
    elif key in arc_B2:
        # Previously frequent: favor frequency by decreasing p
        dec = max(1, len(arc_B1) // max(1, len(arc_B2)))
        dec = min(dec, step_cap, arc_p)
        arc_p = max(0, arc_p - dec)
        last_ghost_hit_access = cache_snapshot.access_count
        arc_B2.pop(key, None)
        arc_B1.pop(key, None)
        _move_to_mru(arc_T2, key)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
        # Ensure ghosts are disjoint from residents
        arc_B1.pop(key, None)
        arc_B2.pop(key, None)

    _trim_ghosts()
    m_key_timestamp[key] = cache_snapshot.access_count

    # Post-condition: keep metadata consistent
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)
=======
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p, last_ghost_hit_access, last_p_update_access
    _ensure_capacity(cache_snapshot)
    _decay_p_if_idle(cache_snapshot)

    key = obj.key
    # ARC admission policy with bounded p updates
    step_cap = max(1, arc_capacity // 8)

    if key in arc_B1 or key in arc_B2:
        # If evict already adapted p this access, don't double-step
        if last_p_update_access != cache_snapshot.access_count:
            if key in arc_B1:
                inc = max(1, len(arc_B2) // max(1, len(arc_B1)))
                inc = min(inc, step_cap, max(0, arc_capacity - arc_p))
                arc_p = min(arc_capacity, arc_p + inc)
            else:
                dec = max(1, len(arc_B1) // max(1, len(arc_B2)))
                dec = min(dec, step_cap, arc_p)
                arc_p = max(0, arc_p - dec)
            last_ghost_hit_access = cache_snapshot.access_count
            last_p_update_access = cache_snapshot.access_count
        # Admission to T2 for ghost reuses
        arc_B1.pop(key, None)
        arc_B2.pop(key, None)
        _move_to_mru(arc_T2, key)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
        # Ensure ghosts are disjoint from residents
        arc_B1.pop(key, None)
        arc_B2.pop(key, None)

    _trim_ghosts()
    m_key_timestamp[key] = cache_snapshot.access_count

    # Post-condition: keep metadata consistent
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)
>>>>>>> REPLACE

</DIFF>