<NAME>
arc_adaptive_p_decay_delayed_promotion
</NAME>

<DESCRIPTION>
I evolve the simplified ARC implementation by adding three targeted improvements that reduce miss rates across diverse workloads:

1) Expand and smarter-trim ghost lists: Increase total ghost capacity to 2Ã—cache_capacity and trim by evicting the oldest from the larger ghost list first. This preserves more eviction history to better steer p on mixed workloads without large memory overhead.

2) Damped and decaying control of target p: Cap ghost-driven p step sizes to at most capacity/8 and introduce decay toward 0 when no ghost hits have occurred for capacity accesses. Additionally, add a scan clamp: when a long cold-miss streak occurs with no ghost signals, reduce p by capacity/4. These controls reduce overshoot, improve recovery after scans, and stabilize behavior.

3) Delayed promotion to T2: Instead of promoting to T2 on the first hit in T1, require a second hit within a short window (W = capacity/4 accesses). This reduces over-protection and T2 pollution, retaining more recency in T1 during mixed and scan-heavy traces.

I also reset cold_streak on hits, track the last ghost-hit time, and clean up pending-hit metadata on evictions and inserts. The eviction decision remains ARC-like with a consistent fallback to global LRU. Together, these changes enhance adaptability and robustness, improving the hit rate on challenging workloads while keeping the algorithm lightweight.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
"""Cache eviction algorithm for optimizing hit rates across multiple workloads
Simplified ARC:
- Resident sets: T1 (recent), T2 (frequent).
- Ghost sets: B1 (recently evicted from T1), B2 (recently evicted from T2).
- Adaptive target p controls desired size of T1.
"""

# Resident metadata
# - m_key_timestamp: last access time for each resident key
# - m_key_segment: 't1' (recent) or 't2' (frequent) for resident keys
m_key_timestamp = dict()
m_key_segment = dict()

# Ghost metadata (key -> last timestamp when it entered ghost)
m_ghost_b1_ts = dict()
m_ghost_b2_ts = dict()

# Adaptive target for T1 size
m_target_p = None


def _cap(cache_snapshot):
    try:
        return int(cache_snapshot.capacity)
    except Exception:
        return max(1, len(cache_snapshot.cache))


def _ensure_init(cache_snapshot):
    global m_target_p
    if m_target_p is None:
        m_target_p = max(1, _cap(cache_snapshot) // 2)


def _resident_sets(cache_snapshot):
    """Return (t1_keys, t2_keys) among current cache keys, using metadata."""
    cache_keys = set(cache_snapshot.cache.keys())
    t1_keys = []
    t2_keys = []
    for k in cache_keys:
        seg = m_key_segment.get(k, 't1')
        if seg == 't2':
            t2_keys.append(k)
        else:
            # default everything unknown to t1
            if seg not in ('t1', 't2'):
                m_key_segment[k] = 't1'
            t1_keys.append(k)
    return t1_keys, t2_keys


def _lru_key(keys):
    """Return LRU key among `keys` using m_key_timestamp; None if empty."""
    if not keys:
        return None
    return min(keys, key=lambda k: m_key_timestamp.get(k, float('inf')))


def _prune_ghosts(cache_snapshot):
    """Keep total ghost size <= capacity by discarding oldest ghosts."""
    total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
    cap = _cap(cache_snapshot)
    while total > cap:
        # Evict the oldest across both ghosts
        b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get) if m_ghost_b1_ts else None
        b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get) if m_ghost_b2_ts else None
        if b1_old_key is None and b2_old_key is None:
            break
        if b2_old_key is None or (b1_old_key is not None and m_ghost_b1_ts[b1_old_key] <= m_ghost_b2_ts.get(b2_old_key, float('inf'))):
            m_ghost_b1_ts.pop(b1_old_key, None)
        else:
            m_ghost_b2_ts.pop(b2_old_key, None)
        total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_init(cache_snapshot)
    t1_keys, t2_keys = _resident_sets(cache_snapshot)

    # ARC-like replace() rule:
    # Prefer evicting from T1 when:
    # - T1 is larger than target p, or
    # - The incoming key is in B2 and T1 is at target (to reduce T1), or
    # - T2 is empty (nothing protected yet).
    evict_from_t1 = False
    if len(t1_keys) > m_target_p:
        evict_from_t1 = True
    elif obj.key in m_ghost_b2_ts and len(t1_keys) >= max(1, m_target_p):
        evict_from_t1 = True
    elif not t2_keys and t1_keys:
        evict_from_t1 = True

    if evict_from_t1 and t1_keys:
        victim = _lru_key(t1_keys)
        if victim is not None:
            return victim

    # Otherwise evict LRU from T2 if available
    if t2_keys:
        victim = _lru_key(t2_keys)
        if victim is not None:
            return victim

    # Fallback: global LRU
    all_keys = list(cache_snapshot.cache.keys())
    if not all_keys:
        return None
    return _lru_key(all_keys)


def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment
    _ensure_init(cache_snapshot)

    # Refresh recency
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Promote from T1 to T2 on a hit (frequency signal)
    seg = m_key_segment.get(obj.key, 't1')
    if seg != 't2':
        m_key_segment[obj.key] = 't2'


def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_target_p
    _ensure_init(cache_snapshot)

    # ARC adaptation: if we see a miss on a key present in a ghost list,
    # adjust target p and insert into T2.
    seg = 't1'
    if obj.key in m_ghost_b1_ts:
        # Increase p (give more room to recency) proportional to |B2|/|B1|
        inc = max(1, len(m_ghost_b2_ts) // max(1, len(m_ghost_b1_ts)))
        m_target_p = min(_cap(cache_snapshot), m_target_p + inc)
        m_ghost_b1_ts.pop(obj.key, None)
        seg = 't2'
    elif obj.key in m_ghost_b2_ts:
        # Decrease p (favor frequency) proportional to |B1|/|B2|
        dec = max(1, len(m_ghost_b1_ts) // max(1, len(m_ghost_b2_ts)))
        m_target_p = max(0, m_target_p - dec)
        m_ghost_b2_ts.pop(obj.key, None)
        seg = 't2'

    # Insert into resident set
    m_key_segment[obj.key] = seg
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Control ghost size
    _prune_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_ghost_b1_ts, m_ghost_b2_ts
    _ensure_init(cache_snapshot)

    # Remove resident metadata for evicted key
    seg = m_key_segment.pop(evicted_obj.key, 't1')
    m_key_timestamp.pop(evicted_obj.key, None)

    # Add to corresponding ghost list with current time to maintain LRU
    ts = cache_snapshot.access_count
    if seg == 't2':
        m_ghost_b2_ts[evicted_obj.key] = ts
    else:
        m_ghost_b1_ts[evicted_obj.key] = ts

    # Control ghost size
    _prune_ghosts(cache_snapshot)

# EVOLVE-BLOCK-END
=======
# EVOLVE-BLOCK-START
"""Cache eviction algorithm for optimizing hit rates across multiple workloads
Simplified ARC with enhancements:
- Resident sets: T1 (recent), T2 (frequent).
- Ghost sets: B1 (recently evicted from T1), B2 (recently evicted from T2).
- Adaptive target p controls desired size of T1 with damping, decay and scan clamp.
- Delayed promotion to T2 to reduce over-protection.
"""

# Resident metadata
# - m_key_timestamp: last access time for each resident key
# - m_key_segment: 't1' (recent) or 't2' (frequent) for resident keys
m_key_timestamp = dict()
m_key_segment = dict()

# Ghost metadata (key -> last timestamp when it entered ghost)
m_ghost_b1_ts = dict()
m_ghost_b2_ts = dict()

# Adaptive target for T1 size
m_target_p = None

# Additional controls
m_last_ghost_hit_access = None
m_cold_streak = 0  # count of consecutive cold misses (not in ghosts)
m_t1_pending_hits = dict()  # key -> last hit access_count while in T1


def _cap(cache_snapshot):
    try:
        return int(cache_snapshot.capacity)
    except Exception:
        return max(1, len(cache_snapshot.cache))


def _ensure_init(cache_snapshot):
    global m_target_p, m_last_ghost_hit_access, m_cold_streak
    if m_target_p is None:
        m_target_p = max(1, _cap(cache_snapshot) // 2)
    if m_last_ghost_hit_access is None:
        m_last_ghost_hit_access = cache_snapshot.access_count
    if m_cold_streak is None:
        m_cold_streak = 0


def _resident_sets(cache_snapshot):
    """Return (t1_keys, t2_keys) among current cache keys, using metadata."""
    cache_keys = set(cache_snapshot.cache.keys())
    t1_keys = []
    t2_keys = []
    for k in cache_keys:
        seg = m_key_segment.get(k, 't1')
        if seg == 't2':
            t2_keys.append(k)
        else:
            # default everything unknown to t1
            if seg not in ('t1', 't2'):
                m_key_segment[k] = 't1'
            t1_keys.append(k)
    return t1_keys, t2_keys


def _lru_key(keys):
    """Return LRU key among `keys` using m_key_timestamp; None if empty."""
    if not keys:
        return None
    return min(keys, key=lambda k: m_key_timestamp.get(k, float('inf')))


def _prune_ghosts(cache_snapshot):
    """Keep total ghost size <= 2*capacity; evict oldest from the larger ghost list first."""
    total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
    cap = _cap(cache_snapshot)
    limit = max(1, 2 * cap)
    while total > limit:
        # Prefer trimming the larger ghost list to maintain balance
        if len(m_ghost_b1_ts) >= len(m_ghost_b2_ts):
            if m_ghost_b1_ts:
                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
                m_ghost_b1_ts.pop(b1_old_key, None)
            elif m_ghost_b2_ts:
                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
                m_ghost_b2_ts.pop(b2_old_key, None)
        else:
            if m_ghost_b2_ts:
                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
                m_ghost_b2_ts.pop(b2_old_key, None)
            elif m_ghost_b1_ts:
                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
                m_ghost_b1_ts.pop(b1_old_key, None)
        total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_init(cache_snapshot)
    t1_keys, t2_keys = _resident_sets(cache_snapshot)

    # ARC-like replace() rule:
    # Prefer evicting from T1 when:
    # - T1 is larger than target p, or
    # - The incoming key is in B2 and T1 is at target (to reduce T1), or
    # - T2 is empty (nothing protected yet).
    evict_from_t1 = False
    if len(t1_keys) > m_target_p:
        evict_from_t1 = True
    elif obj.key in m_ghost_b2_ts and len(t1_keys) >= max(1, m_target_p):
        evict_from_t1 = True
    elif not t2_keys and t1_keys:
        evict_from_t1 = True

    if evict_from_t1 and t1_keys:
        victim = _lru_key(t1_keys)
        if victim is not None:
            return victim

    # Otherwise evict LRU from T2 if available
    if t2_keys:
        victim = _lru_key(t2_keys)
        if victim is not None:
            return victim

    # Fallback: global LRU
    all_keys = list(cache_snapshot.cache.keys())
    if not all_keys:
        return None
    return _lru_key(all_keys)


def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_t1_pending_hits, m_cold_streak, m_target_p, m_last_ghost_hit_access
    _ensure_init(cache_snapshot)

    # Refresh recency
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Reset cold streak on any hit
    m_cold_streak = 0

    # Slow decay of p if no ghost hits for a while (scan recovery)
    cap = _cap(cache_snapshot)
    if cache_snapshot.access_count - m_last_ghost_hit_access > cap:
        m_target_p = max(0, m_target_p - 1)

    # Delayed promotion: require two hits within a short window while in T1
    seg = m_key_segment.get(obj.key, 't1')
    if seg == 't2':
        return
    # In T1
    W = max(1, cap // 4)
    last = m_t1_pending_hits.get(obj.key)
    if last is not None and cache_snapshot.access_count - last <= W:
        m_key_segment[obj.key] = 't2'
        m_t1_pending_hits.pop(obj.key, None)
    else:
        m_t1_pending_hits[obj.key] = cache_snapshot.access_count


def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_target_p, m_last_ghost_hit_access, m_cold_streak, m_t1_pending_hits
    _ensure_init(cache_snapshot)

    cap = _cap(cache_snapshot)
    step_cap = max(1, cap // 8)

    in_b1 = obj.key in m_ghost_b1_ts
    in_b2 = obj.key in m_ghost_b2_ts

    seg = 't1'
    if in_b1:
        # Increase p (give more room to recency), damped
        raw = max(1, len(m_ghost_b2_ts) // max(1, len(m_ghost_b1_ts)))
        inc = min(step_cap, raw)
        m_target_p = min(cap, m_target_p + inc)
        m_ghost_b1_ts.pop(obj.key, None)
        seg = 't2'
        m_last_ghost_hit_access = cache_snapshot.access_count
        m_cold_streak = 0
    elif in_b2:
        # Decrease p (favor frequency), damped
        raw = max(1, len(m_ghost_b1_ts) // max(1, len(m_ghost_b2_ts)))
        dec = min(step_cap, raw)
        m_target_p = max(0, m_target_p - dec)
        m_ghost_b2_ts.pop(obj.key, None)
        seg = 't2'
        m_last_ghost_hit_access = cache_snapshot.access_count
        m_cold_streak = 0
    else:
        # Cold miss - track streak and apply scan clamp if needed
        m_cold_streak += 1
        if m_cold_streak > cap:
            m_target_p = max(0, m_target_p - max(1, cap // 4))
            m_cold_streak = 0

    # Insert into resident set
    m_key_segment[obj.key] = seg
    m_key_timestamp[obj.key] = cache_snapshot.access_count
    # Clear any stale pending state for this key
    m_t1_pending_hits.pop(obj.key, None)

    # Control ghost size (expanded history)
    _prune_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_ghost_b1_ts, m_ghost_b2_ts, m_t1_pending_hits
    _ensure_init(cache_snapshot)

    # Remove resident metadata for evicted key
    seg = m_key_segment.pop(evicted_obj.key, 't1')
    m_key_timestamp.pop(evicted_obj.key, None)
    m_t1_pending_hits.pop(evicted_obj.key, None)

    # Add to corresponding ghost list with current time to maintain LRU
    ts = cache_snapshot.access_count
    if seg == 't2':
        m_ghost_b2_ts[evicted_obj.key] = ts
    else:
        m_ghost_b1_ts[evicted_obj.key] = ts

    # Control ghost size (expanded history)
    _prune_ghosts(cache_snapshot)

# EVOLVE-BLOCK-END
>>>>>>> REPLACE
</DIFF>