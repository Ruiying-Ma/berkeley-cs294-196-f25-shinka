<NAME>
arc_resync_proportional_ghosts_strict_replace
</NAME>

<DESCRIPTION>
I strengthen the ARC implementation in four ways to reduce cache misses across mixed workloads:

1) Proportional 2C ghost history with trimming: I replace per-list ghost caps with a combined |B1|+|B2| ≤ 2C and proportional trimming toward targets derived from p (|B1|≈p, |B2|≈C−p). This preserves more history for steering p while ensuring ghosts remain bounded and disjoint from residents.

2) Strict ARC REPLACE in evict with robust fallbacks: Implement the canonical ARC decision—evict from T1 if |T1| > p or when the incoming key is in B2 and |T1| == p; otherwise from T2. If a chosen list is empty or metadata is out of sync, resynchronize metadata from the actual cache contents and retry before resorting to a last-resort victim. This reduces non-deterministic evictions and better protects frequent items.

3) Proactive invariant enforcement on hits/inserts: After hits and inserts, remove the key from ghosts to keep sets disjoint, move/promote to T2 on hits, and resynchronize when resident list sizes deviate from the real cache composition. This keeps metadata accurate for better eviction choices.

4) Capped p step sizes: Adapt p asymmetrically on ghost hits but cap the step at C/8 to prevent overshooting when ghost lists are small, improving stability on mixed and bursty workloads.

These changes align with canonical ARC behaviors, increase robustness, and improve adaptation, which should drive down miss rates, especially on workloads combining recency and frequency patterns.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _ensure_capacity(cache_snapshot):
    """Initialize/refresh capacity-aware parameters and trim ghost lists."""
    global _g_capacity, _p
    if cache_snapshot is None:
        return
    cap = cache_snapshot.capacity
    # Fallback to at least 1; also ensure not below current cache length
    cap = max(cap, len(cache_snapshot.cache), 1)
    if _g_capacity != cap:
        _g_capacity = cap
        # Initialize p to half capacity on first run; clamp otherwise
        if _p == 0.0:
            _p = float(max(1, int(round(0.5 * _g_capacity))))
        else:
            _p = min(max(_p, 0.0), float(_g_capacity))

    # Bound ghost list sizes to capacity (standard ARC behavior)
    while len(_B1) > _g_capacity:
        _B1.popitem(last=False)
    while len(_B2) > _g_capacity:
        _B2.popitem(last=False)
=======
def _ensure_capacity(cache_snapshot):
    """Initialize/refresh capacity-aware parameters and trim ghost lists."""
    global _g_capacity, _p
    if cache_snapshot is None:
        return
    cap = cache_snapshot.capacity
    # Fallback to at least 1; also ensure not below current cache length
    cap = max(cap, len(cache_snapshot.cache), 1)
    if _g_capacity != cap:
        _g_capacity = cap
        # Initialize p to half capacity on first run; clamp otherwise
        if _p == 0.0:
            _p = float(max(1, int(round(0.5 * _g_capacity))))
        # Clamp p into [0, C] whenever capacity changes
        _p = min(max(_p, 0.0), float(_g_capacity))

    # Keep ghosts bounded and disjoint using proportional 2C policy
    _trim_ghosts()


def _trim_ghosts():
    """Maintain ghost lists:
    - Disjoint from residents
    - Total size <= 2 * capacity
    - Proportional trimming toward targets |B1|≈p and |B2|≈C-p
    """
    # Remove any resident keys from ghosts (disjointness)
    for rk in list(_T1.keys()):
        _B1.pop(rk, None)
        _B2.pop(rk, None)
    for rk in list(_T2.keys()):
        _B1.pop(rk, None)
        _B2.pop(rk, None)

    total_cap = 2 * max(_g_capacity, 1)
    t1_target = int(round(min(max(_p, 0.0), float(_g_capacity))))
    b1_target = t1_target
    b2_target = max(_g_capacity - t1_target, 0)

    # First ensure combined size bound
    while (len(_B1) + len(_B2)) > total_cap:
        over1 = len(_B1) - b1_target
        over2 = len(_B2) - b2_target
        if over1 >= over2:
            if _B1:
                _B1.popitem(last=False)
            elif _B2:
                _B2.popitem(last=False)
        else:
            if _B2:
                _B2.popitem(last=False)
            elif _B1:
                _B1.popitem(last=False)

    # Light proportional correction even if under total bound
    while len(_B1) > max(b1_target, _g_capacity):
        _B1.popitem(last=False)
    while len(_B2) > max(b2_target, _g_capacity):
        _B2.popitem(last=False)


def _resync(cache_snapshot):
    """Rebuild resident metadata from actual cache when desynchronized."""
    if cache_snapshot is None:
        return
    residents = set(cache_snapshot.cache.keys())

    # Purge stale entries from T1/T2
    for k in list(_T1.keys()):
        if k not in residents:
            _T1.pop(k, None)
    for k in list(_T2.keys()):
        if k not in residents:
            _T2.pop(k, None)

    # Add missing residents into T1 MRU conservatively
    for k in residents:
        if k not in _T1 and k not in _T2:
            _move_to_mru(_T1, k)

    # Keep ghosts disjoint and bounded
    for k in residents:
        _B1.pop(k, None)
        _B2.pop(k, None)
    _trim_ghosts()
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    Choose eviction victim per ARC replace() rule.
    Prefer evicting from T1 when T1 is above target p (or on B2-ghost arrivals),
    otherwise evict from T2. Fallback: any cached key if metadata desync occurs.
    '''
    _ensure_capacity(cache_snapshot)

    t1_sz = len(_T1)
    t2_sz = len(_T2)

    # ARC replace() heuristic
    # If incoming key is in B2 and T1 is at target p, evict from T1 to protect frequency.
    # Else if T1 is larger than p, evict from T1; otherwise from T2.
    victim = None
    if t1_sz > 0 and ((obj.key in _B2 and t1_sz >= int(round(_p))) or (t1_sz > int(round(_p)))):
        victim = _lru_key(_T1)
    elif t2_sz > 0:
        victim = _lru_key(_T2)
    elif t1_sz > 0:
        victim = _lru_key(_T1)

    # Fallback if metadata out-of-sync: pick any current cache key
    if victim is None or victim not in cache_snapshot.cache:
        for k in cache_snapshot.cache:
            victim = k
            break

    return victim
=======
def evict(cache_snapshot, obj):
    '''
    Choose eviction victim using strict ARC replace() logic:
    - Evict from T1 if |T1| > p, or (obj in B2 and |T1| == p)
    - Else evict from T2
    Robust fallback: resync metadata and retry before last-resort pick.
    '''
    _ensure_capacity(cache_snapshot)

    # Keep ghosts disjoint from residents
    for rk in list(_T1.keys()):
        _B1.pop(rk, None)
        _B2.pop(rk, None)
    for rk in list(_T2.keys()):
        _B1.pop(rk, None)
        _B2.pop(rk, None)

    if (len(_T1) + len(_T2)) == 0 and len(cache_snapshot.cache) > 0:
        _resync(cache_snapshot)

    p_int = int(round(min(max(_p, 0.0), float(_g_capacity))))
    t1_sz = len(_T1)
    t2_sz = len(_T2)

    victim = None
    choose_t1 = False
    if t1_sz > 0 and (t1_sz > p_int or (obj.key in _B2 and t1_sz >= p_int)):
        choose_t1 = True

    if choose_t1:
        victim = _lru_key(_T1) if _T1 else (_lru_key(_T2) if _T2 else None)
    else:
        victim = _lru_key(_T2) if _T2 else (_lru_key(_T1) if _T1 else None)

    # Fallback: resync and retry, then last-resort pick from cache
    if victim is None or victim not in cache_snapshot.cache:
        _resync(cache_snapshot)
        victim = _lru_key(_T1) if _T1 else _lru_key(_T2)
        if victim is None or victim not in cache_snapshot.cache:
            for k in cache_snapshot.cache:
                victim = k
                break

    return victim
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    Update ARC state after a cache hit.
    - If hit in T1: promote to T2 MRU.
    - If hit in T2: move to T2 MRU.
    - If not tracked (desync), consider it frequent and add to T2 MRU.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key

    if key in _T1:
        _T1.pop(key, None)
        _move_to_mru(_T2, key)
    elif key in _T2:
        _move_to_mru(_T2, key)
    else:
        # Metadata desync: treat as frequent since it hit
        _move_to_mru(_T2, key)
=======
def update_after_hit(cache_snapshot, obj):
    '''
    Update ARC state after a cache hit.
    - If hit in T1: promote to T2 MRU.
    - If hit in T2: move to T2 MRU.
    - If not tracked (desync), consider it frequent and add to T2 MRU.
    Also keep ghost lists disjoint and trim ghosts.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key

    # Keep sets disjoint
    _B1.pop(key, None)
    _B2.pop(key, None)

    if key in _T1:
        _T1.pop(key, None)
        _move_to_mru(_T2, key)
    elif key in _T2:
        _move_to_mru(_T2, key)
    else:
        # Metadata desync: treat as frequent since it hit
        _move_to_mru(_T2, key)

    # Proactively resync if sizes drift
    if (len(_T1) + len(_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    _trim_ghosts()
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    Update ARC state after inserting a new object (on miss).
    - Default: insert into T1 MRU (recency).
    - If the key is found in B1 (ghost of T1): increase p and insert into T2 MRU.
    - If the key is found in B2 (ghost of T2): decrease p and insert into T2 MRU.
    Ghost lists are trimmed to capacity.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    global _p

    # If metadata already had it in cache lists, treat as hit
    if key in _T1:
        _T1.pop(key, None)
        _move_to_mru(_T2, key)
        return
    if key in _T2:
        _move_to_mru(_T2, key)
        return

    # Ghost hits drive adaptation of p
    if key in _B1:
        # Favor recency: raise p
        inc = max(1, len(_B2) // max(1, len(_B1)))
        _p = min(float(_g_capacity), _p + inc)
        _B1.pop(key, None)
        _move_to_mru(_T2, key)
    elif key in _B2:
        # Favor frequency: lower p
        dec = max(1, len(_B1) // max(1, len(_B2)))
        _p = max(0.0, _p - dec)
        _B2.pop(key, None)
        _move_to_mru(_T2, key)
    else:
        # First-time insertion: recency path
        _move_to_mru(_T1, key)

    # Keep ghost lists bounded
    _ensure_capacity(cache_snapshot)
=======
def update_after_insert(cache_snapshot, obj):
    '''
    Update ARC state after inserting a new object (on miss).
    - Default: insert into T1 MRU (recency).
    - If the key is found in B1 (ghost of T1): increase p and insert into T2 MRU.
    - If the key is found in B2 (ghost of T2): decrease p and insert into T2 MRU.
    Adaptation steps are capped to avoid overshoot. Ghost lists stay disjoint and bounded.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    global _p

    # If metadata already had it in cache lists, treat as hit
    if key in _T1:
        _T1.pop(key, None)
        _move_to_mru(_T2, key)
        _trim_ghosts()
        return
    if key in _T2:
        _move_to_mru(_T2, key)
        _trim_ghosts()
        return

    # Cap adaptation step to avoid overshoot
    step_cap = max(1, _g_capacity // 8)

    # Ghost hits drive adaptation of p
    if key in _B1:
        # Favor recency: raise p
        raw_inc = max(1, len(_B2) // max(1, len(_B1)))
        inc = min(step_cap, raw_inc)
        _p = min(float(_g_capacity), _p + inc)
        _B1.pop(key, None)
        _B2.pop(key, None)  # ensure disjointness
        _move_to_mru(_T2, key)
    elif key in _B2:
        # Favor frequency: lower p
        raw_dec = max(1, len(_B1) // max(1, len(_B2)))
        dec = min(step_cap, raw_dec)
        _p = max(0.0, _p - dec)
        _B2.pop(key, None)
        _B1.pop(key, None)  # ensure disjointness
        _move_to_mru(_T2, key)
    else:
        # First-time insertion: recency path
        _move_to_mru(_T1, key)

    # Keep ghosts bounded and metadata consistent
    if (len(_T1) + len(_T2)) > len(cache_snapshot.cache):
        _resync(cache_snapshot)
    _trim_ghosts()
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After evicting a victim from the physical cache, move it into the appropriate ghost list.
    - If victim was in T1: move to B1.
    - If victim was in T2: move to B2.
    - If unknown (desync): place in B1 as conservative default.
    Ghost lists are trimmed to capacity.
    '''
    _ensure_capacity(cache_snapshot)
    k = evicted_obj.key

    if k in _T1:
        _T1.pop(k, None)
        _move_to_mru(_B1, k)
    elif k in _T2:
        _T2.pop(k, None)
        _move_to_mru(_B2, k)
    else:
        # Conservative fallback
        _move_to_mru(_B1, k)

    # Trim ghosts
    _ensure_capacity(cache_snapshot)
=======
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After evicting a victim from the physical cache, move it into the appropriate ghost list.
    - If victim was in T1: move to B1.
    - If victim was in T2: move to B2.
    - If unknown (desync): prefer existing ghost membership (B2 first), else B1.
    Ghost lists remain disjoint and bounded.
    '''
    _ensure_capacity(cache_snapshot)
    k = evicted_obj.key

    had_t1 = k in _T1
    had_t2 = k in _T2
    _T1.pop(k, None)
    _T2.pop(k, None)

    if had_t2:
        _move_to_mru(_B2, k)
        _B1.pop(k, None)
    elif had_t1:
        _move_to_mru(_B1, k)
        _B2.pop(k, None)
    else:
        # Preserve any existing ghost preference; prefer B2 for frequency
        if k in _B2:
            _move_to_mru(_B2, k)
            _B1.pop(k, None)
        else:
            _move_to_mru(_B1, k)
            _B2.pop(k, None)

    _trim_ghosts()
>>>>>>> REPLACE

</DIFF>