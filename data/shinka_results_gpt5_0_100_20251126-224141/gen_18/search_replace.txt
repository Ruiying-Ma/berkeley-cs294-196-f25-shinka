<NAME>
arc_slru_ghost_adapt
</NAME>

<DESCRIPTION>
I introduce two lightweight ARC-style ghost lists (B1 for keys evicted from probationary and B2 for keys evicted from protected) and adapt the protected segment size dynamically based on ghost hits. This preserves SLRU’s scan resistance while adding ARC’s feedback mechanism to balance recency vs frequency online.

Key changes:
- Add m_B1_ts/m_B2_ts ghost histories with bounded size and timestamps.
- Maintain m_prot_target_count (target number of protected items) initialized from _PROTECTED_RATIO, adapted online:
  - B1 ghost hit => decrease protected target (give more room to probation) – akin to ARC’s increase of T1.
  - B2 ghost hit => increase protected target (favor protection) – akin to ARC’s increase of T2.
  - Step changes are bounded by capacity//8 and clamped to [0, cap-1].
- Promote re-referenced ghost hits directly to protected (frequent) on insert, mirroring ARC.
- Modify enforcement to demote protected’s LRU until its size is within the adaptive target.
- In evict, if protected is oversized vs target, choose its LRU as victim; otherwise prefer probationary LRU. This prevents starvation of probation during recency-heavy scans and keeps protected from drifting too large.

This hybrid keeps the simple, robust SLRU core and adds just-enough adaptivity and history to reduce miss rate across mixed workloads.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

# Segmented LRU (SLRU): split cache into probationary and protected segments.
# New items go to probationary; on hit they are promoted to protected.
# Evict from probationary first (LRU), else from protected (LRU).
# This shields reused items from scans/one-hit-wonders and typically lowers miss rate.

m_key_timestamp = dict()           # recency clock (last access time)
m_probationary = set()             # keys currently in probationary segment
m_protected = set()                # keys currently in protected segment
_PROTECTED_RATIO = 0.8             # target fraction of cache for protected segment
=======
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

# Segmented LRU (SLRU): split cache into probationary and protected segments.
# New items go to probationary; on hit they are promoted to protected.
# Evict from probationary first (LRU), else from protected (LRU).
# This shields reused items from scans/one-hit-wonders and typically lowers miss rate.

m_key_timestamp = dict()           # recency clock (last access time)
m_probationary = set()             # keys currently in probationary segment
m_protected = set()                # keys currently in protected segment
_PROTECTED_RATIO = 0.8             # initial fraction of cache for protected segment

# ARC-style ghost history (store only keys with last-seen timestamps)
m_B1_ts = dict()  # ghosts of items evicted from probationary
m_B2_ts = dict()  # ghosts of items evicted from protected

# Adaptive protected target (in items); initialized from capacity * _PROTECTED_RATIO
m_prot_target_count = None


def _cap(cache_snapshot):
    """Return effective capacity in items (>=1)."""
    try:
        cap = int(cache_snapshot.capacity)
    except Exception:
        cap = 0
    if cap <= 0:
        cap = len(getattr(cache_snapshot, 'cache', {}) or {})
    return max(cap, 1)


def _ensure_targets(cache_snapshot):
    """Initialize or clamp the protected target based on capacity."""
    global m_prot_target_count
    cap = _cap(cache_snapshot)
    if m_prot_target_count is None:
        m_prot_target_count = max(0, min(cap - 1, int(round(cap * _PROTECTED_RATIO))))
    else:
        # Keep target within [0, cap-1]
        m_prot_target_count = max(0, min(cap - 1, int(m_prot_target_count)))


def _trim_ghosts(cache_snapshot):
    """Bound total ghost entries to at most 2x capacity by removing oldest globally."""
    cap = _cap(cache_snapshot)
    bound = 2 * cap
    total = len(m_B1_ts) + len(m_B2_ts)
    while total > bound:
        b1_old_key = min(m_B1_ts, key=m_B1_ts.get) if m_B1_ts else None
        b2_old_key = min(m_B2_ts, key=m_B2_ts.get) if m_B2_ts else None
        if b1_old_key is None and b2_old_key is None:
            break
        if b2_old_key is None or (b1_old_key is not None and m_B1_ts[b1_old_key] <= m_B2_ts.get(b2_old_key, float('inf'))):
            m_B1_ts.pop(b1_old_key, None)
        else:
            m_B2_ts.pop(b2_old_key, None)
        total = len(m_B1_ts) + len(m_B2_ts)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def _enforce_segment_limits(cache_snapshot):
    """Ensure protected segment does not exceed its target size; demote LRU protected if needed."""
    # Determine the target maximum size for the protected segment.
    try:
        cap = int(cache_snapshot.capacity)
    except Exception:
        cap = max(len(m_key_timestamp), 1)
    prot_max = max(int(cap * _PROTECTED_RATIO), 1)
    # Demote LRU items from protected to probationary until within target.
    while len(m_protected) > prot_max:
        demote_key = _find_lru_key_in_set(m_protected)
        if demote_key is None:
            break
        m_protected.discard(demote_key)
        m_probationary.add(demote_key)
=======
def _enforce_segment_limits(cache_snapshot):
    """Ensure protected segment does not exceed its adaptive target size; demote LRU protected if needed."""
    _ensure_targets(cache_snapshot)
    cap = _cap(cache_snapshot)
    # Clamp target to available capacity (allow 0 protected if adaptation demands)
    prot_max = max(0, min(m_prot_target_count, cap - 1))
    # Demote LRU items from protected to probationary until within target.
    while len(m_protected) > prot_max:
        demote_key = _find_lru_key_in_set(m_protected)
        if demote_key is None:
            break
        m_protected.discard(demote_key)
        m_probationary.add(demote_key)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    # Clean sets to only include keys actually present in the cache.
    cache_keys = set(cache_snapshot.cache.keys())
    if m_probationary:
        m_probationary.intersection_update(cache_keys)
    if m_protected:
        m_protected.intersection_update(cache_keys)

    # Prefer evicting from probationary (cold) segment; otherwise, from protected.
    candid_obj_key = None
    if m_probationary:
        candid_obj_key = _find_lru_key_in_set(m_probationary)
    if candid_obj_key is None and m_protected:
        candid_obj_key = _find_lru_key_in_set(m_protected)

    # Fallback: global LRU among all cache keys (robustness in case of drift).
    if candid_obj_key is None:
        candid_obj_key = _find_lru_key_in_set(cache_keys)

    # Absolute fallback: choose any key deterministically if timestamps are missing.
    if candid_obj_key is None and cache_keys:
        candid_obj_key = next(iter(cache_keys))
    return candid_obj_key
=======
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    # Clean sets to only include keys actually present in the cache.
    cache_keys = set(cache_snapshot.cache.keys())
    if m_probationary:
        m_probationary.intersection_update(cache_keys)
    if m_protected:
        m_protected.intersection_update(cache_keys)

    # Ensure target is initialized/clamped
    _ensure_targets(cache_snapshot)
    prot_max = max(0, min(m_prot_target_count, _cap(cache_snapshot) - 1))

    # If protected is oversized, evict from protected first to steer back to target
    candid_obj_key = None
    if m_protected and len(m_protected) > prot_max:
        candid_obj_key = _find_lru_key_in_set(m_protected)

    # Otherwise prefer evicting from probationary (cold) segment
    if candid_obj_key is None and m_probationary:
        candid_obj_key = _find_lru_key_in_set(m_probationary)

    # If probationary empty, evict from protected
    if candid_obj_key is None and m_protected:
        candid_obj_key = _find_lru_key_in_set(m_protected)

    # Fallback: global LRU among all cache keys (robustness in case of drift).
    if candid_obj_key is None:
        candid_obj_key = _find_lru_key_in_set(cache_keys)

    # Absolute fallback: choose any key deterministically if timestamps are missing.
    if candid_obj_key is None and cache_keys:
        candid_obj_key = next(iter(cache_keys))
    return candid_obj_key
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, m_probationary, m_protected
    # Update recency clock
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Promote from probationary to protected upon first hit; otherwise just refresh.
    if obj.key in m_probationary:
        m_probationary.discard(obj.key)
        m_protected.add(obj.key)

    # Keep protected segment within target size by demoting its LRU to probationary.
    _enforce_segment_limits(cache_snapshot)
=======
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, m_probationary, m_protected
    _ensure_targets(cache_snapshot)
    # Update recency clock
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Promote from probationary to protected upon first hit; otherwise just refresh.
    if obj.key in m_probationary:
        m_probationary.discard(obj.key)
        m_protected.add(obj.key)

    # Keep protected segment within target size by demoting its LRU to probationary.
    _enforce_segment_limits(cache_snapshot)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_probationary, m_protected
    # Initialize recency and place into probationary segment.
    m_key_timestamp[obj.key] = cache_snapshot.access_count
    m_protected.discard(obj.key)
    m_probationary.add(obj.key)

    # Keep protected segment within target size (no-op if protected not oversized).
    _enforce_segment_limits(cache_snapshot)
=======
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_probationary, m_protected, m_prot_target_count
    _ensure_targets(cache_snapshot)

    # Initialize recency
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # ARC-style adaptation using ghosts: adjust protected target and admission policy
    cap = _cap(cache_snapshot)
    step = max(1, cap // 8)
    in_b1 = obj.key in m_B1_ts
    in_b2 = obj.key in m_B2_ts

    if in_b1 or in_b2:
        if in_b1:
            # B1 hit => increase probationary share (decrease protected target)
            m_prot_target_count = max(0, m_prot_target_count - step)
            m_B1_ts.pop(obj.key, None)
        if in_b2:
            # B2 hit => increase protected target
            m_prot_target_count = min(cap - 1, m_prot_target_count + step)
            m_B2_ts.pop(obj.key, None)
        # Admit ghost hits directly to protected (frequent)
        m_probationary.discard(obj.key)
        m_protected.add(obj.key)
    else:
        # Fresh object: admit to probationary
        m_protected.discard(obj.key)
        m_probationary.add(obj.key)

    # Keep protected segment within adaptive target
    _enforce_segment_limits(cache_snapshot)
    # Trim ghosts regularly
    _trim_ghosts(cache_snapshot)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_probationary, m_protected
    # Remove evicted object from all metadata.
    m_key_timestamp.pop(evicted_obj.key, None)
    m_probationary.discard(evicted_obj.key)
    m_protected.discard(evicted_obj.key)
=======
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_probationary, m_protected
    # Determine which segment the evicted key belonged to (before removal)
    evicted_key = evicted_obj.key
    was_protected = evicted_key in m_protected
    was_probationary = evicted_key in m_probationary

    # Add to appropriate ghost list with current timestamp
    ts = cache_snapshot.access_count
    if was_protected:
        m_B2_ts[evicted_key] = ts
    else:
        # Default to B1 for unknown or probationary
        m_B1_ts[evicted_key] = ts

    # Remove evicted object from all metadata.
    m_key_timestamp.pop(evicted_key, None)
    m_probationary.discard(evicted_key)
    m_protected.discard(evicted_key)

    # Trim ghost history to stay within bounds
    _trim_ghosts(cache_snapshot)
>>>>>>> REPLACE

</DIFF>