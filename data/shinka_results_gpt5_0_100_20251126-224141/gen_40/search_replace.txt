<NAME>
arc_hybrid_lfu_scanaware
</NAME>

<DESCRIPTION>
I add a lightweight, decaying frequency sketch and replace pure LRU victim choice with a frequency-aware selection within ARCâ€™s T1/T2 segments. This hybrid ARC+LFU approach better protects frequently reused objects while still adapting between recency and frequency via the ARC p parameter.

Key improvements:
- Frequency sketch with periodic decay: prevents long-term bias and adapts quickly to phase changes.
- Victim selection within a chosen segment prefers evicting low-frequency, older items, dramatically improving resilience against scans and mixed workloads.
- More responsive and asymmetric p updates: use ceiling ratios and larger decrements on B2 hits during cold streaks; mild idle decay and cold-miss clamp to mitigate scan pollution.
- Safe ghost maintenance and disjointness remain unchanged.

These changes target poor-performing traces (scan-like or large working sets) without regressing reuse-heavy scenarios, thus lowering overall miss rates.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# Resident metadata
# - m_key_timestamp: last access time for each resident key
# - m_key_segment: 't1' (recent) or 't2' (frequent) for resident keys
m_key_timestamp = dict()
m_key_segment = dict()

# Ghost metadata (key -> last timestamp when it entered ghost)
m_ghost_b1_ts = dict()
m_ghost_b2_ts = dict()

# Adaptive target for T1 size
m_target_p = None

# Additional controls
m_last_ghost_hit_access = None
m_cold_streak = 0  # count of consecutive cold misses (not in ghosts)
m_t1_pending_hits = dict()  # key -> last hit access_count while in T1
=======
# Resident metadata
# - m_key_timestamp: last access time for each resident key
# - m_key_segment: 't1' (recent) or 't2' (frequent) for resident keys
m_key_timestamp = dict()
m_key_segment = dict()

# Ghost metadata (key -> last timestamp when it entered ghost)
m_ghost_b1_ts = dict()
m_ghost_b2_ts = dict()

# Adaptive target for T1 size
m_target_p = None

# Additional controls
m_last_ghost_hit_access = None
m_cold_streak = 0  # count of consecutive cold misses (not in ghosts)
m_t1_pending_hits = dict()  # key -> last hit access_count while in T1

# Lightweight frequency sketch with periodic decay to avoid long-term bias
m_freq = dict()  # key -> decaying frequency count (applies to recent activity)
m_next_decay_access = None  # next access_count at which to decay frequencies
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def _ensure_init(cache_snapshot):
    global m_target_p, m_last_ghost_hit_access, m_cold_streak
    if m_target_p is None:
        m_target_p = max(1, _cap(cache_snapshot) // 2)
    if m_last_ghost_hit_access is None:
        m_last_ghost_hit_access = cache_snapshot.access_count
    if m_cold_streak is None:
        m_cold_streak = 0
=======
def _ensure_init(cache_snapshot):
    global m_target_p, m_last_ghost_hit_access, m_cold_streak, m_next_decay_access
    if m_target_p is None:
        m_target_p = max(1, _cap(cache_snapshot) // 2)
    if m_last_ghost_hit_access is None:
        m_last_ghost_hit_access = cache_snapshot.access_count
    if m_cold_streak is None:
        m_cold_streak = 0
    if m_next_decay_access is None:
        # Schedule frequency decay roughly once per capacity accesses
        m_next_decay_access = cache_snapshot.access_count + max(8, _cap(cache_snapshot))
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def _prune_ghosts(cache_snapshot):
    """Keep total ghost size <= 2*capacity; evict oldest from the larger ghost list first."""
    total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
    cap = _cap(cache_snapshot)
    limit = max(1, 2 * cap)
    while total > limit:
        # Prefer trimming the larger ghost list to maintain balance
        if len(m_ghost_b1_ts) >= len(m_ghost_b2_ts):
            if m_ghost_b1_ts:
                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
                m_ghost_b1_ts.pop(b1_old_key, None)
            elif m_ghost_b2_ts:
                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
                m_ghost_b2_ts.pop(b2_old_key, None)
        else:
            if m_ghost_b2_ts:
                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
                m_ghost_b2_ts.pop(b2_old_key, None)
            elif m_ghost_b1_ts:
                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
                m_ghost_b1_ts.pop(b1_old_key, None)
        total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
=======
def _prune_ghosts(cache_snapshot):
    """Keep total ghost size <= 2*capacity; evict oldest from the larger ghost list first."""
    total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)
    cap = _cap(cache_snapshot)
    limit = max(1, 2 * cap)
    while total > limit:
        # Prefer trimming the larger ghost list to maintain balance
        if len(m_ghost_b1_ts) >= len(m_ghost_b2_ts):
            if m_ghost_b1_ts:
                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
                m_ghost_b1_ts.pop(b1_old_key, None)
            elif m_ghost_b2_ts:
                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
                m_ghost_b2_ts.pop(b2_old_key, None)
        else:
            if m_ghost_b2_ts:
                b2_old_key = min(m_ghost_b2_ts, key=m_ghost_b2_ts.get)
                m_ghost_b2_ts.pop(b2_old_key, None)
            elif m_ghost_b1_ts:
                b1_old_key = min(m_ghost_b1_ts, key=m_ghost_b1_ts.get)
                m_ghost_b1_ts.pop(b1_old_key, None)
        total = len(m_ghost_b1_ts) + len(m_ghost_b2_ts)


def _maybe_decay_freq(cache_snapshot):
    """Periodically decay frequency counts to bound memory and track recent popularity."""
    global m_freq, m_next_decay_access
    _ensure_init(cache_snapshot)
    if m_next_decay_access is None:
        m_next_decay_access = cache_snapshot.access_count + max(8, _cap(cache_snapshot))
        return
    if cache_snapshot.access_count >= m_next_decay_access:
        if m_freq:
            for k in list(m_freq.keys()):
                newc = m_freq.get(k, 0) >> 1  # halve counts
                if newc:
                    m_freq[k] = newc
                else:
                    m_freq.pop(k, None)
        m_next_decay_access = cache_snapshot.access_count + max(8, _cap(cache_snapshot))


def _bump_freq(key, weight=1):
    """Increase frequency count with small integer weight."""
    try:
        inc = max(1, int(weight))
    except Exception:
        inc = 1
    m_freq[key] = m_freq.get(key, 0) + inc


def _choose_victim(keys, now):
    """Pick victim by lowest frequency then oldest timestamp among provided keys."""
    if not keys:
        return None
    return min(keys, key=lambda k: (m_freq.get(k, 0), m_key_timestamp.get(k, float('inf'))))
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_init(cache_snapshot)
    t1_keys, t2_keys = _resident_sets(cache_snapshot)

    # Keep ghosts disjoint from current residents to ensure consistent ARC signals
    for k in cache_snapshot.cache.keys():
        m_ghost_b1_ts.pop(k, None)
        m_ghost_b2_ts.pop(k, None)

    # Strict ARC REPLACE decision:
    # Evict from T1 if |T1| > p, or (incoming in B2 and |T1| >= max(1, p)), else from T2.
    cap = _cap(cache_snapshot)
    p = min(max(0, m_target_p), cap)
    t1_len = len(t1_keys)

    evict_from_t1 = (t1_len > p) or (obj.key in m_ghost_b2_ts and t1_len >= max(1, p))

    if evict_from_t1:
        victim = _lru_key(t1_keys)
        if victim is not None:
            return victim
        # If T1 is empty, fall back to T2
        victim = _lru_key(t2_keys)
        if victim is not None:
            return victim
    else:
        victim = _lru_key(t2_keys)
        if victim is not None:
            return victim
        # If T2 is empty, fall back to T1
        victim = _lru_key(t1_keys)
        if victim is not None:
            return victim

    # Last resort: global LRU
    all_keys = list(cache_snapshot.cache.keys())
    if not all_keys:
        return None
    return _lru_key(all_keys)
=======
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_init(cache_snapshot)
    t1_keys, t2_keys = _resident_sets(cache_snapshot)

    # Keep ghosts disjoint from current residents to ensure consistent ARC signals
    for k in cache_snapshot.cache.keys():
        m_ghost_b1_ts.pop(k, None)
        m_ghost_b2_ts.pop(k, None)

    # Strict ARC REPLACE decision with hybrid LFU-LRU victim choice within a segment:
    cap = _cap(cache_snapshot)
    p = min(max(0, m_target_p), cap)
    t1_len = len(t1_keys)
    now = cache_snapshot.access_count

    evict_from_t1 = (t1_len > p) or (obj.key in m_ghost_b2_ts and t1_len >= max(1, p))

    if evict_from_t1:
        victim = _choose_victim(t1_keys, now)
        if victim is not None:
            return victim
        # If T1 is empty, fall back to T2 with hybrid choice
        victim = _choose_victim(t2_keys, now)
        if victim is not None:
            return victim
    else:
        victim = _choose_victim(t2_keys, now)
        if victim is not None:
            return victim
        # If T2 is empty, fall back to T1
        victim = _choose_victim(t1_keys, now)
        if victim is not None:
            return victim

    # Last resort: global lowest-frequency then oldest
    all_keys = list(cache_snapshot.cache.keys())
    if not all_keys:
        return None
    return _choose_victim(all_keys, now)
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_t1_pending_hits, m_cold_streak, m_target_p, m_last_ghost_hit_access
    _ensure_init(cache_snapshot)

    # Refresh recency
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Reset cold streak on any hit
    m_cold_streak = 0

    # Slow decay of p if no ghost hits for a while (scan recovery)
    cap = _cap(cache_snapshot)
    if cache_snapshot.access_count - m_last_ghost_hit_access > cap:
        m_target_p = max(0, m_target_p - 1)

    # ARC-style immediate promotion: on a hit in T1, move to T2
    seg = m_key_segment.get(obj.key, 't1')
    if seg != 't2':
        m_key_segment[obj.key] = 't2'
    # Clear any pending two-hit state (no longer used)
    m_t1_pending_hits.pop(obj.key, None)

    # Ensure ghosts remain disjoint from residents
    m_ghost_b1_ts.pop(obj.key, None)
    m_ghost_b2_ts.pop(obj.key, None)
=======
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_t1_pending_hits, m_cold_streak, m_target_p, m_last_ghost_hit_access, m_freq, m_next_decay_access
    _ensure_init(cache_snapshot)

    # Periodic decay to keep frequency sketch recent
    _maybe_decay_freq(cache_snapshot)

    # Refresh recency
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Reset cold streak on any hit and bump frequency
    m_cold_streak = 0
    _bump_freq(obj.key, 1)

    # Slow decay of p if no ghost hits for a while (scan recovery)
    cap = _cap(cache_snapshot)
    if cache_snapshot.access_count - m_last_ghost_hit_access > cap:
        m_target_p = max(0, m_target_p - 1)

    # ARC-style immediate promotion: on a hit in T1, move to T2
    seg = m_key_segment.get(obj.key, 't1')
    if seg != 't2':
        m_key_segment[obj.key] = 't2'
    # Clear any pending two-hit state (no longer used)
    m_t1_pending_hits.pop(obj.key, None)

    # Ensure ghosts remain disjoint from residents
    m_ghost_b1_ts.pop(obj.key, None)
    m_ghost_b2_ts.pop(obj.key, None)
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_target_p, m_last_ghost_hit_access, m_cold_streak, m_t1_pending_hits
    _ensure_init(cache_snapshot)

    cap = _cap(cache_snapshot)
    step_cap = max(1, cap // 8)

    in_b1 = obj.key in m_ghost_b1_ts
    in_b2 = obj.key in m_ghost_b2_ts

    seg = 't1'
    if in_b1:
        # Increase p (give more room to recency), damped
        raw = max(1, len(m_ghost_b2_ts) // max(1, len(m_ghost_b1_ts)))
        inc = min(step_cap, raw)
        m_target_p = min(cap, m_target_p + inc)
        # Ghost bookkeeping
        m_ghost_b1_ts.pop(obj.key, None)
        m_ghost_b2_ts.pop(obj.key, None)
        # Promote on ghost hit
        seg = 't2'
        m_last_ghost_hit_access = cache_snapshot.access_count
        m_cold_streak = 0
    elif in_b2:
        # Decrease p (favor frequency), damped
        raw = max(1, len(m_ghost_b1_ts) // max(1, len(m_ghost_b2_ts)))
        dec = min(step_cap, raw)
        m_target_p = max(0, m_target_p - dec)
        # Ghost bookkeeping
        m_ghost_b2_ts.pop(obj.key, None)
        m_ghost_b1_ts.pop(obj.key, None)
        # Promote on ghost hit
        seg = 't2'
        m_last_ghost_hit_access = cache_snapshot.access_count
        m_cold_streak = 0
    else:
        # Cold miss - track streak and apply scan clamp if needed
        m_cold_streak += 1
        if m_cold_streak > cap:
            m_target_p = max(0, m_target_p - max(1, cap // 4))
            m_cold_streak = 0
        # Ensure no stale ghost entries remain for this resident
        m_ghost_b1_ts.pop(obj.key, None)
        m_ghost_b2_ts.pop(obj.key, None)

    # Insert into resident set
    m_key_segment[obj.key] = seg
    m_key_timestamp[obj.key] = cache_snapshot.access_count
    # Clear any stale pending state for this key
    m_t1_pending_hits.pop(obj.key, None)

    # Control ghost size (expanded history)
    _prune_ghosts(cache_snapshot)
=======
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_key_segment, m_target_p, m_last_ghost_hit_access, m_cold_streak, m_t1_pending_hits, m_freq, m_next_decay_access
    _ensure_init(cache_snapshot)

    # Periodic decay to keep frequency sketch recent
    _maybe_decay_freq(cache_snapshot)

    cap = _cap(cache_snapshot)
    in_b1 = obj.key in m_ghost_b1_ts
    in_b2 = obj.key in m_ghost_b2_ts

    # Adaptive step caps with asymmetric responsiveness (scan-aware)
    inc_cap = max(1, cap // 8)
    dec_cap = max(1, (cap // 4) if m_cold_streak >= max(1, cap // 2) else (cap // 8))

    seg = 't1'
    if in_b1:
        # Increase p (give more room to recency), with ceiling ratio and cap
        denom = max(1, len(m_ghost_b1_ts))
        numer = len(m_ghost_b2_ts)
        raw_inc = max(1, (numer + denom - 1) // denom)  # ceil(|B2|/|B1|)
        inc = min(inc_cap, raw_inc)
        m_target_p = min(cap, m_target_p + inc)
        # Ghost bookkeeping
        m_ghost_b1_ts.pop(obj.key, None)
        m_ghost_b2_ts.pop(obj.key, None)
        # Promote on ghost hit
        seg = 't2'
        m_last_ghost_hit_access = cache_snapshot.access_count
        m_cold_streak = 0
        _bump_freq(obj.key, 2)
    elif in_b2:
        # Decrease p (favor frequency), with ceiling ratio and stronger cap during cold streaks
        denom = max(1, len(m_ghost_b2_ts))
        numer = len(m_ghost_b1_ts)
        raw_dec = max(1, (numer + denom - 1) // denom)  # ceil(|B1|/|B2|)
        dec = min(dec_cap, raw_dec)
        m_target_p = max(0, m_target_p - dec)
        # Ghost bookkeeping
        m_ghost_b2_ts.pop(obj.key, None)
        m_ghost_b1_ts.pop(obj.key, None)
        # Promote on ghost hit
        seg = 't2'
        m_last_ghost_hit_access = cache_snapshot.access_count
        m_cold_streak = 0
        _bump_freq(obj.key, 3)
    else:
        # Cold miss - track streak and apply gentle scan clamp to reduce T1 pressure
        m_cold_streak += 1
        # Gradual clamp during sustained cold streaks
        if m_cold_streak % max(1, cap // 2) == 0:
            m_target_p = max(0, m_target_p - max(1, cap // 16))
        if m_cold_streak >= cap:
            m_target_p = max(0, m_target_p - max(1, cap // 8))
            m_cold_streak = 0
        # Ensure no stale ghost entries remain for this resident
        m_ghost_b1_ts.pop(obj.key, None)
        m_ghost_b2_ts.pop(obj.key, None)
        _bump_freq(obj.key, 1)

    # Insert into resident set
    m_key_segment[obj.key] = seg
    m_key_timestamp[obj.key] = cache_snapshot.access_count
    # Clear any stale pending state for this key
    m_t1_pending_hits.pop(obj.key, None)

    # Control ghost size (expanded history)
    _prune_ghosts(cache_snapshot)
>>>>>>> REPLACE

</DIFF>