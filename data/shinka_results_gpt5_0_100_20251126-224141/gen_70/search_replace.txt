<NAME>
arc_bias_trim_freq_guard
</NAME>

<DESCRIPTION>
I introduce four targeted improvements to reduce miss rate by stabilizing ARC adaptation, making ghost metadata more informative, and improving victim selection with lightweight frequency and ghost-informed bias:

1) One-time cold-streak clamp with proper reset: The previous decay applied an extra clamp on every call while the cold streak persisted, which can over-bias toward recency and oscillate. I add a cold_clamp_applied flag that applies this clamp only once per cold phase and reset it only on a ghost hit. This matches the gentle decay behavior that stabilizes ARC variants.

2) P-aware ghost trimming with tighter budget: I change ghost trimming to enforce |B1| + |B2| â‰¤ C (rather than 2C) and use targets target_B1 = p, target_B2 = C - p. This keeps the ghost signal crisp and aligned with the current recency/frequency split, improving adaptation under phase shifts.

3) Scan guard reset and gentle p bias: I change scan guard logic to bias REPLACE using an effective_p rather than hard-forcing T1, and I reset the scan guard on any cache hit and on any ghost hit. This prevents the guard from lingering and over-evicting useful items once a repeated access appears.

4) Smarter victim choice with lightweight frequency and ghost hints: I extend the depth-limited victim sampler to accept bias toward/against items based on ghost membership. We prefer to avoid evicting T1 entries present in B2 (historically frequent) and prefer evicting T2 entries that are in B1 (recently demoted). This ordering is used first; deterministic fallbacks probe for (a) T1 LRU not in B2, (b) T2 LRU in B1, and (c) bounded peeks, before final deterministic fallback.

Together these changes improve resilience to scans, reduce unnecessary thrashing, and maintain consistent ARC invariants for better hit rates across diverse workloads.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
scan_guard_until = 0            # bias REPLACE toward T1 during suspected scans
=======
scan_guard_until = 0            # bias REPLACE toward T1 during suspected scans
cold_clamp_applied = False      # one-time extra clamp applied flag
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def _pick_victim_from_od(od, now, depth):
    # Pick the lowest-score candidate among the first 'depth' LRU entries
    best_k = None
    best_s = None
    for k in _iter_peek_lru(od, depth):
        s = _eff_score(k, now)
        if best_s is None or s < best_s:
            best_s = s
            best_k = k
    return best_k
=======
def _pick_victim_from_od(od, now, depth, prefer_in=None, avoid_in=None):
    # Pick the lowest-score candidate among the first 'depth' LRU entries with optional bias
    best_k = None
    best_s = None
    for k in _iter_peek_lru(od, depth):
        s = _eff_score(k, now)
        if prefer_in is not None and k in prefer_in:
            s = max(0, s - 1)
        if avoid_in is not None and k in avoid_in:
            s = s + 1
        if best_s is None or s < best_s:
            best_s = s
            best_k = k
    return best_k
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def _trim_ghosts():
    # Keep ghosts total size within 2x capacity with proportional trimming to p
    cap_base = (arc_capacity if arc_capacity is not None else 1)
    total_cap = cap_base * 2
    # Targets proportional to current p (approximately 2*p and 2C-2*p)
    target_B1 = min(total_cap, max(0, int(total_cap * (float(arc_p) / max(1, cap_base)))))
    target_B2 = max(0, total_cap - target_B1)

    def _over_target():
        return (len(arc_B1) + len(arc_B2)) - total_cap

    # Trim until under the total budget, favoring lists above their targets
    while (len(arc_B1) + len(arc_B2)) > total_cap:
        if len(arc_B1) > target_B1:
            _pop_lru(arc_B1)
        elif len(arc_B2) > target_B2:
            _pop_lru(arc_B2)
        else:
            # If neither is over its proportional target, drop from the larger one
            if len(arc_B1) >= len(arc_B2):
                _pop_lru(arc_B1)
            else:
                _pop_lru(arc_B2)
=======
def _trim_ghosts():
    # Keep ghosts total size within 1x capacity with proportional trimming to p
    cap_base = (arc_capacity if arc_capacity is not None else 1)
    total_cap = cap_base
    # Targets proportional to current p (approximately p and C-p)
    target_B1 = min(total_cap, max(0, int(arc_p)))
    target_B2 = max(0, total_cap - target_B1)

    # Trim until under the total budget, favoring lists above their targets
    while (len(arc_B1) + len(arc_B2)) > total_cap:
        if len(arc_B1) > target_B1:
            _pop_lru(arc_B1)
        elif len(arc_B2) > target_B2:
            _pop_lru(arc_B2)
        else:
            # If neither is over its proportional target, drop from the larger one
            if len(arc_B1) >= len(arc_B2):
                _pop_lru(arc_B1)
            else:
                _pop_lru(arc_B2)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
# Decay controller: if no ghost hits for a while, bias toward recency (smaller p) with bounded proportional decay
def _decay_arc_p_if_idle(now):
    global arc_p, arc_last_decay_access, cold_streak
    if arc_capacity is None:
        return
    cap = arc_capacity
    idle = now - arc_last_ghost_hit_access
    step_interval = max(1, cap // 8)
    if idle >= cap and (now - arc_last_decay_access) >= step_interval:
        step = min(max(1, cap // 8), max(1, idle // max(1, cap // 4)))
        arc_p = max(0, arc_p - step)
        arc_last_decay_access = now
    # If we've accumulated a cold streak, apply a one-time clamp to speed recovery from scans
    if cold_streak >= max(1, cap // 2):
        clamp = min(max(1, cap // 4), max(1, cold_streak // max(1, cap // 8)))
        arc_p = max(0, arc_p - clamp)
=======
# Decay controller: if no ghost hits for a while, bias toward recency (smaller p) with bounded proportional decay
def _decay_arc_p_if_idle(now):
    global arc_p, arc_last_decay_access, cold_streak, cold_clamp_applied
    if arc_capacity is None:
        return
    cap = arc_capacity
    idle = now - arc_last_ghost_hit_access
    step_interval = max(1, cap // 8)
    if idle >= cap and (now - arc_last_decay_access) >= step_interval:
        step = min(max(1, cap // 8), max(1, idle // max(1, cap // 4)))
        arc_p = max(0, arc_p - step)
        arc_last_decay_access = now
    # One-time clamp during prolonged cold streaks; reset only on a ghost hit
    if (not cold_clamp_applied) and cold_streak >= max(1, cap // 2):
        clamp = min(max(1, cap // 4), max(1, cold_streak // max(1, cap // 8)))
        arc_p = max(0, arc_p - clamp)
        cold_clamp_applied = True
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_capacity(cache_snapshot)
    _resync(cache_snapshot)
    # Apply ghost-driven adaptation before replacement (canonical ARC behavior)
    global arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until
    now = cache_snapshot.access_count
    step_cap = max(1, (arc_capacity if arc_capacity is not None else 1) // 8)
    if obj.key in arc_B1:
        # Favor recency: increase p
        ratio = max(1, len(arc_B2) // max(1, len(arc_B1)))
        inc = min(ratio, step_cap, max(0, (arc_capacity if arc_capacity is not None else 0) - arc_p))
        arc_p = min((arc_capacity if arc_capacity is not None else arc_p), arc_p + inc)
        arc_last_ghost_hit_access = now
        cold_streak = 0
    elif obj.key in arc_B2:
        # Favor frequency: decrease p
        ratio = max(1, len(arc_B1) // max(1, len(arc_B2)))
        dec = min(ratio, step_cap, arc_p)
        arc_p = max(0, arc_p - dec)
        arc_last_ghost_hit_access = now
        cold_streak = 0

    # ARC replacement: choose between T1 and T2 depending on p and whether obj is in B2
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)
    choose_from_T1 = False
    if t1_sz >= 1 and (t1_sz > arc_p or (x_in_B2 and t1_sz == arc_p)):
        choose_from_T1 = True

    # Scan-guard bias: during suspected scans, prefer evicting from T1 if possible
    if now <= scan_guard_until and t1_sz > 0:
        choose_from_T1 = True

    # Depth-limited, frequency- and age-informed victim selection within chosen list
    victim = None
    depth = min(8, max(1, (arc_capacity if arc_capacity is not None else 1) // 16))
    if choose_from_T1:
        if arc_T1:
            victim = _pick_victim_from_od(arc_T1, now, depth)
    else:
        if arc_T2:
            victim = _pick_victim_from_od(arc_T2, now, depth)

    # Try the other list with sampling if the chosen one is empty
    if victim is None:
        if choose_from_T1 and arc_T2:
            victim = _pick_victim_from_od(arc_T2, now, depth)
        elif (not choose_from_T1) and arc_T1:
            victim = _pick_victim_from_od(arc_T1, now, depth)

    # Fallbacks: if still none, use classic ARC fallbacks
    if victim is None:
        if t1_sz > 0 and arc_T1:
            victim = next(iter(arc_T1))
        elif len(arc_T2) > 0 and arc_T2:
            victim = next(iter(arc_T2))
        else:
            # Rare drift: resync once and retry small bounded scan, then deterministic fallback
            _resync(cache_snapshot)
            scan_lim = max(1, (arc_capacity if arc_capacity is not None else 1) // 16)
            if arc_T1:
                i = 0
                for k in arc_T1.keys():
                    victim = k
                    i += 1
                    if i >= scan_lim:
                        break
            elif arc_T2:
                i = 0
                for k in arc_T2.keys():
                    victim = k
                    i += 1
                    if i >= scan_lim:
                        break
            elif cache_snapshot.cache:
                # Deterministic final fallback: first key iteration
                victim = next(iter(cache_snapshot.cache.keys()))
    return victim
=======
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    _ensure_capacity(cache_snapshot)
    _resync(cache_snapshot)
    # Apply ghost-driven adaptation before replacement (canonical ARC behavior)
    global arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until, cold_clamp_applied
    now = cache_snapshot.access_count
    cap = arc_capacity if arc_capacity is not None else 1
    step_cap = max(1, cap // 8)
    if obj.key in arc_B1:
        # Favor recency: increase p
        ratio = max(1, len(arc_B2) // max(1, len(arc_B1)))
        inc = min(ratio, step_cap, max(0, cap - arc_p))
        arc_p = min(cap, arc_p + inc)
        arc_last_ghost_hit_access = now
        cold_streak = 0
        cold_clamp_applied = False
        scan_guard_until = 0
    elif obj.key in arc_B2:
        # Favor frequency: decrease p
        ratio = max(1, len(arc_B1) // max(1, len(arc_B2)))
        dec = min(ratio, step_cap, arc_p)
        arc_p = max(0, arc_p - dec)
        arc_last_ghost_hit_access = now
        cold_streak = 0
        cold_clamp_applied = False
        scan_guard_until = 0

    # ARC replacement: choose between T1 and T2 depending on effective p and whether obj is in B2
    x_in_B2 = obj.key in arc_B2
    t1_sz = len(arc_T1)
    eff_p = arc_p
    if now <= scan_guard_until:
        eff_p = max(0, arc_p - max(1, cap // 8))
    choose_from_T1 = False
    if t1_sz >= 1 and (t1_sz > eff_p or (x_in_B2 and t1_sz == eff_p)):
        choose_from_T1 = True

    # Depth-limited, frequency- and age-informed victim selection within chosen list, with ghost-aware bias
    victim = None
    depth = min(8, max(1, cap // 16))
    if choose_from_T1 and arc_T1:
        victim = _pick_victim_from_od(arc_T1, now, depth, prefer_in=None, avoid_in=arc_B2)
    elif (not choose_from_T1) and arc_T2:
        victim = _pick_victim_from_od(arc_T2, now, depth, prefer_in=arc_B1, avoid_in=None)

    # Try the other list with the complementary bias if the chosen one is empty
    if victim is None:
        if choose_from_T1 and arc_T2:
            victim = _pick_victim_from_od(arc_T2, now, depth, prefer_in=arc_B1, avoid_in=None)
        elif (not choose_from_T1) and arc_T1:
            victim = _pick_victim_from_od(arc_T1, now, depth, prefer_in=None, avoid_in=arc_B2)

    # Strengthened deterministic fallbacks
    if victim is None:
        # (a) T1 LRU not in B2
        for k in arc_T1.keys():
            if k not in arc_B2:
                victim = k
                break
    if victim is None:
        # (b) T2 LRU that appears in B1
        for k in arc_T2.keys():
            if k in arc_B1:
                victim = k
                break
    if victim is None:
        # (c) bounded peek for not-in-B2 from T1 then T2
        scan_lim = max(1, cap // 16)
        i = 0
        for k in arc_T1.keys():
            if k not in arc_B2:
                victim = k
                break
            i += 1
            if i >= scan_lim:
                break
        if victim is None:
            i = 0
            for k in arc_T2.keys():
                victim = k
                i += 1
                if i >= scan_lim:
                    break
    if victim is None:
        # (d) Classic ARC/last resort
        if t1_sz > 0 and arc_T1:
            victim = next(iter(arc_T1))
        elif len(arc_T2) > 0 and arc_T2:
            victim = next(iter(arc_T2))
        elif cache_snapshot.cache:
            victim = next(iter(cache_snapshot.cache.keys()))
    return victim
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, cold_streak, m_key_freq
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    # Any hit breaks cold streaks
    cold_streak = 0
=======
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp, cold_streak, m_key_freq, scan_guard_until
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    # Any hit breaks cold streaks and cancels scan guard
    cold_streak = 0
    scan_guard_until = 0
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until, m_key_freq
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    key = obj.key

    # Keep resident metadata consistent with actual cache
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    # ARC admission: p was already adapted in evict on ghost hits. Place accordingly.
    if key in arc_B1 or key in arc_B2:
        # On a ghost hit, item becomes frequent
        _move_to_mru(arc_T2, key)
        arc_last_ghost_hit_access = now
        cold_streak = 0
        # Initialize/boost frequency on ghost re-admission
        m_key_freq[key] = max(m_key_freq.get(key, 0), 2)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
        cold_streak += 1
        # Initialize frequency low for brand new items
        m_key_freq[key] = max(m_key_freq.get(key, 0), 1)
        # Streaming-aware behavior: early guard and progressive clamp
        if arc_capacity is not None:
            cap = arc_capacity
            # Bias REPLACE to draw from T1 for a short window during scans
            if cold_streak >= max(1, cap // 2):
                scan_guard_until = now + max(1, cap // 8)
            if cold_streak >= cap:
                dec = max(1, cap // 8)
                arc_p = max(0, arc_p - dec)
                # Keep pending hints conservative after clamp
                t1_pending.clear()
                # Prevent unbounded growth of the streak counter to keep adaptation responsive
                if cold_streak >= 2 * cap:
                    arc_p = max(0, arc_p - dec)
                    cold_streak = cap

    # Maintain disjointness: resident keys must not appear in ghosts
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)

    _trim_ghosts()
    m_key_timestamp[key] = now
=======
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, arc_p, arc_last_ghost_hit_access, cold_streak, scan_guard_until, m_key_freq, cold_clamp_applied
    _ensure_capacity(cache_snapshot)
    now = cache_snapshot.access_count
    _decay_arc_p_if_idle(now)
    key = obj.key

    # Keep resident metadata consistent with actual cache
    if (len(arc_T1) + len(arc_T2)) != len(cache_snapshot.cache):
        _resync(cache_snapshot)

    # ARC admission: p was already adapted in evict on ghost hits. Place accordingly.
    if key in arc_B1 or key in arc_B2:
        # On a ghost hit, item becomes frequent
        _move_to_mru(arc_T2, key)
        arc_last_ghost_hit_access = now
        cold_streak = 0
        cold_clamp_applied = False
        scan_guard_until = 0
        # Initialize/boost frequency on ghost re-admission
        m_key_freq[key] = max(m_key_freq.get(key, 0), 2)
    else:
        # Brand new: insert into T1 (recent)
        _move_to_mru(arc_T1, key)
        cold_streak += 1
        # Initialize frequency low for brand new items
        m_key_freq[key] = max(m_key_freq.get(key, 0), 1)
        # Streaming-aware behavior: early guard and progressive clamp
        if arc_capacity is not None:
            cap = arc_capacity
            # Bias REPLACE to draw from T1 for a short window during scans
            if cold_streak >= max(1, cap // 2):
                scan_guard_until = now + max(1, cap // 8)
            if cold_streak >= cap:
                dec = max(1, cap // 8)
                arc_p = max(0, arc_p - dec)
                # Keep pending hints conservative after clamp
                t1_pending.clear()
                # Prevent unbounded growth of the streak counter to keep adaptation responsive
                if cold_streak >= 2 * cap:
                    arc_p = max(0, arc_p - dec)
                    cold_streak = cap

    # Maintain disjointness: resident keys must not appear in ghosts
    arc_B1.pop(key, None)
    arc_B2.pop(key, None)

    _trim_ghosts()
    m_key_timestamp[key] = now
>>>>>>> REPLACE

</DIFF>