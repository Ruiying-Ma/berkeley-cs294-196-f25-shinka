<NAME>
adaptive_s3_fifo
</NAME>

<DESCRIPTION>
Implement an Adaptive S3-FIFO algorithm with Conditional Demotion.
1.  **Adaptive Sizing**: Introduce `_s3_p` (Small queue target ratio) that adapts based on workload.
    *   Increase `_s3_p` on Ghost hits (indicating premature eviction from Small).
    *   Decrease `_s3_p` on Main hits (indicating Main is effective and should be maximized).
    *   This allows the cache to dynamically partition between Probation (Small) and Protected (Main) regions.
2.  **Conditional Demotion**: Modify `evict` to only demote items from Main to Small if Small is within its target size. If Small is overflowing (`> s_target`), cold items from Main are evicted directly. This prevents cold Main data from polluting the Small queue during scan phases.
3.  **Expanded Ghost Registry**: Increase Ghost queue capacity to 5x cache size to better capture long loops and recurring items, improving the adaptivity signal.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# Global state
# We use dicts as ordered sets (insertion order preserved)
_s3_small = {}
_s3_main = {}
_s3_ghost = {} # Ghost queue for tracking evicted small objects
_s3_freq = {}
_last_ts = -1

def _check_reset(snapshot):
    global _s3_small, _s3_main, _s3_ghost, _s3_freq, _last_ts
    # If access count drops, it indicates a new trace has started
    if snapshot.access_count < _last_ts:
        _s3_small.clear()
        _s3_main.clear()
        _s3_ghost.clear()
        _s3_freq.clear()
    _last_ts = snapshot.access_count

def evict(cache_snapshot, obj):
    '''
    S3-FIFO eviction with Ghost Queue support and Main->Small Demotion.
    Evicts from Small (Probation) or Main (Protected).
    '''
    global _s3_small, _s3_main, _s3_freq
    _check_reset(cache_snapshot)

    # Target size for small queue (10% of capacity)
    curr_size = len(cache_snapshot.cache)
    s_target = max(1, int(curr_size * 0.1))

    while True:
        # 1. Check Small FIFO (Probation)
        # If Small is larger than target, we prefer to evict from it.
        # This handles both new items and demoted items from Main.
        if len(_s3_small) > s_target:
            if not _s3_small:
                break

            candidate = next(iter(_s3_small))

            # Consistency check
            if candidate not in cache_snapshot.cache:
                _s3_small.pop(candidate, None)
                _s3_freq.pop(candidate, None)
                continue

            # S3-FIFO Logic:
            freq = _s3_freq.get(candidate, 0)
            if freq > 0:
                # Accessed while in Small -> Promote to Main
                _s3_small.pop(candidate)
                _s3_main[candidate] = None # Insert at tail (MRU)
                _s3_freq[candidate] = 0    # Reset frequency
                continue
            else:
                # Not visited: Evict from Small
                return candidate

        # 2. Check Main FIFO (Protected)
        # We process Main if Small is small enough.
        if _s3_main:
            candidate = next(iter(_s3_main))

            if candidate not in cache_snapshot.cache:
                _s3_main.pop(candidate, None)
                _s3_freq.pop(candidate, None)
                continue

            freq = _s3_freq.get(candidate, 0)
            if freq > 0:
                # Accessed while in Main -> Second Chance (Reinsert)
                _s3_main.pop(candidate)
                _s3_main[candidate] = None # Reinsert at tail
                _s3_freq[candidate] = 0
                continue
            else:
                # Not visited in Main: Demote to Small instead of immediate eviction
                # This gives the item a chance to survive in the probationary queue
                # while allowing Small queue logic to decide the final victim.
                _s3_main.pop(candidate)
                _s3_small[candidate] = None # Insert at tail of Small
                _s3_freq[candidate] = 0
                continue
=======
# Global state
# We use dicts as ordered sets (insertion order preserved)
_s3_small = {}
_s3_main = {}
_s3_ghost = {} # Ghost queue for tracking evicted small objects
_s3_freq = {}
_last_ts = -1
_s3_p = 0.1    # Adaptive Small queue target ratio

def _check_reset(snapshot):
    global _s3_small, _s3_main, _s3_ghost, _s3_freq, _last_ts, _s3_p
    # If access count drops, it indicates a new trace has started
    if snapshot.access_count < _last_ts:
        _s3_small.clear()
        _s3_main.clear()
        _s3_ghost.clear()
        _s3_freq.clear()
        _s3_p = 0.1
    _last_ts = snapshot.access_count

def evict(cache_snapshot, obj):
    '''
    S3-FIFO eviction with Ghost Queue support, Adaptive Sizing, and Conditional Demotion.
    Evicts from Small (Probation) or Main (Protected).
    '''
    global _s3_small, _s3_main, _s3_freq, _s3_p
    _check_reset(cache_snapshot)

    # Adaptive target size for small queue
    curr_size = len(cache_snapshot.cache)
    s_target = max(1, int(curr_size * _s3_p))

    while True:
        # 1. Check Small FIFO (Probation)
        # If Small is larger than target, we prefer to evict from it.
        if len(_s3_small) > s_target:
            if not _s3_small:
                break

            candidate = next(iter(_s3_small))

            # Consistency check
            if candidate not in cache_snapshot.cache:
                _s3_small.pop(candidate, None)
                _s3_freq.pop(candidate, None)
                continue

            # S3-FIFO Logic:
            freq = _s3_freq.get(candidate, 0)
            if freq > 0:
                # Accessed while in Small -> Promote to Main
                _s3_small.pop(candidate)
                _s3_main[candidate] = None # Insert at tail (MRU)
                _s3_freq[candidate] = 0    # Reset frequency
                continue
            else:
                # Not visited: Evict from Small
                return candidate

        # 2. Check Main FIFO (Protected)
        # We process Main if Small is small enough.
        if _s3_main:
            candidate = next(iter(_s3_main))

            if candidate not in cache_snapshot.cache:
                _s3_main.pop(candidate, None)
                _s3_freq.pop(candidate, None)
                continue

            freq = _s3_freq.get(candidate, 0)
            if freq > 0:
                # Accessed while in Main -> Second Chance (Reinsert)
                _s3_main.pop(candidate)
                _s3_main[candidate] = None # Reinsert at tail
                _s3_freq[candidate] = 0
                continue
            else:
                # Not visited in Main: Conditional Demotion
                # If Small is already overflowing, don't dump more trash there.
                # Evict directly to protect Small from scan pollution.
                if len(_s3_small) > s_target:
                    _s3_main.pop(candidate)
                    _s3_freq.pop(candidate, None)
                    return candidate

                # Otherwise, Demote to Small to give a second chance
                _s3_main.pop(candidate)
                _s3_small[candidate] = None # Insert at tail of Small
                _s3_freq[candidate] = 0
                continue
>>>>>>> REPLACE
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    global _s3_freq
    _check_reset(cache_snapshot)
    # Cap frequency at 3
    current = _s3_freq.get(obj.key, 0)
    _s3_freq[obj.key] = min(current + 1, 3)

def update_after_insert(cache_snapshot, obj):
    global _s3_small, _s3_main, _s3_ghost, _s3_freq
    _check_reset(cache_snapshot)

    key = obj.key
    # S3-FIFO with Ghost:
    # If in ghost, it means it was evicted from Small recently.
    # Promote directly to Main to avoid "scan" classification.
    if key in _s3_ghost:
        if key not in _s3_main and key not in _s3_small:
            _s3_main[key] = None
            _s3_freq[key] = 0
        _s3_ghost.pop(key)
    else:
        # Standard insert to Small (Probation)
        if key not in _s3_small and key not in _s3_main:
            _s3_small[key] = None
            _s3_freq[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    global _s3_small, _s3_main, _s3_ghost, _s3_freq
    _check_reset(cache_snapshot)

    key = evicted_obj.key

    # If evicted from Small, track in Ghost.
    # Note: With demotion, items from Main go to Small before eviction,
    # so they will also pass through here.
    if key in _s3_small:
        _s3_small.pop(key)
        _s3_ghost[key] = None
        # Limit ghost size to avoid unbounded memory usage
        # Increased to 2x capacity to capture longer loops
        if len(_s3_ghost) > cache_snapshot.capacity * 2:
            # Remove oldest (FIFO head)
            _s3_ghost.pop(next(iter(_s3_ghost)), None)
=======
def update_after_hit(cache_snapshot, obj):
    global _s3_freq, _s3_p, _s3_main
    _check_reset(cache_snapshot)
    # Cap frequency at 3
    current = _s3_freq.get(obj.key, 0)
    _s3_freq[obj.key] = min(current + 1, 3)

    # Adaptive: Hit in Main -> Main is effective -> Shrink Small target (Grow Main)
    if obj.key in _s3_main:
        _s3_p = max(_s3_p - 0.0001, 0.05)

def update_after_insert(cache_snapshot, obj):
    global _s3_small, _s3_main, _s3_ghost, _s3_freq, _s3_p
    _check_reset(cache_snapshot)

    key = obj.key
    # S3-FIFO with Ghost:
    # If in ghost, it means it was evicted from Small recently.
    # Promote directly to Main to avoid "scan" classification.
    if key in _s3_ghost:
        # Adaptive: Ghost hit -> Missed opportunity -> Grow Small target
        _s3_p = min(_s3_p + 0.001, 0.5)

        if key not in _s3_main and key not in _s3_small:
            _s3_main[key] = None
            _s3_freq[key] = 0
        _s3_ghost.pop(key)
    else:
        # Standard insert to Small (Probation)
        if key not in _s3_small and key not in _s3_main:
            _s3_small[key] = None
            _s3_freq[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    global _s3_small, _s3_main, _s3_ghost, _s3_freq
    _check_reset(cache_snapshot)

    key = evicted_obj.key

    # If evicted from Small, track in Ghost.
    # Note: With demotion, items from Main go to Small before eviction,
    # so they will also pass through here.
    if key in _s3_small:
        _s3_small.pop(key)
        _s3_ghost[key] = None
        # Limit ghost size to avoid unbounded memory usage
        # Increased to 5x capacity to capture longer loops
        if len(_s3_ghost) > cache_snapshot.capacity * 5:
            # Remove oldest (FIFO head)
            _s3_ghost.pop(next(iter(_s3_ghost)), None)
>>>>>>> REPLACE
</DIFF>