<NAME>
slru_lfu_hybrid
</NAME>

<DESCRIPTION>
I replace the simple LRU (based solely on last-access timestamps) with a Segmented LRU (SLRU) augmented with a lightweight frequency bias (LFU) to reduce cache misses across diverse workloads.

Why this helps:
- LRU fails under scans and bursty workloads, evicting useful items due to sequential one-hit wonders. SLRU splits the cache into probation (new/cold entries) and protected (hot/promoted entries) segments:
  - New inserts go to probation.
  - On hit, items in probation are promoted to protected.
  - Eviction prefers the probation segment, preserving long-term hot items in protected.
- Frequency bias within probation improves eviction choice: among probation items, evict the least-frequent, tie-broken by LRU. This keeps items with recent repeat references from being evicted too early.
- We maintain a target protected size (~80% of capacity) and demote protected LRU back to probation if protected exceeds this target, ensuring the probation segment always has candidates and preventing overprotection of stale items.
- Robustness:
  - Metadata is pruned for consistency with actual cache contents.
  - We detect new traces/resets via access_count and clear metadata to avoid cross-trace contamination.
  - Graceful fallbacks if metadata is empty.

The update functions implement:
- update_after_hit: promote from probation to protected, maintain MRU order, increment frequency, and maintain protected target size.
- update_after_insert: place new items into probation and initialize a small frequency credit.
- update_after_evict: remove evicted from segments but keep frequency history to favor popular keys upon reinsertion.

This SLRU+LFU hybrid preserves recency for hot items while filtering out one-hit wonders, typically outperforming pure LRU on mixed workloads and reducing miss rates.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

m_key_timestamp = dict()

def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_ts = min(m_key_timestamp.values())
    candid_obj_keys = list(key for key in cache_snapshot.cache if m_key_timestamp[key] == min_ts)
    candid_obj_key = candid_obj_keys[0]
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key in m_key_timestamp
    m_key_timestamp[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key not in m_key_timestamp
    m_key_timestamp[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key not in m_key_timestamp
    assert evicted_obj.key in m_key_timestamp
    m_key_timestamp.pop(evicted_obj.key)
=======
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

from collections import OrderedDict

# Segmented LRU with LFU bias:
# - probation: new/cold entries, LRU ordered
# - protected: hot/promoted entries, LRU ordered
# - m_freq: frequency count used to bias eviction within probation
m_probation = OrderedDict()
m_protected = OrderedDict()
m_freq = dict()
_m_last_seen_access = -1  # detect new traces to reset metadata


def _reset_if_new_run(cache_snapshot):
    """Reset metadata when a new trace/cache run starts."""
    global m_probation, m_protected, m_freq, _m_last_seen_access
    # New run if access counter restarts or at very beginning
    if cache_snapshot.access_count <= 1 or _m_last_seen_access > cache_snapshot.access_count:
        m_probation.clear()
        m_protected.clear()
        m_freq.clear()
    _m_last_seen_access = cache_snapshot.access_count


def _prune_metadata(cache_snapshot):
    """Keep metadata consistent with actual cache content."""
    cache_keys = cache_snapshot.cache.keys()
    for seg in (m_probation, m_protected):
        to_del = [k for k in seg.keys() if k not in cache_keys]
        for k in to_del:
            seg.pop(k, None)


def _protected_target_size(cache_snapshot):
    """Aim to keep most entries protected while leaving room in probation."""
    cap = max(int(cache_snapshot.capacity), 1)
    return max(1, int(cap * 0.8))


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global m_probation, m_protected, m_freq
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)

    # Seed metadata if empty by placing all current cache keys into probation.
    if not m_probation and not m_protected and cache_snapshot.cache:
        for k in cache_snapshot.cache.keys():
            m_probation[k] = None

    candid_obj_key = None
    if m_probation:
        # Evict the least frequent in probation; tie-broken by LRU order.
        min_freq = None
        for k in m_probation.keys():
            f = m_freq.get(k, 0)
            if min_freq is None or f < min_freq:
                min_freq = f
                candid_obj_key = k
                if min_freq == 0:
                    break
        if candid_obj_key is None:
            candid_obj_key = next(iter(m_probation))  # fallback to LRU
    elif m_protected:
        candid_obj_key = next(iter(m_protected))  # LRU from protected if probation empty
    else:
        # Fallback: choose any key from the cache
        for k in cache_snapshot.cache.keys():
            candid_obj_key = k
            break
    return candid_obj_key


def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_probation, m_protected, m_freq
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)

    k = obj.key
    # Bump frequency
    m_freq[k] = m_freq.get(k, 0) + 1

    if k in m_protected:
        # Refresh recency in protected
        m_protected.move_to_end(k, last=True)
    elif k in m_probation:
        # Promote to protected on first hit
        m_probation.pop(k, None)
        m_protected[k] = None
    else:
        # Metadata miss but cache hit: treat as hot and place into protected
        m_protected[k] = None

    # Keep protected around its target size; demote LRU if it grows too large.
    target = _protected_target_size(cache_snapshot)
    if len(m_protected) > target:
        demote_k, _ = m_protected.popitem(last=False)  # LRU of protected
        m_probation[demote_k] = None


def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_probation, m_protected, m_freq
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)

    k = obj.key
    # New items start in probation (to filter one-hit wonders)
    if k in m_protected:
        m_protected.pop(k, None)
    if k in m_probation:
        m_probation.pop(k, None)
    m_probation[k] = None

    # Give a small initial credit to favor quick repeats
    m_freq[k] = m_freq.get(k, 0) + 1

    # Ensure protected doesn't monopolize space
    target = _protected_target_size(cache_snapshot)
    while len(m_protected) > target:
        demote_k, _ = m_protected.popitem(last=False)
        m_probation[demote_k] = None


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_probation, m_protected, m_freq
    _reset_if_new_run(cache_snapshot)
    # Remove evicted object from segments; retain frequency for long-term bias.
    m_probation.pop(evicted_obj.key, None)
    m_protected.pop(evicted_obj.key, None)
>>>>>>> REPLACE

</DIFF>