--- a/original.py
+++ b/original.py
@@ -1,408 +1,453 @@
 # EVOLVE-BLOCK-START
 """Adaptive cache eviction using ARC augmented with TinyLFU competitive admission.
 
 Public API:
 - evict(cache_snapshot, obj) -> key
 - update_after_hit(cache_snapshot, obj)
 - update_after_insert(cache_snapshot, obj)
 - update_after_evict(cache_snapshot, obj, evicted_obj)
 """
 
 from collections import OrderedDict
 
 
 class _CmSketch:
     """
     Compact Count-Min Sketch with conservative aging.
     - d hash functions, width w = 2^p (masking for speed).
     - Halves counters periodically to forget stale history.
     """
     __slots__ = ("d", "w", "tables", "mask", "ops", "age_period", "seeds")
 
     def __init__(self, width_power=12, d=3):
         self.d = int(max(1, d))
         w = 1 << int(max(8, width_power))  # at least 256
         self.w = w
         self.mask = w - 1
         self.tables = [[0] * w for _ in range(self.d)]
         self.ops = 0
         self.age_period = max(1024, w)
         self.seeds = (0x9e3779b1, 0x85ebca77, 0xc2b2ae3d, 0x27d4eb2f)
 
     def _hash(self, key_hash: int, i: int) -> int:
         h = key_hash ^ self.seeds[i % len(self.seeds)]
         h ^= (h >> 33) & 0xFFFFFFFFFFFFFFFF
         h *= 0xff51afd7ed558ccd
         h &= 0xFFFFFFFFFFFFFFFF
         h ^= (h >> 33)
         h *= 0xc4ceb9fe1a85ec53
         h &= 0xFFFFFFFFFFFFFFFF
         h ^= (h >> 33)
         return h & self.mask
 
     def _maybe_age(self):
         self.ops += 1
         if self.ops % self.age_period == 0:
             for t in self.tables:
                 for i in range(self.w):
                     t[i] >>= 1
 
     def increment(self, key: str, amount: int = 1):
         h = hash(key)
         for i in range(self.d):
             idx = self._hash(h, i)
             v = self.tables[i][idx] + amount
             if v > 255:
                 v = 255
             self.tables[i][idx] = v
         self._maybe_age()
 
     def estimate(self, key: str) -> int:
         h = hash(key)
         est = 1 << 30
         for i in range(self.d):
             idx = self._hash(h, i)
             v = self.tables[i][idx]
             if v < est:
                 est = v
         return est
 
 
 class _ArcPolicy:
     """
     ARC-like policy enhanced with TinyLFU:
     - T1: recency list (seen once, resident)
     - T2: frequency list (seen >=2 or hot-admitted, resident)
     - B1: ghost of keys evicted from T1
     - B2: ghost of keys evicted from T2
     - p: target size for T1 (adaptive; 0..capacity)
     - sketch: decayed frequency estimator guiding admission and eviction tie-breaking
     """
 
     __slots__ = (
         "T1", "T2", "B1", "B2",
         "p", "capacity", "_last_evicted_from",
-        "sketch", "_sample_k", "_miss_streak", "_scan_mode_until"
+        "sketch", "_sample_k", "_miss_streak", "_scan_mode_until", "_ghost_ops"
     )
 
     def __init__(self):
         self.T1 = OrderedDict()
         self.T2 = OrderedDict()
         self.B1 = OrderedDict()
         self.B2 = OrderedDict()
         self.p = 0.0
         self.capacity = None
         self._last_evicted_from = None  # 'T1' or 'T2'
         self.sketch = _CmSketch(width_power=12, d=3)
         self._sample_k = 6
         self._miss_streak = 0
         self._scan_mode_until = 0
+        self._ghost_ops = 0
 
     # ---------- internal helpers ----------
 
     def _ensure_capacity(self, cap: int):
         # On first call or capacity change, reset safely.
         if self.capacity is None:
             self.capacity = max(int(cap), 1)
             # sample size relative to capacity
             self._sample_k = max(4, min(12, (self.capacity // 8) or 4))
             # make sketch aging responsive to capacity
             try:
                 self.sketch.age_period = max(512, min(16384, self.capacity * 8))
             except Exception:
                 pass
             return
         if self.capacity != cap:
             # Reset state to avoid inconsistencies if framework changes capacity.
             self.T1.clear(); self.T2.clear(); self.B1.clear(); self.B2.clear()
             self.p = 0
             self.capacity = max(int(cap), 1)
             self._sample_k = max(4, min(12, (self.capacity // 8) or 4))
             try:
                 self.sketch.age_period = max(512, min(16384, self.capacity * 8))
             except Exception:
                 pass
 
     def _is_in_cache(self, key: str, cache_snapshot) -> bool:
         return key in cache_snapshot.cache
 
     def _prune_stale_residents(self, cache_snapshot):
         # Drop keys that our policy still tracks but the cache no longer has.
         cache_keys = cache_snapshot.cache.keys()
         for k in list(self.T1.keys()):
             if k not in cache_keys:
                 self.T1.pop(k, None)
         for k in list(self.T2.keys()):
             if k not in cache_keys:
                 self.T2.pop(k, None)
 
     def _prune_ghosts(self):
         cap = self.capacity or 1
         # Prefer canonical ARC bound: |B1| + |B2| â‰¤ capacity.
         # Also drop entries that became resident.
         for k in list(self.B1.keys()):
             if k in self.T1 or k in self.T2:
                 self.B1.pop(k, None)
         for k in list(self.B2.keys()):
             if k in self.T1 or k in self.T2:
                 self.B2.pop(k, None)
+
+        # Increment maintenance counter and periodically age ghosts from the larger list.
+        self._ghost_ops = getattr(self, "_ghost_ops", 0) + 1
+        if self._ghost_ops % max(1, cap) == 0:
+            # Drop ~10% of the larger ghost list from its LRU side to keep history fresh.
+            if len(self.B1) >= len(self.B2):
+                n = max(1, len(self.B1) // 10)
+                for _ in range(n):
+                    if not self.B1:
+                        break
+                    self.B1.popitem(last=False)
+            else:
+                n = max(1, len(self.B2) // 10)
+                for _ in range(n):
+                    if not self.B2:
+                        break
+                    self.B2.popitem(last=False)
+
         while (len(self.B1) + len(self.B2)) > cap:
             if len(self.B1) > int(self.p):
                 self.B1.popitem(last=False)
             else:
                 self.B2.popitem(last=False)
 
     def _touch_T1(self, key: str):
         # Place/move key to MRU end of T1
         self.T1[key] = None
         self.T1.move_to_end(key)
 
     def _touch_T2(self, key: str):
         # Place/move key to MRU end of T2
         self.T2[key] = None
         self.T2.move_to_end(key)
 
     def _move_T1_to_B1(self, key: str):
         self.T1.pop(key, None)
         self.B1[key] = None
         self.B1.move_to_end(key)
 
     def _move_T2_to_B2(self, key: str):
         self.T2.pop(key, None)
         self.B2[key] = None
         self.B2.move_to_end(key)
 
     def _sample_lru_min_freq(self, od: OrderedDict) -> str:
         # Sample k keys from LRU side and return the one with minimal estimated frequency.
         if not od:
             return None
         k = min(self._sample_k, len(od))
         it = iter(od.keys())  # OrderedDict iterates from LRU to MRU
         min_key = None
         min_freq = None
         for _ in range(k):
             key = next(it)
             f = self.sketch.estimate(key)
             if min_freq is None or f < min_freq:
                 min_freq = f
                 min_key = key
         return min_key if min_key is not None else next(iter(od))
 
     # ---------- public hooks called by the cache framework ----------
 
     def choose_victim(self, cache_snapshot, new_obj) -> str:
         """
         ARC + TinyLFU victim selection:
         - If T1 exceeds target p, evict from T1 (sampled LRU-min-freq).
         - If new key is in B2 and |T1| == p, evict from T1 (ARC rule).
         - During detected scan mode, prefer evicting from T1 to protect T2.
         - Else choose between T1 and T2 by comparing TinyLFU(new) vs TinyLFU(candidate_T2).
         """
         self._ensure_capacity(cache_snapshot.capacity)
         self._prune_stale_residents(cache_snapshot)
 
         cand_T1 = self._sample_lru_min_freq(self.T1) if len(self.T1) > 0 else None
         cand_T2 = self._sample_lru_min_freq(self.T2) if len(self.T2) > 0 else None
 
         in_B2 = (new_obj.key in self.B2)
 
         # Scan-mode bias: for the next ~capacity accesses, evict from T1 when possible
         now = cache_snapshot.access_count
         if now < getattr(self, "_scan_mode_until", 0) and cand_T1 is not None:
             self._last_evicted_from = 'T1'
             return cand_T1
 
         # ARC rule: if T1 is larger than target, or (new in B2 and |T1|==p), evict from T1
         if cand_T1 is not None and (len(self.T1) > self.p or (in_B2 and len(self.T1) == int(self.p))):
             self._last_evicted_from = 'T1'
             return cand_T1
 
         # If only one segment is non-empty, evict from it
         if cand_T1 is None and cand_T2 is not None:
             self._last_evicted_from = 'T2'
             return cand_T2
         if cand_T2 is None and cand_T1 is not None:
             self._last_evicted_from = 'T1'
             return cand_T1
 
-        # Both candidates exist: competitive admission via TinyLFU
+        # Both candidates exist: pick the colder candidate first; if close, use TinyLFU(new) vs T2
         if cand_T1 is not None and cand_T2 is not None:
+            f_t1 = self.sketch.estimate(cand_T1)
+            f_t2 = self.sketch.estimate(cand_T2)
+            if f_t1 < f_t2:
+                self._last_evicted_from = 'T1'
+                return cand_T1
+            elif f_t2 < f_t1:
+                self._last_evicted_from = 'T2'
+                return cand_T2
+            # Tie or too close: competitive admission via TinyLFU(new) vs T2-candidate
             f_new = self.sketch.estimate(new_obj.key)
-            f_t2 = self.sketch.estimate(cand_T2)
             if f_new <= f_t2:
                 self._last_evicted_from = 'T1'
                 return cand_T1
             else:
                 self._last_evicted_from = 'T2'
                 return cand_T2
 
         # Fallbacks
         if len(self.T2) > 0:
             self._last_evicted_from = 'T2'
             return self._sample_lru_min_freq(self.T2)
         if len(self.T1) > 0:
             self._last_evicted_from = 'T1'
             return self._sample_lru_min_freq(self.T1)
 
         # Final resort: pick any key from the actual cache
         self._last_evicted_from = 'T1'
         return next(iter(cache_snapshot.cache))
 
     def on_hit(self, cache_snapshot, obj):
         """
         Hit handling:
         - Increment TinyLFU.
-        - If in T1: promote to T2 only if frequency estimate >= threshold, else refresh in T1.
+        - If in T1: promote to T2 only if frequency estimate >= threshold (higher in scan mode), else refresh in T1.
         - If in T2: refresh in T2.
         - If not tracked but cache hit: treat as frequent and place in T2.
+        Also remove any ghost entries for this resident key to keep ghost history clean.
         """
         self._ensure_capacity(cache_snapshot.capacity)
         key = obj.key
         self.sketch.increment(key, 1)
 
         # Any hit breaks a pure-miss streaming streak
         self._miss_streak = 0
 
+        # Remove stale ghost duplicates for resident key
+        self.B1.pop(key, None)
+        self.B2.pop(key, None)
+
+        # Promotion threshold: stricter during scan-mode cooldown
+        now = cache_snapshot.access_count
+        in_scan = now < getattr(self, "_scan_mode_until", 0)
+        promote_threshold = 3 if in_scan else 2
+
         if key in self.T1:
             # Gate promotion to avoid polluting T2 with one-hit wonders
-            if self.sketch.estimate(key) >= 2:
+            if self.sketch.estimate(key) >= promote_threshold:
                 self.T1.pop(key, None)
                 self._touch_T2(key)
             else:
                 self._touch_T1(key)
         elif key in self.T2:
             self._touch_T2(key)
         else:
             # Metadata desync: cache had it; assume it's frequent
             self._touch_T2(key)
+        # Light ghost maintenance
+        self._prune_ghosts()
 
     def on_insert(self, cache_snapshot, obj):
         """
         Insert handling (called on miss after space made, if needed):
         - If key in B1: increase p (favor recency) with damped adaptation and insert into T2.
         - If key in B2: decrease p (favor frequency) with damped adaptation and insert into T2.
         - Else: TinyLFU competitive admission:
             Compare f(new) to a sampled T2 victim; if f(new) > f(victim_T2), insert to T2,
             otherwise insert to T1.
+        - During scan-mode cooldown, avoid direct T2 admissions for non-ghosts to prevent pollution.
         - Track miss streak to detect scans and temporarily bias eviction to T1.
         """
         self._ensure_capacity(cache_snapshot.capacity)
         key = obj.key
         now = cache_snapshot.access_count
         # Count misses as well
         self.sketch.increment(key, 1)
 
         # Update miss streak and detect scans
         self._miss_streak = getattr(self, "_miss_streak", 0) + 1
         if self._miss_streak > (2 * self.capacity) and len(self.T2) < max(1, self.capacity // 10) and len(self.B1) >= len(self.B2):
             # Streaming pattern: strongly prefer recency for a short cooldown window
             self.p = max(0.0, self.p * 0.5)
             self._scan_mode_until = now + self.capacity
 
         alpha = 0.25  # dampening factor for p updates
 
         if key in self.B1:
             delta = float(max(1, len(self.B2) // max(1, len(self.B1))))
             self.p = min(self.capacity, max(0.0, self.p + alpha * delta))
             self.B1.pop(key, None)
             self._touch_T2(key)
         elif key in self.B2:
             delta = float(max(1, len(self.B1) // max(1, len(self.B2))))
             self.p = max(0.0, min(self.capacity, self.p - alpha * delta))
             self.B2.pop(key, None)
             self._touch_T2(key)
         else:
-            f_new = self.sketch.estimate(key)
-            if len(self.T2) > 0:
-                k2 = self._sample_lru_min_freq(self.T2)
-                f_k2 = self.sketch.estimate(k2) if k2 is not None else 0
-                if f_new > f_k2:
-                    self._touch_T2(key)
+            # Scan mode: avoid admitting directly into T2
+            in_scan = now < getattr(self, "_scan_mode_until", 0)
+            if in_scan:
+                self._touch_T1(key)
+            else:
+                f_new = self.sketch.estimate(key)
+                if len(self.T2) > 0:
+                    k2 = self._sample_lru_min_freq(self.T2)
+                    f_k2 = self.sketch.estimate(k2) if k2 is not None else 0
+                    if f_new > f_k2:
+                        self._touch_T2(key)
+                    else:
+                        self._touch_T1(key)
                 else:
-                    self._touch_T1(key)
-            else:
-                # Mild threshold when T2 empty
-                if f_new >= 2:
-                    self._touch_T2(key)
-                else:
-                    self._touch_T1(key)
-
-        # Ensure ghosts are bounded
+                    # Mild threshold when T2 empty
+                    if f_new >= 2:
+                        self._touch_T2(key)
+                    else:
+                        self._touch_T1(key)
+
+        # Ensure ghosts are bounded and maintained
         self._prune_ghosts()
 
     def on_evict(self, cache_snapshot, obj, evicted_obj):
         """
         Eviction handling: move the evicted resident to its corresponding ghost list.
         Maintain bounded ghost metadata and adapt later on insert.
         """
         self._ensure_capacity(cache_snapshot.capacity)
         evk = evicted_obj.key
 
         if evk in self.T1:
             self._move_T1_to_B1(evk)
         elif evk in self.T2:
             self._move_T2_to_B2(evk)
         else:
             # Fall back to our last decision if state was pruned.
             if self._last_evicted_from == 'T1':
                 self.B1[evk] = None
                 self.B1.move_to_end(evk)
             else:
                 self.B2[evk] = None
                 self.B2.move_to_end(evk)
 
         self._prune_ghosts()
 
 
 # Single policy instance reused across calls
 _policy = _ArcPolicy()
 
 
 def evict(cache_snapshot, obj):
     """
     Choose eviction victim key for the incoming obj.
     """
     return _policy.choose_victim(cache_snapshot, obj)
 
 
 def update_after_hit(cache_snapshot, obj):
     """
     Update policy state after a cache hit on obj.
     """
     _policy.on_hit(cache_snapshot, obj)
 
 
 def update_after_insert(cache_snapshot, obj):
     """
     Update policy state after a new obj is inserted into the cache.
     """
     _policy.on_insert(cache_snapshot, obj)
 
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     """
     Update policy state after evicting evicted_obj to make room for obj.
     """
     _policy.on_evict(cache_snapshot, obj, evicted_obj)
 
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate