--- a/original.py
+++ b/original.py
@@ -1,306 +1,442 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
+"""Adaptive Segmented-LRU + TinyLFU policy with modular design and online tuning."""
 
 from collections import OrderedDict
 
-# Segmented LRU with TinyLFU-style bias and adaptive protected sizing.
-# - m_probation: LRU list for new/cold entries (T1)
-# - m_protected: LRU list for hot/promoted entries (T2)
-# - Count-Min Sketch (CMS) for frequency estimates, periodically aged
-# - m_p: smoothed protected ratio target via EMA, steered by hit-rate and scan detector
-
-m_probation = OrderedDict()
-m_protected = OrderedDict()
-
-# TinyLFU Count-Min Sketch parameters/state
-_SKETCH_DEPTH = 4
-m_sketch_w = 0
-m_sketch = []           # list of lists (depth x width)
-m_sketch_ops = 0        # number of updates since last aging
-m_sketch_age_threshold = 0
-
-# Adaptive control
-m_p = 0.5               # protected ratio target (0..1), EMA-smoothed
-m_miss_streak = 0       # consecutive misses
-_m_last_seen_access = -1  # detect new traces to reset metadata
-
-
-def _reset_if_new_run(cache_snapshot):
-    """Reset metadata when a new trace/cache run starts."""
-    global m_probation, m_protected, m_sketch, m_sketch_w, m_sketch_ops, m_sketch_age_threshold
-    global m_p, m_miss_streak, _m_last_seen_access
-    # New run if access counter restarts or at very beginning
-    if cache_snapshot.access_count <= 1 or _m_last_seen_access > cache_snapshot.access_count:
-        m_probation.clear()
-        m_protected.clear()
-        m_sketch_w = 0
-        m_sketch = []
-        m_sketch_ops = 0
-        m_sketch_age_threshold = 0
-        m_p = 0.5
-        m_miss_streak = 0
-    _m_last_seen_access = cache_snapshot.access_count
-
-
-def _ensure_sketch(cache_snapshot):
-    """Initialize the CMS sketch if needed, sizing it relative to capacity."""
-    global m_sketch_w, m_sketch, m_sketch_ops, m_sketch_age_threshold
-    if m_sketch_w:
-        return
-    cap = max(1, int(cache_snapshot.capacity))
-    # Width target between 128 and 16384, scaled by capacity
-    target = max(128, min(16384, cap * 16))
-    # Use power-of-two width for cheaper masking
-    w = 1
-    while w < target:
-        w <<= 1
-    m_sketch_w = w
-    m_sketch = [[0] * m_sketch_w for _ in range(_SKETCH_DEPTH)]
-    m_sketch_ops = 0
-    # Age conservatively: aging cost is O(depth*width); perform infrequently
-    m_sketch_age_threshold = max(m_sketch_w * 8, cap * 64)
-
-
-def _hash_idx(key, i):
-    """Compute index into sketch row i for key."""
-    # Use Python's hash with row index salt; mask for speed (width is power of two)
-    return (hash((key, i, 0x9E3779B97F4A7C15)) & (m_sketch_w - 1))
-
-
-def _sketch_add(cache_snapshot, key, delta=1):
-    """Add delta count for key in the sketch and maybe age."""
-    global m_sketch_ops
-    _ensure_sketch(cache_snapshot)
-    if not m_sketch_w:
-        return
-    for i in range(_SKETCH_DEPTH):
-        idx = _hash_idx(key, i)
-        # Cap growth implicitly via periodic aging
-        m_sketch[i][idx] += delta
-    m_sketch_ops += 1
-    if m_sketch_ops >= m_sketch_age_threshold:
-        # Age by halving counters to forget stale history
-        for i in range(_SKETCH_DEPTH):
-            row = m_sketch[i]
-            for j in range(m_sketch_w):
-                row[j] >>= 1
-        m_sketch_ops = 0
-
-
-def _sketch_est(cache_snapshot, key):
-    """Estimate frequency of key using the sketch (min across rows)."""
-    _ensure_sketch(cache_snapshot)
-    if not m_sketch_w:
-        return 0
-    est = None
-    for i in range(_SKETCH_DEPTH):
-        idx = _hash_idx(key, i)
-        v = m_sketch[i][idx]
-        if est is None or v < est:
-            est = v
-    return est if est is not None else 0
-
-
-def _prune_metadata(cache_snapshot):
-    """Keep metadata consistent with actual cache content."""
-    cache_keys = cache_snapshot.cache.keys()
-    for seg in (m_probation, m_protected):
-        to_del = [k for k in seg.keys() if k not in cache_keys]
-        for k in to_del:
-            seg.pop(k, None)
-
-
-def _seed_from_cache(cache_snapshot):
-    """If both segments are empty but cache has content, seed probation."""
-    if not m_probation and not m_protected and cache_snapshot.cache:
-        for k in cache_snapshot.cache.keys():
-            m_probation[k] = None
-
-
-def _protected_target_size(cache_snapshot):
-    """Adaptive protected target size via EMA of hit-rate with scan sensitivity."""
-    global m_p
-    cap = max(1, int(cache_snapshot.capacity))
-    # Base target driven by current hit-rate (favor protected when HR is higher)
-    hr = (cache_snapshot.hit_count / cache_snapshot.access_count) if cache_snapshot.access_count > 0 else 0.0
-    base = 0.2 + 0.6 * hr  # in [0.2, 0.8]
-    # Scan detector: long miss streak -> bias toward more probation space
-    if m_miss_streak > 2 * cap:
-        base = min(base, 0.3)
-    # Smooth with EMA to avoid oscillation
-    alpha = 0.25
-    m_p = (1 - alpha) * m_p + alpha * base
-    # Clamp and convert to item count
-    m_p = max(0.1, min(0.9, m_p))
-    target = int(round(cap * m_p))
-    if cap > 1:
-        target = max(1, min(cap - 1, target))
-    else:
-        target = 1
-    return target
-
-
-def _eviction_sample(cache_snapshot, seg, sample_k=8):
-    """Sample up to K LRU candidates from seg and return one with lowest freq."""
-    candid = []
-    it = iter(seg.keys())  # iteration yields from LRU to MRU
-    for _ in range(sample_k):
-        try:
-            candid.append(next(it))
-        except StopIteration:
-            break
-    if not candid:
-        # Fallback to pure LRU
-        for k in seg.keys():
-            return k
-        return None
-    # Choose lowest estimated frequency; tie-break by LRU order (first min)
-    best_k = candid[0]
-    best_f = _sketch_est(cache_snapshot, best_k)
-    for k in candid[1:]:
-        f = _sketch_est(cache_snapshot, k)
-        if f < best_f:
-            best_f = f
-            best_k = k
-            if best_f == 0:
+# ----------------------------
+# TinyLFU: capacity-aware sketch with conservative updates and adaptive aging
+# ----------------------------
+class TinyLFUSketch:
+    def __init__(self):
+        self.depth = 4
+        self.w = 0
+        self.rows = []
+        self.ops = 0
+        self.age_period = 0
+        self.min_age = 0
+        self.max_age = 0
+
+    @staticmethod
+    def _next_pow2(x: int) -> int:
+        p = 1
+        while p < x:
+            p <<= 1
+        return p
+
+    def ensure(self, capacity: int):
+        if self.w:
+            return
+        cap = max(1, int(capacity))
+        target = max(512, 4 * cap)
+        self.w = self._next_pow2(target)
+        self.rows = [[0] * self.w for _ in range(self.depth)]
+        # Age bounds ~ [4C, 16C]
+        self.min_age = max(4 * cap, self.w // 4)
+        self.max_age = max(16 * cap, self.w)
+        # Start mid-way
+        self.age_period = (self.min_age + self.max_age) // 2
+        self.ops = 0
+
+    def _idx(self, key, i):
+        return (hash((key, i, 0x9E3779B97F4A7C15)) & (self.w - 1))
+
+    def estimate(self, key) -> int:
+        if not self.w:
+            return 0
+        est = None
+        for i in range(self.depth):
+            v = self.rows[i][self._idx(key, i)]
+            est = v if est is None or v < est else est
+        return est or 0
+
+    def add(self, key, delta=1, conservative=True):
+        if not self.w:
+            return
+        # Conservative update: increment only counters equal to current min
+        idxs = [self._idx(key, i) for i in range(self.depth)]
+        vals = [self.rows[i][idxs[i]] for i in range(self.depth)]
+        m = min(vals)
+        for i in range(self.depth):
+            if not conservative or self.rows[i][idxs[i]] == m:
+                self.rows[i][idxs[i]] = self.rows[i][idxs[i]] + delta
+        self.ops += 1
+        if self.ops >= self.age_period:
+            for i in range(self.depth):
+                row = self.rows[i]
+                # In-place halving
+                for j in range(self.w):
+                    row[j] >>= 1
+            self.ops = 0
+
+    def retune_age(self, tighter: bool):
+        # Move age_period toward min_age (tighter, faster forgetting) or max_age (slower)
+        if tighter:
+            self.age_period = max(self.min_age, self.age_period - max(1, (self.age_period - self.min_age) // 4))
+        else:
+            self.age_period = min(self.max_age, self.age_period + max(1, (self.max_age - self.age_period) // 4))
+
+
+# ----------------------------
+# Segmented LRU: probationary (W1) + protected (W2)
+# ----------------------------
+class SegmentedLRU:
+    def __init__(self):
+        self.prob = OrderedDict()     # W1: probationary
+        self.prot = OrderedDict()     # W2: protected
+
+    def prune(self, live_keys):
+        live = set(live_keys)
+        for seg in (self.prob, self.prot):
+            dead = [k for k in seg.keys() if k not in live]
+            for k in dead:
+                seg.pop(k, None)
+
+    def seed_from_cache(self, live_keys):
+        if not self.prob and not self.prot:
+            for k in live_keys:
+                self.prob[k] = None
+
+    def in_prob(self, k): return k in self.prob
+    def in_prot(self, k): return k in self.prot
+    def touch_prob(self, k): 
+        if k in self.prob: self.prob.move_to_end(k, last=True)
+    def touch_prot(self, k): 
+        if k in self.prot: self.prot.move_to_end(k, last=True)
+
+    def remove(self, k):
+        self.prob.pop(k, None)
+        self.prot.pop(k, None)
+
+    def insert_prob_mru(self, k):
+        self.remove(k)
+        self.prob[k] = None
+
+    def promote_to_prot(self, k):
+        self.prob.pop(k, None)
+        self.prot[k] = None
+
+    def demote_prot_lru_to_prob(self):
+        if not self.prot:
+            return None
+        k, _ = self.prot.popitem(last=False)
+        self.prob[k] = None
+        return k
+
+    def ensure_protected_target(self, target_count: int):
+        # Demote oldest until protected size <= target
+        while len(self.prot) > target_count:
+            self.demote_prot_lru_to_prob()
+
+    def choose_sampled_lru(self, which: str, est_func, k: int = 8):
+        seg = self.prob if which == "prob" else self.prot
+        if not seg:
+            return None, None
+        # Iterate from LRU to MRU; pick candidate with lowest estimated frequency
+        it = iter(seg.keys())
+        cand = []
+        for _ in range(k):
+            try:
+                cand.append(next(it))
+            except StopIteration:
                 break
-    return best_k
+        if not cand:
+            return None, None
+        best = cand[0]
+        best_f = est_func(best)
+        for c in cand[1:]:
+            f = est_func(c)
+            if f < best_f:
+                best_f = f
+                best = c
+                if best_f == 0:
+                    break
+        return best, best_f
+
+
+# ----------------------------
+# Recent ring (tiny recency bias for new phases)
+# ----------------------------
+class RecentRing:
+    def __init__(self):
+        self.q = OrderedDict()
+        self.limit = 0
+
+    def ensure_limit(self, limit: int):
+        self.limit = max(1, int(limit))
+
+    def note(self, k):
+        if self.limit <= 0:
+            return
+        if k in self.q:
+            self.q.move_to_end(k, last=True)
+        else:
+            self.q[k] = None
+        while len(self.q) > self.limit:
+            self.q.popitem(last=False)
+
+    def has(self, k) -> bool:
+        return k in self.q
+
+
+# ----------------------------
+# Policy Orchestrator
+# ----------------------------
+class Policy:
+    def __init__(self):
+        self.inited = False
+        self.last_access_seen = -1
+        self.sketch = TinyLFUSketch()
+        self.slru = SegmentedLRU()
+        self.recent = RecentRing()
+
+        # Adaptive control
+        self.prot_frac = 0.7  # target protected fraction of capacity
+        self.miss_streak = 0
+
+        # Stats for tuning
+        self.hits_prob = 0
+        self.hits_prot = 0
+        self.promotions = 0
+        self.demotions = 0
+
+        self.last_tune_access = 0
+        self.last_hits = 0
+
+    # -------------
+    # Lifecycle
+    # -------------
+    def _reset(self, capacity):
+        self.inited = True
+        self.last_access_seen = -1
+        self.sketch = TinyLFUSketch()
+        self.sketch.ensure(capacity)
+        self.slru = SegmentedLRU()
+        self.recent = RecentRing()
+        self.recent.ensure_limit(capacity)
+        self.prot_frac = 0.7
+        self.miss_streak = 0
+        self.hits_prob = 0
+        self.hits_prot = 0
+        self.promotions = 0
+        self.demotions = 0
+        self.last_tune_access = 0
+        self.last_hits = 0
+
+    def _ensure_run(self, cache_snapshot):
+        # Reset when a new run is detected
+        if not self.inited or cache_snapshot.access_count <= 1 or self.last_access_seen > cache_snapshot.access_count:
+            self._reset(max(1, int(cache_snapshot.capacity)))
+        self.last_access_seen = cache_snapshot.access_count
+        # Ensure sketch and ring are sized
+        self.sketch.ensure(max(1, int(cache_snapshot.capacity)))
+        self.recent.ensure_limit(max(1, int(cache_snapshot.capacity)))
+        # Sync metadata with actual cache
+        self.slru.prune(cache_snapshot.cache.keys())
+        if not self.slru.prob and not self.slru.prot and cache_snapshot.cache:
+            self.slru.seed_from_cache(cache_snapshot.cache.keys())
+        # Maintain protected target
+        self._enforce_protected_target(cache_snapshot)
+
+    # -------------
+    # Adaptation
+    # -------------
+    def _protected_target_count(self, capacity):
+        cap = max(1, int(capacity))
+        tgt = int(round(self.prot_frac * cap))
+        if cap > 1:
+            tgt = max(1, min(cap - 1, tgt))
+        else:
+            tgt = 1
+        return tgt
+
+    def _enforce_protected_target(self, cache_snapshot):
+        target = self._protected_target_count(cache_snapshot.capacity)
+        self.slru.ensure_protected_target(target)
+
+    def _tune(self, cache_snapshot):
+        # Periodically adapt prot_frac and sketch age based on segment performance and miss streak
+        period = max(256, int(max(1, cache_snapshot.capacity)))
+        if cache_snapshot.access_count - self.last_tune_access < period:
+            return
+        access_delta = cache_snapshot.access_count - self.last_tune_access
+        hit_delta = cache_snapshot.hit_count - self.last_hits
+        hr = (hit_delta / access_delta) if access_delta > 0 else 0.0
+
+        # Adjust protected fraction:
+        # - If probation hits dominate or long miss streak -> shrink protected (favor recency)
+        # - If protected hits dominate -> grow protected (retain hot items)
+        total_seg_hits = self.hits_prob + self.hits_prot
+        prob_share = (self.hits_prob / total_seg_hits) if total_seg_hits > 0 else 0.5
+        prot_share = 1.0 - prob_share
+
+        if self.miss_streak > 2 * cache_snapshot.capacity or prob_share > 0.6:
+            self.prot_frac = max(0.55, self.prot_frac - 0.05)
+            self.sketch.retune_age(tighter=True)   # forget faster during scans/recency phases
+        elif prot_share > 0.7 and hr > 0.2:
+            self.prot_frac = min(0.9, self.prot_frac + 0.05)
+            self.sketch.retune_age(tighter=False)  # preserve long-term during steady hot sets
+
+        # Reset segment counters for next window
+        self.hits_prob = 0
+        self.hits_prot = 0
+        self.promotions = 0
+        self.demotions = 0
+        self.last_tune_access = cache_snapshot.access_count
+        self.last_hits = cache_snapshot.hit_count
+
+    # -------------
+    # Helpers
+    # -------------
+    def _freq_estimate(self, key):
+        base = self.sketch.estimate(key)
+        # Small recent bias
+        if self.recent.has(key):
+            base += 1
+        return base
+
+    def _choose_victim(self, cache_snapshot, new_key):
+        # Two-way sampling with bias toward probation
+        k1, f1 = self.slru.choose_sampled_lru("prob", self._freq_estimate, k=8)
+        k2, f2 = self.slru.choose_sampled_lru("prot", self._freq_estimate, k=6)
+        if k1 is None and k2 is None:
+            # Fallback to any key from cache
+            for k in cache_snapshot.cache.keys():
+                return k
+            return None
+        if k1 is None:
+            return k2
+        if k2 is None:
+            return k1
+
+        # Bias: normally evict from probation unless clearly colder in protected
+        bias = 1
+        if self.miss_streak > cache_snapshot.capacity:
+            bias = 2  # stronger W1 preference during scans
+
+        f_new = self._freq_estimate(new_key)
+
+        # If new item is hot, allow evicting from protected if its LRU is much colder
+        # Otherwise, prefer probation unless protected LRU is strictly colder beyond bias.
+        if f_new >= (f1 + f2) // 2:
+            # Hot admission: pick the colder among the two
+            return k1 if (f1 <= f2) else k2
+        else:
+            # Default path with bias to probation
+            return k1 if (f1 <= f2 + bias) else k2
+
+    # -------------
+    # Public API (wired into framework)
+    # -------------
+    def on_evict(self, cache_snapshot, obj):
+        self._ensure_run(cache_snapshot)
+        victim = self._choose_victim(cache_snapshot, obj.key)
+        return victim
+
+    def on_hit(self, cache_snapshot, obj):
+        self._ensure_run(cache_snapshot)
+        k = obj.key
+        self.miss_streak = 0
+        # Learn frequency conservatively
+        self.sketch.add(k, 1, conservative=True)
+        self.recent.note(k)
+
+        if self.slru.in_prot(k):
+            self.slru.touch_prot(k)
+            self.hits_prot += 1
+        elif self.slru.in_prob(k):
+            # Promote on probation hit
+            self.slru.promote_to_prot(k)
+            self.hits_prob += 1
+            self.promotions += 1
+        else:
+            # Metadata miss but cache hit: reinsert guided by freq
+            if self._freq_estimate(k) >= 2:
+                self.slru.prot[k] = None
+            else:
+                self.slru.prob[k] = None
+
+        # Keep protected near target via demotions
+        before = len(self.slru.prot)
+        self._enforce_protected_target(cache_snapshot)
+        self.demotions += max(0, before - len(self.slru.prot))
+
+        # Periodic tuning
+        self._tune(cache_snapshot)
+
+    def on_insert(self, cache_snapshot, obj):
+        self._ensure_run(cache_snapshot)
+        k = obj.key
+        self.miss_streak += 1
+        # Doorkeeper credit
+        self.sketch.add(k, 1, conservative=True)
+        self.recent.note(k)
+
+        # Always place new items into probation to avoid polluting protected
+        self.slru.remove(k)
+        self.slru.prob[k] = None
+
+        # Maintain protected capacity bound
+        self._enforce_protected_target(cache_snapshot)
+        self._tune(cache_snapshot)
+
+    def on_post_evict(self, cache_snapshot, obj, evicted_obj):
+        self._ensure_run(cache_snapshot)
+        evk = evicted_obj.key
+        # Remove from metadata but keep sketch memory to bias future admissions
+        self.slru.remove(evk)
+        # Note evicted in recent ring to help immediate re-entries avoid thrash
+        self.recent.note(evk)
+        # No tuning needed here
+
+
+# Global singleton policy
+_POL = Policy()
 
 
 def evict(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm chooses the eviction victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The new object that needs to be inserted into the cache.
-    - Return:
-        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
-    '''
-    _reset_if_new_run(cache_snapshot)
-    _prune_metadata(cache_snapshot)
-    _ensure_sketch(cache_snapshot)
-    _seed_from_cache(cache_snapshot)
-
-    # Prefer evicting from probation; fall back to protected when necessary.
-    if m_probation:
-        victim = _eviction_sample(cache_snapshot, m_probation, sample_k=8)
-        if victim is not None:
-            return victim
-    if m_protected:
-        victim = _eviction_sample(cache_snapshot, m_protected, sample_k=8)
-        if victim is not None:
-            return victim
-    # Fallback: choose any key from the cache
-    for k in cache_snapshot.cache.keys():
-        return k
-    return None
+    """
+    Choose an eviction victim using two-way sampled SLRU + TinyLFU guidance.
+    """
+    return _POL.on_evict(cache_snapshot, obj)
 
 
 def update_after_hit(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object accessed during the cache hit.
-    - Return: `None`
-    '''
-    global m_miss_streak
-    _reset_if_new_run(cache_snapshot)
-    _prune_metadata(cache_snapshot)
-    _ensure_sketch(cache_snapshot)
-
-    k = obj.key
-    m_miss_streak = 0
-    # Learn frequency
-    _sketch_add(cache_snapshot, k, 1)
-
-    if k in m_protected:
-        # Refresh recency in protected
-        m_protected.move_to_end(k, last=True)
-    elif k in m_probation:
-        # Promote only if sufficiently frequent; else refresh in probation
-        if _sketch_est(cache_snapshot, k) >= 2:
-            m_probation.pop(k, None)
-            m_protected[k] = None
-        else:
-            m_probation.move_to_end(k, last=True)
-    else:
-        # Metadata miss but cache hit: re-admit conservatively
-        if _sketch_est(cache_snapshot, k) >= 2:
-            m_protected[k] = None
-        else:
-            m_probation[k] = None
-
-    # Keep protected near its target by demoting oldest if needed
-    target = _protected_target_size(cache_snapshot)
-    while len(m_protected) > target:
-        demote_k, _ = m_protected.popitem(last=False)  # LRU of protected
-        m_probation[demote_k] = None
+    """
+    Update metadata on cache hit: conservative frequency learning and SLRU promotion.
+    """
+    _POL.on_hit(cache_snapshot, obj)
 
 
 def update_after_insert(cache_snapshot, obj):
-    '''
-    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object that was just inserted into the cache.
-    - Return: `None`
-    '''
-    global m_miss_streak
-    _reset_if_new_run(cache_snapshot)
-    _prune_metadata(cache_snapshot)
-    _ensure_sketch(cache_snapshot)
-
-    k = obj.key
-    m_miss_streak += 1
-
-    # New items enter probation (doorkeeper credit to sketch)
-    _sketch_add(cache_snapshot, k, 1)
-    m_protected.pop(k, None)
-    m_probation.pop(k, None)
-    m_probation[k] = None
-
-    # Ensure protected doesn't exceed its target (shouldn't change on insert,
-    # but keep the invariant in case of out-of-band changes)
-    target = _protected_target_size(cache_snapshot)
-    while len(m_protected) > target:
-        demote_k, _ = m_protected.popitem(last=False)
-        m_probation[demote_k] = None
+    """
+    Update metadata on cache insert (miss path): doorkeeper credit and probation admission.
+    """
+    _POL.on_insert(cache_snapshot, obj)
 
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
-    '''
-    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object to be inserted into the cache.
-        - `evicted_obj`: The object that was just evicted from the cache.
-    - Return: `None`
-    '''
-    _reset_if_new_run(cache_snapshot)
-    # Remove evicted object from segments; keep sketch counts to preserve long-term bias
-    evk = evicted_obj.key
-    m_probation.pop(evk, None)
-    m_protected.pop(evk, None)
-
+    """
+    Update metadata after eviction: purge from segments (keep sketch long-term memory).
+    """
+    _POL.on_post_evict(cache_snapshot, obj, evicted_obj)
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate