<NAME>
arc_tinylfu_sampling_aging
</NAME>

<DESCRIPTION>
I propose enhancing the current ARC-inspired policy with three targeted improvements that are lightweight yet effective across mixed workloads:

1) Frequency-aware victim choice via LRU-tail sampling: Instead of always evicting the strict LRU in the chosen ARC segment, sample a small number of keys from the LRU end and evict the one with the lowest observed frequency. This helps protect moderately reused items against one-timers that happen to be older, reducing misses in heavy-tail regimes.

2) Tiny aging on frequency counters: Add periodic halving of the simple per-key frequency map to forget stale history. This prevents old hot items from being overprotected and improves responsiveness to phase changes.

3) Protected segment hygiene: Enforce an adaptive protected size (≈80% of capacity) by demoting the LRU of protected back to probation when over capacity. Promotion from probation is gated by current protected pressure, reducing pollution during bursts.

Additionally, I tighten ghost management by bounding the combined ghost size |B1| + |B2| ≤ capacity.

These changes are consistent with ARC’s core adaptation (m_p), improve victim selection without significant overhead, and add mild scan-resistance through stricter promotion when protected is full. They require minimal code changes and reuse existing metadata.

</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
m_probation = OrderedDict()
m_protected = OrderedDict()
m_b1 = OrderedDict()
m_b2 = OrderedDict()
m_freq = dict()
m_p = 0
_m_last_seen_access = -1  # detect new traces to reset metadata
=======
m_probation = OrderedDict()
m_protected = OrderedDict()
m_b1 = OrderedDict()
m_b2 = OrderedDict()
m_freq = dict()
m_p = 0
_m_last_seen_access = -1  # detect new traces to reset metadata

# Sampling and light frequency aging
m_sample_k = 6
_age_ops = 0
_age_period = 1024
m_miss_streak = 0
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def _reset_if_new_run(cache_snapshot):
    """Reset metadata when a new trace/cache run starts."""
    global m_probation, m_protected, m_b1, m_b2, m_freq, m_p, _m_last_seen_access
    # New run if access counter restarts or at very beginning
    if cache_snapshot.access_count <= 1 or _m_last_seen_access > cache_snapshot.access_count:
        m_probation.clear()
        m_protected.clear()
        m_b1.clear()
        m_b2.clear()
        m_freq.clear()
        m_p = 0
    _m_last_seen_access = cache_snapshot.access_count
=======
def _reset_if_new_run(cache_snapshot):
    """Reset metadata when a new trace/cache run starts."""
    global m_probation, m_protected, m_b1, m_b2, m_freq, m_p, _m_last_seen_access, m_sample_k, _age_ops, _age_period, m_miss_streak
    # New run if access counter restarts or at very beginning
    if cache_snapshot.access_count <= 1 or _m_last_seen_access > cache_snapshot.access_count:
        m_probation.clear()
        m_protected.clear()
        m_b1.clear()
        m_b2.clear()
        m_freq.clear()
        m_p = 0
        _age_ops = 0
        m_miss_streak = 0
    # keep aging period and sample size responsive to capacity
    cap = max(int(cache_snapshot.capacity), 1)
    _age_period = max(512, cap * 8)
    m_sample_k = max(4, min(12, (cap // 8) or 4))
    _m_last_seen_access = cache_snapshot.access_count
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def _protected_target_size(cache_snapshot):
    """Aim to keep most entries protected while leaving room in probation."""
    cap = max(int(cache_snapshot.capacity), 1)
    return max(1, int(cap * 0.8))
=======
def _protected_target_size(cache_snapshot):
    """Aim to keep most entries protected while leaving room in probation."""
    cap = max(int(cache_snapshot.capacity), 1)
    return max(1, int(cap * 0.8))


def _maybe_age_freq(cache_snapshot):
    """Periodically halve frequency counters to forget stale history."""
    global _age_ops, _age_period, m_freq
    _age_ops += 1
    if _age_ops % max(1, _age_period) == 0:
        for k in list(m_freq.keys()):
            v = m_freq.get(k, 0) >> 1
            if v <= 0:
                # keep dictionary small
                m_freq.pop(k, None)
            else:
                m_freq[k] = v


def _sample_lowfreq(od: OrderedDict) -> str:
    """Sample a few keys from the LRU side and pick the lowest-frequency candidate."""
    if not od:
        return None
    k = min(m_sample_k, len(od))
    it = iter(od.keys())  # LRU -> MRU
    best_k = None
    best_f = None
    for _ in range(k):
        key = next(it)
        f = m_freq.get(key, 0)
        if best_f is None or f < best_f:
            best_f = f
            best_k = key
    return best_k if best_k is not None else next(iter(od))


def _enforce_protected_target(cache_snapshot):
    """Demote LRU of protected to probation until protected meets its target."""
    target = _protected_target_size(cache_snapshot)
    while len(m_protected) > target:
        demote_k, _ = m_protected.popitem(last=False)  # LRU of protected
        # place demoted key at MRU of probation to avoid immediate eviction
        m_probation[demote_k] = None
        m_probation.move_to_end(demote_k, last=True)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global m_probation, m_protected, m_b1, m_b2, m_freq, m_p
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)

    # Seed metadata if empty by placing all current cache keys into probation.
    if not m_probation and not m_protected and cache_snapshot.cache:
        for k0 in cache_snapshot.cache.keys():
            m_probation[k0] = None

    # ARC: adjust target m_p based on ghost hit of incoming key
    cap = max(int(cache_snapshot.capacity), 1)
    k = obj.key
    if k in m_b1:
        delta = max(1, len(m_b2) // max(1, len(m_b1)))
        m_p = min(cap, m_p + delta)
    elif k in m_b2:
        delta = max(1, len(m_b1) // max(1, len(m_b2)))
        m_p = max(0, m_p - delta)

    # ARC replacement decision (do not mutate structures here)
    candid_obj_key = None
    if m_probation and ((k in m_b2 and len(m_probation) == m_p) or (len(m_probation) > m_p)):
        # Evict LRU from probation (T1)
        candid_obj_key = next(iter(m_probation))
    elif m_protected:
        # Evict LRU from protected (T2)
        candid_obj_key = next(iter(m_protected))
    elif m_probation:
        candid_obj_key = next(iter(m_probation))
    else:
        # Fallback: choose any key from the cache
        for k_any in cache_snapshot.cache.keys():
            candid_obj_key = k_any
            break
    return candid_obj_key
=======
def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global m_probation, m_protected, m_b1, m_b2, m_freq, m_p
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)

    # Seed metadata if empty by placing all current cache keys into probation.
    if not m_probation and not m_protected and cache_snapshot.cache:
        for k0 in cache_snapshot.cache.keys():
            m_probation[k0] = None

    # ARC: adjust target m_p based on ghost hit of incoming key
    cap = max(int(cache_snapshot.capacity), 1)
    k = obj.key
    if k in m_b1:
        delta = max(1, len(m_b2) // max(1, len(m_b1)))
        m_p = min(cap, m_p + delta)
    elif k in m_b2:
        delta = max(1, len(m_b1) // max(1, len(m_b2)))
        m_p = max(0, m_p - delta)

    # ARC replacement decision (do not mutate structures here)
    if m_probation and ((k in m_b2 and len(m_probation) == m_p) or (len(m_probation) > m_p)):
        # Evict low-frequency among LRU tail of probation (T1)
        candid_obj_key = _sample_lowfreq(m_probation)
    elif m_protected:
        # Evict low-frequency among LRU tail of protected (T2)
        candid_obj_key = _sample_lowfreq(m_protected)
    elif m_probation:
        candid_obj_key = _sample_lowfreq(m_probation)
    else:
        # Fallback: choose any key from the cache
        candid_obj_key = next(iter(cache_snapshot.cache))
    return candid_obj_key
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_probation, m_protected, m_b1, m_b2, m_freq
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)

    k = obj.key
    # Bump lightweight frequency (not central to ARC but harmless)
    m_freq[k] = m_freq.get(k, 0) + 1

    if k in m_protected:
        # Refresh recency in protected
        m_protected.move_to_end(k, last=True)
    elif k in m_probation:
        # Promote to protected on hit
        m_probation.pop(k, None)
        m_protected[k] = None
    else:
        # Metadata miss but cache hit: treat as hot and place into protected
        m_protected[k] = None
=======
def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_probation, m_protected, m_b1, m_b2, m_freq, m_miss_streak
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)
    _maybe_age_freq(cache_snapshot)

    k = obj.key
    # Bump lightweight frequency
    m_freq[k] = m_freq.get(k, 0) + 1

    # Any hit breaks a miss streak
    m_miss_streak = 0

    if k in m_protected:
        # Refresh recency in protected
        m_protected.move_to_end(k, last=True)
    elif k in m_probation:
        # Gate promotion by current protected pressure
        target = _protected_target_size(cache_snapshot)
        promote_thr = 3 if len(m_protected) >= target else 2
        if m_freq.get(k, 0) >= promote_thr:
            m_probation.pop(k, None)
            m_protected[k] = None
        else:
            # Refresh recency in probation
            m_probation.move_to_end(k, last=True)
    else:
        # Metadata miss but cache hit: treat as hot and place into protected
        m_protected[k] = None

    # Keep protected within target by demoting LRU when necessary
    _enforce_protected_target(cache_snapshot)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_probation, m_protected, m_b1, m_b2, m_freq
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)

    k = obj.key
    # Remove any stale placements
    m_protected.pop(k, None)
    m_probation.pop(k, None)

    # ARC admission: if key was in ghosts, insert to protected (T2); else to probation (T1)
    if k in m_b1:
        m_b1.pop(k, None)
        m_protected[k] = None
    elif k in m_b2:
        m_b2.pop(k, None)
        m_protected[k] = None
    else:
        m_probation[k] = None

    # Light frequency credit
    m_freq[k] = m_freq.get(k, 0) + 1

    # Bound ghost sizes to capacity
    cap = max(int(cache_snapshot.capacity), 1)
    while len(m_b1) > cap:
        m_b1.popitem(last=False)
    while len(m_b2) > cap:
        m_b2.popitem(last=False)
=======
def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_probation, m_protected, m_b1, m_b2, m_freq, m_miss_streak
    _reset_if_new_run(cache_snapshot)
    _prune_metadata(cache_snapshot)
    _maybe_age_freq(cache_snapshot)

    k = obj.key
    # Remove any stale placements
    m_protected.pop(k, None)
    m_probation.pop(k, None)

    # ARC admission: if key was in ghosts, insert to protected (T2); else to probation (T1)
    if k in m_b1:
        m_b1.pop(k, None)
        m_protected[k] = None
    elif k in m_b2:
        m_b2.pop(k, None)
        m_protected[k] = None
    else:
        m_probation[k] = None

    # Light frequency credit and miss streak for scan detection
    m_freq[k] = m_freq.get(k, 0) + 1
    m_miss_streak += 1

    # Keep protected within target by demoting LRU when necessary
    _enforce_protected_target(cache_snapshot)

    # Bound combined ghost sizes to capacity
    cap = max(int(cache_snapshot.capacity), 1)
    while (len(m_b1) + len(m_b2)) > cap:
        if len(m_b1) > len(m_b2):
            m_b1.popitem(last=False)
        else:
            m_b2.popitem(last=False)
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_probation, m_protected, m_b1, m_b2
    _reset_if_new_run(cache_snapshot)
    # Move evicted object into appropriate ghost list
    k = evicted_obj.key
    if k in m_probation:
        m_probation.pop(k, None)
        m_b1.pop(k, None)
        m_b1[k] = None  # MRU of B1
    elif k in m_protected:
        m_protected.pop(k, None)
        m_b2.pop(k, None)
        m_b2[k] = None  # MRU of B2
    else:
        # No segment info; do nothing
        pass

    # Bound ghost sizes to capacity
    cap = max(int(cache_snapshot.capacity), 1)
    while len(m_b1) > cap:
        m_b1.popitem(last=False)
    while len(m_b2) > cap:
        m_b2.popitem(last=False)
=======
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_probation, m_protected, m_b1, m_b2
    _reset_if_new_run(cache_snapshot)
    # Move evicted object into appropriate ghost list
    k = evicted_obj.key
    if k in m_probation:
        m_probation.pop(k, None)
        m_b1.pop(k, None)
        m_b1[k] = None  # MRU of B1
    elif k in m_protected:
        m_protected.pop(k, None)
        m_b2.pop(k, None)
        m_b2[k] = None  # MRU of B2
    else:
        # No segment info; do nothing
        pass

    # Bound combined ghost sizes to capacity
    cap = max(int(cache_snapshot.capacity), 1)
    while (len(m_b1) + len(m_b2)) > cap:
        if len(m_b1) > len(m_b2):
            m_b1.popitem(last=False)
        else:
            m_b2.popitem(last=False)
>>>>>>> REPLACE

</DIFF>