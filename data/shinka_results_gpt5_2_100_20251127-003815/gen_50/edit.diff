--- a/original.py
+++ b/original.py
@@ -1,246 +1,446 @@
 # EVOLVE-BLOCK-START
-"""Adaptive cache eviction using an ARC-like policy with ghost history.
+"""Adaptive cache eviction using ARC augmented with TinyLFU competitive admission.
 
 Public API:
 - evict(cache_snapshot, obj) -> key
 - update_after_hit(cache_snapshot, obj)
 - update_after_insert(cache_snapshot, obj)
 - update_after_evict(cache_snapshot, obj, evicted_obj)
 """
 
 from collections import OrderedDict
 
+
+class _CmSketch:
+    """
+    Compact Count-Min Sketch with conservative aging.
+    - d hash functions, width w = 2^p (masking for speed).
+    - Halves counters periodically to forget stale history.
+    """
+    __slots__ = ("d", "w", "tables", "mask", "ops", "age_period", "seeds")
+
+    def __init__(self, width_power=12, d=3):
+        self.d = int(max(1, d))
+        w = 1 << int(max(8, width_power))  # at least 256
+        self.w = w
+        self.mask = w - 1
+        self.tables = [[0] * w for _ in range(self.d)]
+        self.ops = 0
+        self.age_period = max(1024, w)
+        self.seeds = (0x9e3779b1, 0x85ebca77, 0xc2b2ae3d, 0x27d4eb2f)
+
+    def _hash(self, key_hash: int, i: int) -> int:
+        h = key_hash ^ self.seeds[i % len(self.seeds)]
+        h ^= (h >> 33) & 0xFFFFFFFFFFFFFFFF
+        h *= 0xff51afd7ed558ccd
+        h &= 0xFFFFFFFFFFFFFFFF
+        h ^= (h >> 33)
+        h *= 0xc4ceb9fe1a85ec53
+        h &= 0xFFFFFFFFFFFFFFFF
+        h ^= (h >> 33)
+        return h & self.mask
+
+    def _maybe_age(self):
+        self.ops += 1
+        if self.ops % self.age_period == 0:
+            for t in self.tables:
+                for i in range(self.w):
+                    t[i] >>= 1
+
+    def increment(self, key: str, amount: int = 1):
+        h = hash(key)
+        for i in range(self.d):
+            idx = self._hash(h, i)
+            v = self.tables[i][idx] + amount
+            if v > 255:
+                v = 255
+            self.tables[i][idx] = v
+        self._maybe_age()
+
+    def estimate(self, key: str) -> int:
+        h = hash(key)
+        est = 1 << 30
+        for i in range(self.d):
+            idx = self._hash(h, i)
+            v = self.tables[i][idx]
+            if v < est:
+                est = v
+        return est
+
+
 class _ArcPolicy:
     """
-    ARC-like policy:
+    ARC-like policy enhanced with TinyLFU:
     - T1: recency list (seen once, resident)
-    - T2: frequency list (seen >=2, resident)
+    - T2: frequency list (seen >=2 or hot-admitted, resident)
     - B1: ghost of keys evicted from T1
     - B2: ghost of keys evicted from T2
     - p: target size for T1 (adaptive; 0..capacity)
+    - sketch: decayed frequency estimator guiding admission and eviction tie-breaking
     """
 
     __slots__ = (
         "T1", "T2", "B1", "B2",
-        "p", "capacity", "_last_evicted_from"
+        "p", "capacity", "_last_evicted_from",
+        "sketch", "_sample_k", "_miss_streak", "_scan_mode_until", "_ghost_ops"
     )
 
     def __init__(self):
         self.T1 = OrderedDict()
         self.T2 = OrderedDict()
         self.B1 = OrderedDict()
         self.B2 = OrderedDict()
-        self.p = 0
+        self.p = 0.0
         self.capacity = None
         self._last_evicted_from = None  # 'T1' or 'T2'
+        self.sketch = _CmSketch(width_power=12, d=3)
+        self._sample_k = 6
+        self._miss_streak = 0
+        self._scan_mode_until = 0
+        self._ghost_ops = 0
 
     # ---------- internal helpers ----------
 
     def _ensure_capacity(self, cap: int):
         # On first call or capacity change, reset safely.
         if self.capacity is None:
             self.capacity = max(int(cap), 1)
+            # sample size relative to capacity
+            self._sample_k = max(4, min(12, (self.capacity // 8) or 4))
+            # make sketch aging responsive to capacity
+            try:
+                self.sketch.age_period = max(512, min(16384, self.capacity * 8))
+            except Exception:
+                pass
             return
         if self.capacity != cap:
             # Reset state to avoid inconsistencies if framework changes capacity.
             self.T1.clear(); self.T2.clear(); self.B1.clear(); self.B2.clear()
-            self.p = 0
+            self.p = 0.0
             self.capacity = max(int(cap), 1)
-
-    def _is_in_cache(self, key: str, cache_snapshot) -> bool:
-        return key in cache_snapshot.cache
+            self._sample_k = max(4, min(12, (self.capacity // 8) or 4))
+            try:
+                self.sketch.age_period = max(512, min(16384, self.capacity * 8))
+            except Exception:
+                pass
 
     def _prune_stale_residents(self, cache_snapshot):
         # Drop keys that our policy still tracks but the cache no longer has.
         cache_keys = cache_snapshot.cache.keys()
         for k in list(self.T1.keys()):
             if k not in cache_keys:
                 self.T1.pop(k, None)
         for k in list(self.T2.keys()):
             if k not in cache_keys:
                 self.T2.pop(k, None)
 
     def _prune_ghosts(self):
         cap = self.capacity or 1
-        # Bound each ghost list individually to capacity.
-        while len(self.B1) > cap:
-            self.B1.popitem(last=False)
-        while len(self.B2) > cap:
-            self.B2.popitem(last=False)
+        # Remove any ghosts that became resident
+        for k in list(self.B1.keys()):
+            if k in self.T1 or k in self.T2:
+                self.B1.pop(k, None)
+        for k in list(self.B2.keys()):
+            if k in self.T1 or k in self.T2:
+                self.B2.pop(k, None)
+        # Periodically trim ghost history from the larger list and bound total
+        self._ghost_ops = getattr(self, "_ghost_ops", 0) + 1
+        if self._ghost_ops % max(1, cap) == 0:
+            if len(self.B1) >= len(self.B2):
+                n = max(1, len(self.B1) // 10)
+                for _ in range(n):
+                    if not self.B1:
+                        break
+                    self.B1.popitem(last=False)
+            else:
+                n = max(1, len(self.B2) // 10)
+                for _ in range(n):
+                    if not self.B2:
+                        break
+                    self.B2.popitem(last=False)
+        while (len(self.B1) + len(self.B2)) > cap:
+            if len(self.B1) > int(self.p):
+                self.B1.popitem(last=False)
+            else:
+                self.B2.popitem(last=False)
 
     def _touch_T1(self, key: str):
-        # Place/move key to MRU end of T1
         self.T1[key] = None
         self.T1.move_to_end(key)
 
     def _touch_T2(self, key: str):
-        # Place/move key to MRU end of T2
         self.T2[key] = None
         self.T2.move_to_end(key)
 
     def _move_T1_to_B1(self, key: str):
         self.T1.pop(key, None)
         self.B1[key] = None
         self.B1.move_to_end(key)
 
     def _move_T2_to_B2(self, key: str):
         self.T2.pop(key, None)
         self.B2[key] = None
         self.B2.move_to_end(key)
 
+    def _sample_lru_min_freq(self, od: OrderedDict) -> str:
+        # Sample k keys from LRU side and return the one with minimal estimated frequency.
+        if not od:
+            return None
+        k = min(self._sample_k, len(od))
+        it = iter(od.keys())  # OrderedDict iterates from LRU to MRU
+        min_key = None
+        min_freq = None
+        for _ in range(k):
+            try:
+                key = next(it)
+            except StopIteration:
+                break
+            f = self.sketch.estimate(key)
+            if min_freq is None or f < min_freq:
+                min_freq = f
+                min_key = key
+        return min_key if min_key is not None else next(iter(od))
+
     # ---------- public hooks called by the cache framework ----------
 
     def choose_victim(self, cache_snapshot, new_obj) -> str:
         """
-        ARC victim selection:
-        Prefer evicting from T1 (recency) when T1 is large (>p) or incoming key
-        has history in B2 and T1 == p; else from T2 (frequency).
+        Hybrid ARC + TinyLFU victim selection:
+        - Warmup: early on, prefer evicting from T1 to preserve frequency.
+        - ARC rule: if |T1| > p or (new in B2 and |T1|==p), evict from T1.
+        - Otherwise: compare TinyLFU(new) vs TinyLFU(candidate_T2); if new is colder, evict from T1.
+        - Always pick the sampled LRU-min-freq key within the chosen segment.
         """
         self._ensure_capacity(cache_snapshot.capacity)
         self._prune_stale_residents(cache_snapshot)
 
         in_B2 = (new_obj.key in self.B2)
 
-        # Decide which resident list to evict from
-        choose_T1 = len(self.T1) > 0 and (len(self.T1) > self.p or (in_B2 and len(self.T1) == self.p))
-
-        victim_key = None
-        if choose_T1 and len(self.T1) > 0:
-            victim_key = next(iter(self.T1))
+        cand_T1 = self._sample_lru_min_freq(self.T1) if self.T1 else None
+        cand_T2 = self._sample_lru_min_freq(self.T2) if self.T2 else None
+
+        # Warmup bias: protect emerging hot set
+        if cache_snapshot.access_count < (2 * self.capacity):
+            if cand_T1 is not None:
+                self._last_evicted_from = 'T1'
+                return cand_T1
+            if cand_T2 is not None:
+                self._last_evicted_from = 'T2'
+                return cand_T2
+
+        # Scan-mode bias: during cooldown prefer evicting from T1
+        now = cache_snapshot.access_count
+        if now < getattr(self, "_scan_mode_until", 0) and cand_T1 is not None:
             self._last_evicted_from = 'T1'
-        elif len(self.T2) > 0:
-            victim_key = next(iter(self.T2))
+            return cand_T1
+
+        # ARC rule first
+        if cand_T1 is not None and (len(self.T1) > self.p or (in_B2 and len(self.T1) == int(self.p))):
+            self._last_evicted_from = 'T1'
+            return cand_T1
+
+        # If only one segment has candidates
+        if cand_T1 is None and cand_T2 is not None:
             self._last_evicted_from = 'T2'
-        elif len(self.T1) > 0:
-            victim_key = next(iter(self.T1))
+            return cand_T2
+        if cand_T2 is None and cand_T1 is not None:
             self._last_evicted_from = 'T1'
-        else:
-            # Fallback: unknown ordering, pick an arbitrary key from current cache.
-            # This also handles situations where our state is cold but cache is warm.
-            victim_key = next(iter(cache_snapshot.cache))
+            return cand_T1
+
+        # Both candidates exist: TinyLFU competitive decision
+        if cand_T1 is not None and cand_T2 is not None:
+            f_new = self.sketch.estimate(new_obj.key)
+            f_t2 = self.sketch.estimate(cand_T2)
+            if f_new <= f_t2:
+                self._last_evicted_from = 'T1'
+                return cand_T1
+            else:
+                self._last_evicted_from = 'T2'
+                return cand_T2
+
+        # Fallbacks
+        if len(self.T2) > 0:
+            self._last_evicted_from = 'T2'
+            return self._sample_lru_min_freq(self.T2)
+        if len(self.T1) > 0:
             self._last_evicted_from = 'T1'
-
-        return victim_key
+            return self._sample_lru_min_freq(self.T1)
+
+        # Final resort: pick any key from the actual cache
+        self._last_evicted_from = 'T1'
+        return next(iter(cache_snapshot.cache))
 
     def on_hit(self, cache_snapshot, obj):
-        """Hit handling: promote to T2 if in T1; reorder within T2 if already there."""
+        """
+        Hit handling:
+        - Increment TinyLFU.
+        - If in T1: promote to T2 if frequency estimate >= 2 (first re-reference), unless in scan-mode where threshold is 3.
+        - If in T2: refresh in T2.
+        - If not tracked but cache hit: treat as frequent and place in T2.
+        Also remove any ghost entries for this resident key to keep ghost history clean.
+        """
         self._ensure_capacity(cache_snapshot.capacity)
-
         key = obj.key
-        # Keep state robust to any desyncs:
+        self.sketch.increment(key, 1)
+
+        # Any hit breaks a pure-miss streaming streak
+        self._miss_streak = 0
+
+        # Remove stale ghost duplicates for resident key
+        self.B1.pop(key, None)
+        self.B2.pop(key, None)
+
+        # Promotion threshold: stricter during scan-mode cooldown
+        now = cache_snapshot.access_count
+        in_scan = now < getattr(self, "_scan_mode_until", 0)
+        promote_threshold = 3 if in_scan else 2
+
         if key in self.T1:
-            # Second touch: promote to T2
-            self.T1.pop(key, None)
-            self._touch_T2(key)
+            if self.sketch.estimate(key) >= promote_threshold:
+                self.T1.pop(key, None)
+                self._touch_T2(key)
+            else:
+                self._touch_T1(key)
         elif key in self.T2:
-            # Renew recency in T2
             self._touch_T2(key)
         else:
-            # If our metadata missed this key, place it as frequent (it was a hit).
+            # Metadata desync: cache had it; assume it's frequent
             self._touch_T2(key)
 
+        self._prune_ghosts()
+
     def on_insert(self, cache_snapshot, obj):
         """
         Insert handling (called on miss after space made, if needed):
-        - If key in B1: increase p (bias to recency) and insert into T2.
-        - If key in B2: decrease p (bias to frequency) and insert into T2.
-        - Else: insert into T1.
+        - If key in B1: increase p (favor recency) with damped adaptation and insert into T2.
+        - If key in B2: decrease p (favor frequency) with damped adaptation and insert into T2.
+        - Else: W-TinyLFU style competitive admission:
+            Compare f(new) to sampled T2 victim and sampled T1 LRU. Admit to T2 only if f(new) is
+            strictly greater than both, otherwise admit to T1.
+        - During scan-mode cooldown, avoid direct T2 admissions for non-ghosts to prevent pollution.
+        - Track miss streak to detect scans and temporarily bias eviction to T1.
         """
         self._ensure_capacity(cache_snapshot.capacity)
         key = obj.key
+        now = cache_snapshot.access_count
+        # Count misses as well
+        self.sketch.increment(key, 1)
+
+        # Update miss streak and detect scans (streaming)
+        self._miss_streak = getattr(self, "_miss_streak", 0) + 1
+        if self._miss_streak > (2 * self.capacity) and len(self.T2) < max(1, self.capacity // 10) and len(self.B1) >= len(self.B2):
+            # Streaming pattern: strongly prefer recency for a short cooldown window
+            self.p = max(0.0, self.p * 0.5)
+            self._scan_mode_until = now + self.capacity
+
+        alpha = 0.25  # dampening factor for p updates
 
         if key in self.B1:
             # Increase p toward recency
-            delta = max(1, len(self.B2) // max(1, len(self.B1)))
-            self.p = min(self.capacity, self.p + delta)
+            delta = float(max(1, len(self.B2) // max(1, len(self.B1))))
+            self.p = min(self.capacity, max(0.0, self.p + alpha * delta))
             self.B1.pop(key, None)
             self._touch_T2(key)
         elif key in self.B2:
             # Decrease p toward frequency
-            delta = max(1, len(self.B1) // max(1, len(self.B2)))
-            self.p = max(0, self.p - delta)
+            delta = float(max(1, len(self.B1) // max(1, len(self.B2))))
+            self.p = max(0.0, min(self.capacity, self.p - alpha * delta))
             self.B2.pop(key, None)
             self._touch_T2(key)
         else:
-            # New key with no ghost history: start in T1
-            self._touch_T1(key)
-
-        # Ensure ghosts are bounded
+            # Scan mode: avoid admitting directly into T2
+            in_scan = now < getattr(self, "_scan_mode_until", 0)
+            if in_scan:
+                self._touch_T1(key)
+            else:
+                f_new = self.sketch.estimate(key)
+                # Sample T2 and T1 LRU-side representatives
+                k2 = self._sample_lru_min_freq(self.T2) if self.T2 else None
+                f_k2 = self.sketch.estimate(k2) if k2 is not None else -1  # -1 so any seen item can beat empty T2
+                k1 = self._sample_lru_min_freq(self.T1) if self.T1 else None
+                f_k1 = self.sketch.estimate(k1) if k1 is not None else -1
+                # Admit to T2 only if clearly hotter than both samples
+                if f_new > max(f_k2, f_k1):
+                    self._touch_T2(key)
+                else:
+                    self._touch_T1(key)
+
+        # Ensure ghosts are bounded and maintained
         self._prune_ghosts()
 
     def on_evict(self, cache_snapshot, obj, evicted_obj):
         """
         Eviction handling: move the evicted resident to its corresponding ghost list.
         Maintain bounded ghost metadata and adapt later on insert.
         """
         self._ensure_capacity(cache_snapshot.capacity)
         evk = evicted_obj.key
 
         if evk in self.T1:
             self._move_T1_to_B1(evk)
         elif evk in self.T2:
             self._move_T2_to_B2(evk)
         else:
             # Fall back to our last decision if state was pruned.
             if self._last_evicted_from == 'T1':
                 self.B1[evk] = None
                 self.B1.move_to_end(evk)
             else:
                 self.B2[evk] = None
                 self.B2.move_to_end(evk)
 
         self._prune_ghosts()
 
 
 # Single policy instance reused across calls
 _policy = _ArcPolicy()
 
 
 def evict(cache_snapshot, obj):
     """
     Choose eviction victim key for the incoming obj.
     """
     return _policy.choose_victim(cache_snapshot, obj)
 
 
 def update_after_hit(cache_snapshot, obj):
     """
     Update policy state after a cache hit on obj.
     """
     _policy.on_hit(cache_snapshot, obj)
 
 
 def update_after_insert(cache_snapshot, obj):
     """
     Update policy state after a new obj is inserted into the cache.
     """
     _policy.on_insert(cache_snapshot, obj)
 
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     """
     Update policy state after evicting evicted_obj to make room for obj.
     """
     _policy.on_evict(cache_snapshot, obj, evicted_obj)
 
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate