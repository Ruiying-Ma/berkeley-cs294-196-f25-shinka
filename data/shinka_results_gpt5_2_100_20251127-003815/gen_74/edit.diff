--- a/original.py
+++ b/original.py
@@ -1,446 +1,522 @@
 # EVOLVE-BLOCK-START
-"""Adaptive cache eviction using ARC augmented with TinyLFU competitive admission.
+"""Window-TinyLFU with Segmented LRU and EMA-based scan/phase guard.
 
 Public API:
 - evict(cache_snapshot, obj) -> key
 - update_after_hit(cache_snapshot, obj)
 - update_after_insert(cache_snapshot, obj)
 - update_after_evict(cache_snapshot, obj, evicted_obj)
 """
 
-from collections import OrderedDict
+from collections import OrderedDict, deque
 
 
 class _CmSketch:
     """
     Compact Count-Min Sketch with conservative aging.
     - d hash functions, width w = 2^p (masking for speed).
     - Halves counters periodically to forget stale history.
     """
     __slots__ = ("d", "w", "tables", "mask", "ops", "age_period", "seeds")
 
     def __init__(self, width_power=12, d=3):
         self.d = int(max(1, d))
         w = 1 << int(max(8, width_power))  # at least 256
         self.w = w
         self.mask = w - 1
         self.tables = [[0] * w for _ in range(self.d)]
         self.ops = 0
         self.age_period = max(1024, w)
         self.seeds = (0x9e3779b1, 0x85ebca77, 0xc2b2ae3d, 0x27d4eb2f)
 
     def _hash(self, key_hash: int, i: int) -> int:
         h = key_hash ^ self.seeds[i % len(self.seeds)]
         h ^= (h >> 33) & 0xFFFFFFFFFFFFFFFF
         h *= 0xff51afd7ed558ccd
         h &= 0xFFFFFFFFFFFFFFFF
         h ^= (h >> 33)
         h *= 0xc4ceb9fe1a85ec53
         h &= 0xFFFFFFFFFFFFFFFF
         h ^= (h >> 33)
         return h & self.mask
 
     def _maybe_age(self):
         self.ops += 1
         if self.ops % self.age_period == 0:
             for t in self.tables:
                 for i in range(self.w):
                     t[i] >>= 1
 
     def increment(self, key: str, amount: int = 1):
         h = hash(key)
         for i in range(self.d):
             idx = self._hash(h, i)
             v = self.tables[i][idx] + amount
             if v > 255:
                 v = 255
             self.tables[i][idx] = v
         self._maybe_age()
 
     def estimate(self, key: str) -> int:
         h = hash(key)
         est = 1 << 30
         for i in range(self.d):
             idx = self._hash(h, i)
             v = self.tables[i][idx]
             if v < est:
                 est = v
+        if est == (1 << 30):
+            return 0
         return est
 
 
-class _ArcPolicy:
-    """
-    ARC-like policy enhanced with TinyLFU:
-    - T1: recency list (seen once, resident)
-    - T2: frequency list (seen >=2 or hot-admitted, resident)
-    - B1: ghost of keys evicted from T1
-    - B2: ghost of keys evicted from T2
-    - p: target size for T1 (adaptive; 0..capacity)
-    - sketch: decayed frequency estimator guiding admission and eviction tie-breaking
+class _WTinyLFUPolicy:
+    """
+    Window-TinyLFU with Segmented LRU (W, M1, M2) and EMA-driven tuning.
+
+    Segments:
+      - W: recency window (new admissions and recency-protected)
+      - M1: probationary main (items promoted from W or bypass admission)
+      - M2: protected main (frequent items)
+
+    Victim selection:
+      - If W exceeds its target, evict from W using TinyLFU-aware tail sampling.
+      - Else sample the tails of M1 and M2, choose colder by (TinyLFU asc, age desc),
+        with a +1 TinyLFU bias on M2 to make it harder to evict.
+
+    Admission and promotions:
+      - Miss insert: default to W; bypass to M1 only if new beats the colder of M1/M2.
+      - Hit promotions: W -> M1 (guarded during scan), M1 -> M2, refresh in resident segment.
+
+    Scan/phase guard:
+      - EMA of miss rate (alpha ~ 0.05). When high for a window, enter cooldown:
+        increases window share, raises promotion/admission thresholds, and prefers evicting from W.
     """
 
     __slots__ = (
-        "T1", "T2", "B1", "B2",
-        "p", "capacity", "_last_evicted_from",
-        "sketch", "_sample_k", "_miss_streak", "_scan_mode_until", "_ghost_ops"
+        "W", "M1", "M2",
+        "capacity", "win_frac", "prot_frac",
+        "sketch",
+        "sample_k", "last_touch",
+        "ema_miss", "ema_alpha", "cooldown_until",
+        "tune_period", "last_tune_at",
+        "w_hits", "m1_hits", "m2_hits",
+        "recent_out_ring", "recent_out_set",
     )
 
     def __init__(self):
-        self.T1 = OrderedDict()
-        self.T2 = OrderedDict()
-        self.B1 = OrderedDict()
-        self.B2 = OrderedDict()
-        self.p = 0.0
+        self.W = OrderedDict()
+        self.M1 = OrderedDict()
+        self.M2 = OrderedDict()
         self.capacity = None
-        self._last_evicted_from = None  # 'T1' or 'T2'
+        # Default targets: 20% window; main protected 80% of main space
+        self.win_frac = 0.20
+        self.prot_frac = 0.80
         self.sketch = _CmSketch(width_power=12, d=3)
-        self._sample_k = 6
-        self._miss_streak = 0
-        self._scan_mode_until = 0
-        self._ghost_ops = 0
-
-    # ---------- internal helpers ----------
+        self.sample_k = 6
+        self.last_touch = {}
+        # EMA miss tracking
+        self.ema_miss = 0.0
+        self.ema_alpha = 0.05
+        self.cooldown_until = 0
+        # Periodic tuning
+        self.tune_period = 1024
+        self.last_tune_at = 0
+        # Segment hit counts
+        self.w_hits = 0
+        self.m1_hits = 0
+        self.m2_hits = 0
+        # Recent-membership ring buffer
+        self.recent_out_ring = deque(maxlen=1024)
+        self.recent_out_set = set()
+
+    # ---------- Helpers and maintenance ----------
 
     def _ensure_capacity(self, cap: int):
-        # On first call or capacity change, reset safely.
+        if cap is None:
+            return
+        cap = max(int(cap), 1)
         if self.capacity is None:
-            self.capacity = max(int(cap), 1)
-            # sample size relative to capacity
-            self._sample_k = max(4, min(12, (self.capacity // 8) or 4))
-            # make sketch aging responsive to capacity
-            try:
-                self.sketch.age_period = max(512, min(16384, self.capacity * 8))
-            except Exception:
-                pass
+            self.capacity = cap
+            self.sample_k = max(4, min(12, (cap // 8) or 4))
+            self.sketch.age_period = max(512, min(16384, cap * 8))
+            self.tune_period = max(512, min(8192, cap * 4))
+            self.recent_out_ring = deque(maxlen=max(64, min(4096, cap)))
             return
         if self.capacity != cap:
-            # Reset state to avoid inconsistencies if framework changes capacity.
-            self.T1.clear(); self.T2.clear(); self.B1.clear(); self.B2.clear()
-            self.p = 0.0
-            self.capacity = max(int(cap), 1)
-            self._sample_k = max(4, min(12, (self.capacity // 8) or 4))
-            try:
-                self.sketch.age_period = max(512, min(16384, self.capacity * 8))
-            except Exception:
-                pass
+            # Reinitialize conservatively on capacity change
+            self.capacity = cap
+            self.W.clear(); self.M1.clear(); self.M2.clear()
+            self.sample_k = max(4, min(12, (cap // 8) or 4))
+            self.sketch.age_period = max(512, min(16384, cap * 8))
+            self.tune_period = max(512, min(8192, cap * 4))
+            self.recent_out_ring = deque(maxlen=max(64, min(4096, cap)))
+            self.recent_out_set.clear()
+            self.last_touch.clear()
+            self.ema_miss = 0.0
+            self.cooldown_until = 0
+            self.w_hits = self.m1_hits = self.m2_hits = 0
 
     def _prune_stale_residents(self, cache_snapshot):
-        # Drop keys that our policy still tracks but the cache no longer has.
         cache_keys = cache_snapshot.cache.keys()
-        for k in list(self.T1.keys()):
-            if k not in cache_keys:
-                self.T1.pop(k, None)
-        for k in list(self.T2.keys()):
-            if k not in cache_keys:
-                self.T2.pop(k, None)
-
-    def _prune_ghosts(self):
-        cap = self.capacity or 1
-        # Remove any ghosts that became resident
-        for k in list(self.B1.keys()):
-            if k in self.T1 or k in self.T2:
-                self.B1.pop(k, None)
-        for k in list(self.B2.keys()):
-            if k in self.T1 or k in self.T2:
-                self.B2.pop(k, None)
-        # Periodically trim ghost history from the larger list and bound total
-        self._ghost_ops = getattr(self, "_ghost_ops", 0) + 1
-        if self._ghost_ops % max(1, cap) == 0:
-            if len(self.B1) >= len(self.B2):
-                n = max(1, len(self.B1) // 10)
-                for _ in range(n):
-                    if not self.B1:
-                        break
-                    self.B1.popitem(last=False)
-            else:
-                n = max(1, len(self.B2) // 10)
-                for _ in range(n):
-                    if not self.B2:
-                        break
-                    self.B2.popitem(last=False)
-        while (len(self.B1) + len(self.B2)) > cap:
-            if len(self.B1) > int(self.p):
-                self.B1.popitem(last=False)
-            else:
-                self.B2.popitem(last=False)
-
-    def _touch_T1(self, key: str):
-        self.T1[key] = None
-        self.T1.move_to_end(key)
-
-    def _touch_T2(self, key: str):
-        self.T2[key] = None
-        self.T2.move_to_end(key)
-
-    def _move_T1_to_B1(self, key: str):
-        self.T1.pop(key, None)
-        self.B1[key] = None
-        self.B1.move_to_end(key)
-
-    def _move_T2_to_B2(self, key: str):
-        self.T2.pop(key, None)
-        self.B2[key] = None
-        self.B2.move_to_end(key)
-
-    def _sample_lru_min_freq(self, od: OrderedDict) -> str:
-        # Sample k keys from LRU side and return the one with minimal estimated frequency.
+        for od in (self.W, self.M1, self.M2):
+            for k in list(od.keys()):
+                if k not in cache_keys:
+                    od.pop(k, None)
+        # Prune last_touch for memory control
+        if len(self.last_touch) > 4 * max(1, self.capacity):
+            for k in list(self.last_touch.keys()):
+                if (k not in self.W) and (k not in self.M1) and (k not in self.M2):
+                    self.last_touch.pop(k, None)
+
+    def _now(self, cache_snapshot):
+        return int(getattr(cache_snapshot, "access_count", 0))
+
+    def _update_last_touch(self, key: str, now: int):
+        self.last_touch[key] = now
+
+    def _age(self, key: str, now: int) -> int:
+        lt = self.last_touch.get(key, now)
+        d = now - lt
+        if d < 0:
+            d = 0
+        if d > 65535:
+            d = 65535
+        return d
+
+    def _tail_sample_min(self, od: OrderedDict, now: int, k: int) -> str | None:
         if not od:
             return None
-        k = min(self._sample_k, len(od))
+        k = min(k, len(od))
         it = iter(od.keys())  # OrderedDict iterates from LRU to MRU
         min_key = None
-        min_freq = None
+        min_tuple = None
         for _ in range(k):
             try:
                 key = next(it)
             except StopIteration:
                 break
             f = self.sketch.estimate(key)
-            if min_freq is None or f < min_freq:
-                min_freq = f
+            age = self._age(key, now)
+            # coldness tuple: (freq asc, age desc) => (f, -age)
+            tup = (f, -age)
+            if (min_tuple is None) or (tup < min_tuple):
+                min_tuple = tup
                 min_key = key
         return min_key if min_key is not None else next(iter(od))
 
-    # ---------- public hooks called by the cache framework ----------
+    def _score_coldness(self, key: str, now: int, m2_bias: int = 0):
+        f = self.sketch.estimate(key)
+        age = self._age(key, now)
+        return (f + m2_bias, -age)
+
+    def _record_access_ema(self, miss: bool):
+        x = 1.0 if miss else 0.0
+        self.ema_miss = (1.0 - self.ema_alpha) * self.ema_miss + self.ema_alpha * x
+
+    def _tune(self, cache_snapshot):
+        now = self._now(cache_snapshot)
+        if now - self.last_tune_at < self.tune_period:
+            return
+        self.last_tune_at = now
+
+        # Scan detection and cooldown extension
+        if self.ema_miss > 0.80:
+            self.cooldown_until = max(self.cooldown_until, now + max(1, self.capacity // 2))
+
+        # Protected main sizing adjustments (prot_frac in [0.60, 0.90])
+        total_hits = self.w_hits + self.m1_hits + self.m2_hits
+        m2_share = (self.m2_hits / total_hits) if total_hits > 0 else 0.0
+        if m2_share > 0.70:
+            self.prot_frac = min(0.90, self.prot_frac + 0.05)
+        elif self.m1_hits > self.m2_hits * 1.2:
+            self.prot_frac = max(0.60, self.prot_frac - 0.05)
+
+        # Window sizing reacts to cooldown and churn
+        if now < self.cooldown_until:
+            self.win_frac = min(0.40, self.win_frac + 0.05)
+        else:
+            # settle toward 0.20 if stable
+            if m2_share > 0.70 and self.ema_miss < 0.30:
+                self.win_frac = max(0.15, self.win_frac - 0.02)
+            else:
+                self.win_frac = min(0.30, max(0.18, self.win_frac))
+
+        # TinyLFU aging period capacity/phase-aware
+        # Push toward shorter aging when EMA is high (more churn)
+        target_age = int(max(4 * self.capacity, min(16 * self.capacity,
+                          (12 * self.capacity) if m2_share > 0.70 else (6 * self.capacity if self.ema_miss > 0.70 else 8 * self.capacity))))
+        self.sketch.age_period = max(512, min(16384, target_age))
+
+        # Tail sampling size adaptive
+        if self.ema_miss > 0.70 or self.w_hits > self.m2_hits:
+            self.sample_k = 4
+        elif m2_share > 0.70:
+            self.sample_k = 12
+        else:
+            self.sample_k = 8
+
+        # Reset hit counters
+        self.w_hits = self.m1_hits = self.m2_hits = 0
+
+    def _targets(self):
+        cap = self.capacity or 1
+        w_target = max(1, int(self.win_frac * cap))
+        main = max(0, cap - w_target)
+        m2_target = int(self.prot_frac * main)
+        if m2_target > main:
+            m2_target = main
+        return w_target, main, m2_target
+
+    def _in_any(self, key: str) -> bool:
+        return key in self.W or key in self.M1 or key in self.M2
+
+    def _touch(self, od: OrderedDict, key: str):
+        od[key] = None
+        od.move_to_end(key)
+
+    def _promote_to_M1(self, key: str):
+        if key in self.W:
+            self.W.pop(key, None)
+        elif key in self.M2:
+            self.M2.pop(key, None)
+        self._touch(self.M1, key)
+
+    def _promote_to_M2(self, key: str):
+        if key in self.W:
+            self.W.pop(key, None)
+        if key in self.M1:
+            self.M1.pop(key, None)
+        self._touch(self.M2, key)
+
+    def _place_in_W(self, key: str):
+        # Insert or move to MRU in window
+        if key in self.M1:
+            self.M1.pop(key, None)
+        if key in self.M2:
+            self.M2.pop(key, None)
+        self._touch(self.W, key)
+
+    # ---------- Victim selection ----------
 
     def choose_victim(self, cache_snapshot, new_obj) -> str:
-        """
-        Hybrid ARC + TinyLFU victim selection:
-        - Warmup: early on, prefer evicting from T1 to preserve frequency.
-        - ARC rule: if |T1| > p or (new in B2 and |T1|==p), evict from T1.
-        - Otherwise: compare TinyLFU(new) vs TinyLFU(candidate_T2); if new is colder, evict from T1.
-        - Always pick the sampled LRU-min-freq key within the chosen segment.
-        """
         self._ensure_capacity(cache_snapshot.capacity)
         self._prune_stale_residents(cache_snapshot)
-
-        in_B2 = (new_obj.key in self.B2)
-
-        cand_T1 = self._sample_lru_min_freq(self.T1) if self.T1 else None
-        cand_T2 = self._sample_lru_min_freq(self.T2) if self.T2 else None
-
-        # Warmup bias: protect emerging hot set
-        if cache_snapshot.access_count < (2 * self.capacity):
-            if cand_T1 is not None:
-                self._last_evicted_from = 'T1'
-                return cand_T1
-            if cand_T2 is not None:
-                self._last_evicted_from = 'T2'
-                return cand_T2
-
-        # Scan-mode bias: during cooldown prefer evicting from T1
-        now = cache_snapshot.access_count
-        if now < getattr(self, "_scan_mode_until", 0) and cand_T1 is not None:
-            self._last_evicted_from = 'T1'
-            return cand_T1
-
-        # ARC rule first
-        if cand_T1 is not None and (len(self.T1) > self.p or (in_B2 and len(self.T1) == int(self.p))):
-            self._last_evicted_from = 'T1'
-            return cand_T1
-
-        # If only one segment has candidates
-        if cand_T1 is None and cand_T2 is not None:
-            self._last_evicted_from = 'T2'
-            return cand_T2
-        if cand_T2 is None and cand_T1 is not None:
-            self._last_evicted_from = 'T1'
-            return cand_T1
-
-        # Both candidates exist: TinyLFU competitive decision
-        if cand_T1 is not None and cand_T2 is not None:
-            f_new = self.sketch.estimate(new_obj.key)
-            f_t2 = self.sketch.estimate(cand_T2)
-            if f_new <= f_t2:
-                self._last_evicted_from = 'T1'
-                return cand_T1
+        now = self._now(cache_snapshot)
+        self._tune(cache_snapshot)
+
+        # Prefer evicting from the window if it exceeds target
+        w_target, _, _ = self._targets()
+        if len(self.W) > w_target:
+            # Randomized tail sampling by taking first k keys from LRU side
+            victim = self._tail_sample_min(self.W, now, max(4, min(12, 4 * (self.sample_k // 4))))
+            if victim is not None:
+                return victim
+
+        # Otherwise, choose colder between M1 and M2 using lexicographic coldness with +1 bias for M2
+        cand_m1 = self._tail_sample_min(self.M1, now, self.sample_k) if self.M1 else None
+        cand_m2 = self._tail_sample_min(self.M2, now, self.sample_k) if self.M2 else None
+
+        if cand_m1 is None and cand_m2 is None:
+            # Fall back to window if available
+            if len(self.W) > 0:
+                victim = self._tail_sample_min(self.W, now, self.sample_k)
+                if victim is not None:
+                    return victim
+            # Final resort: pick any key from cache snapshot
+            return next(iter(cache_snapshot.cache))
+
+        if cand_m1 is None:
+            return cand_m2
+        if cand_m2 is None:
+            return cand_m1
+
+        s1 = self._score_coldness(cand_m1, now, m2_bias=0)
+        s2 = self._score_coldness(cand_m2, now, m2_bias=1)  # +1 bias to protect M2
+        return cand_m1 if s1 <= s2 else cand_m2
+
+    # ---------- Hooks called by framework ----------
+
+    def on_hit(self, cache_snapshot, obj):
+        self._ensure_capacity(cache_snapshot.capacity)
+        now = self._now(cache_snapshot)
+        key = obj.key
+
+        # Update TinyLFU and EMA (hit => miss=0)
+        self.sketch.increment(key, 1)
+        self._record_access_ema(miss=False)
+
+        # Maintain resident metadata
+        self._prune_stale_residents(cache_snapshot)
+        self._update_last_touch(key, now)
+
+        in_cooldown = now < self.cooldown_until
+
+        if key in self.W:
+            # Guard promotions during cooldown to avoid promoting scans
+            self.w_hits += 1
+            if in_cooldown:
+                # Require some frequency and competitiveness to promote
+                f = self.sketch.estimate(key)
+                cand_m1 = self._tail_sample_min(self.M1, now, max(1, self.sample_k // 2))
+                thr = self.sketch.estimate(cand_m1) if cand_m1 else 1
+                if f >= max(2, thr + 1):
+                    self._promote_to_M1(key)
+                else:
+                    self._touch(self.W, key)
             else:
-                self._last_evicted_from = 'T2'
-                return cand_T2
-
-        # Fallbacks
-        if len(self.T2) > 0:
-            self._last_evicted_from = 'T2'
-            return self._sample_lru_min_freq(self.T2)
-        if len(self.T1) > 0:
-            self._last_evicted_from = 'T1'
-            return self._sample_lru_min_freq(self.T1)
-
-        # Final resort: pick any key from the actual cache
-        self._last_evicted_from = 'T1'
-        return next(iter(cache_snapshot.cache))
-
-    def on_hit(self, cache_snapshot, obj):
-        """
-        Hit handling:
-        - Increment TinyLFU.
-        - If in T1: promote to T2 if frequency estimate >= 2 (first re-reference), unless in scan-mode where threshold is 3.
-        - If in T2: refresh in T2.
-        - If not tracked but cache hit: treat as frequent and place in T2.
-        Also remove any ghost entries for this resident key to keep ghost history clean.
-        """
+                # Normal path: W -> M1 on hit
+                self._promote_to_M1(key)
+        elif key in self.M1:
+            self.m1_hits += 1
+            # Promote to M2 on hit in probationary
+            self._promote_to_M2(key)
+        elif key in self.M2:
+            self.m2_hits += 1
+            self._touch(self.M2, key)
+        else:
+            # Desync: cache has it, we don't. Treat as hot and place into M2.
+            self._promote_to_M2(key)
+
+        self._tune(cache_snapshot)
+
+    def on_insert(self, cache_snapshot, obj):
         self._ensure_capacity(cache_snapshot.capacity)
+        now = self._now(cache_snapshot)
         key = obj.key
+
+        # Miss => update TinyLFU and EMA
         self.sketch.increment(key, 1)
-
-        # Any hit breaks a pure-miss streaming streak
-        self._miss_streak = 0
-
-        # Remove stale ghost duplicates for resident key
-        self.B1.pop(key, None)
-        self.B2.pop(key, None)
-
-        # Promotion threshold: stricter during scan-mode cooldown
-        now = cache_snapshot.access_count
-        in_scan = now < getattr(self, "_scan_mode_until", 0)
-        promote_threshold = 3 if in_scan else 2
-
-        if key in self.T1:
-            if self.sketch.estimate(key) >= promote_threshold:
-                self.T1.pop(key, None)
-                self._touch_T2(key)
+        self._record_access_ema(miss=True)
+
+        # Maintain resident metadata
+        self._prune_stale_residents(cache_snapshot)
+        self._update_last_touch(key, now)
+
+        f_new = self.sketch.estimate(key)
+        in_cooldown = now < self.cooldown_until
+
+        # Recent ring buffer membership gives a small boost
+        recent_boost = 1 if key in self.recent_out_set else 0
+        f_new_eff = f_new + recent_boost
+
+        # Early-bypass rule: bypass W to M1 only if new beats the colder of M1/M2
+        cand_m1 = self._tail_sample_min(self.M1, now, self.sample_k) if self.M1 else None
+        cand_m2 = self._tail_sample_min(self.M2, now, self.sample_k) if self.M2 else None
+        cold_ref = None
+        if cand_m1 and cand_m2:
+            s1 = self._score_coldness(cand_m1, now, m2_bias=0)
+            s2 = self._score_coldness(cand_m2, now, m2_bias=1)
+            cold_ref = cand_m1 if s1 <= s2 else cand_m2
+        else:
+            cold_ref = cand_m1 or cand_m2
+
+        thr_est = self.sketch.estimate(cold_ref) if cold_ref else 0
+
+        if in_cooldown:
+            # During cooldown, avoid bypass and fill window
+            self._place_in_W(key)
+        else:
+            # Allow bypass if clearly hotter than both segments' colder candidate
+            if f_new_eff >= thr_est + 1:
+                self._promote_to_M1(key)
             else:
-                self._touch_T1(key)
-        elif key in self.T2:
-            self._touch_T2(key)
-        else:
-            # Metadata desync: cache had it; assume it's frequent
-            self._touch_T2(key)
-
-        self._prune_ghosts()
-
-    def on_insert(self, cache_snapshot, obj):
-        """
-        Insert handling (called on miss after space made, if needed):
-        - If key in B1: increase p (favor recency) with damped adaptation and insert into T2.
-        - If key in B2: decrease p (favor frequency) with damped adaptation and insert into T2.
-        - Else: W-TinyLFU style competitive admission:
-            Compare f(new) to sampled T2 victim and sampled T1 LRU. Admit to T2 only if f(new) is
-            strictly greater than both, otherwise admit to T1.
-        - During scan-mode cooldown, avoid direct T2 admissions for non-ghosts to prevent pollution.
-        - Track miss streak to detect scans and temporarily bias eviction to T1.
-        """
+                self._place_in_W(key)
+
+        # Soft rebalance: if M2 grows too large, demote its LRU to M1
+        w_target, main_target, m2_target = self._targets()
+        if len(self.M2) > m2_target and len(self.M2) > 0:
+            # Demote a cold M2 tail sample to M1 to respect protected size
+            demote_key = self._tail_sample_min(self.M2, now, max(1, self.sample_k // 2))
+            if demote_key is not None and demote_key in self.M2:
+                self.M2.pop(demote_key, None)
+                self._touch(self.M1, demote_key)
+
+        # If window is starved vs target and we have room in W, consider moving a cold M1 tail into W
+        # (rare; metadata-only rebalancing to keep W active under churn)
+        if len(self.W) < w_target and len(self.M1) > 0:
+            move_key = self._tail_sample_min(self.M1, now, max(1, self.sample_k // 2))
+            if move_key is not None and move_key in self.M1:
+                self.M1.pop(move_key, None)
+                self._touch(self.W, move_key)
+
+        self._tune(cache_snapshot)
+
+    def on_evict(self, cache_snapshot, obj, evicted_obj):
         self._ensure_capacity(cache_snapshot.capacity)
-        key = obj.key
-        now = cache_snapshot.access_count
-        # Count misses as well
-        self.sketch.increment(key, 1)
-
-        # Update miss streak and detect scans (streaming)
-        self._miss_streak = getattr(self, "_miss_streak", 0) + 1
-        if self._miss_streak > (2 * self.capacity) and len(self.T2) < max(1, self.capacity // 10) and len(self.B1) >= len(self.B2):
-            # Streaming pattern: strongly prefer recency for a short cooldown window
-            self.p = max(0.0, self.p * 0.5)
-            self._scan_mode_until = now + self.capacity
-
-        alpha = 0.25  # dampening factor for p updates
-
-        if key in self.B1:
-            # Increase p toward recency
-            delta = float(max(1, len(self.B2) // max(1, len(self.B1))))
-            self.p = min(self.capacity, max(0.0, self.p + alpha * delta))
-            self.B1.pop(key, None)
-            self._touch_T2(key)
-        elif key in self.B2:
-            # Decrease p toward frequency
-            delta = float(max(1, len(self.B1) // max(1, len(self.B2))))
-            self.p = max(0.0, min(self.capacity, self.p - alpha * delta))
-            self.B2.pop(key, None)
-            self._touch_T2(key)
-        else:
-            # Scan mode: avoid admitting directly into T2
-            in_scan = now < getattr(self, "_scan_mode_until", 0)
-            if in_scan:
-                self._touch_T1(key)
-            else:
-                f_new = self.sketch.estimate(key)
-                # Sample T2 and T1 LRU-side representatives
-                k2 = self._sample_lru_min_freq(self.T2) if self.T2 else None
-                f_k2 = self.sketch.estimate(k2) if k2 is not None else -1  # -1 so any seen item can beat empty T2
-                k1 = self._sample_lru_min_freq(self.T1) if self.T1 else None
-                f_k1 = self.sketch.estimate(k1) if k1 is not None else -1
-                # Admit to T2 only if clearly hotter than both samples
-                if f_new > max(f_k2, f_k1):
-                    self._touch_T2(key)
-                else:
-                    self._touch_T1(key)
-
-        # Ensure ghosts are bounded and maintained
-        self._prune_ghosts()
-
-    def on_evict(self, cache_snapshot, obj, evicted_obj):
-        """
-        Eviction handling: move the evicted resident to its corresponding ghost list.
-        Maintain bounded ghost metadata and adapt later on insert.
-        """
-        self._ensure_capacity(cache_snapshot.capacity)
+        self._prune_stale_residents(cache_snapshot)
         evk = evicted_obj.key
-
-        if evk in self.T1:
-            self._move_T1_to_B1(evk)
-        elif evk in self.T2:
-            self._move_T2_to_B2(evk)
-        else:
-            # Fall back to our last decision if state was pruned.
-            if self._last_evicted_from == 'T1':
-                self.B1[evk] = None
-                self.B1.move_to_end(evk)
-            else:
-                self.B2[evk] = None
-                self.B2.move_to_end(evk)
-
-        self._prune_ghosts()
+        # Remove from any resident segment
+        if evk in self.W:
+            self.W.pop(evk, None)
+        elif evk in self.M1:
+            self.M1.pop(evk, None)
+        elif evk in self.M2:
+            self.M2.pop(evk, None)
+        # Record in recent-membership ring buffer
+        if evk in self.recent_out_set:
+            # refresh recency in ring by re-adding
+            try:
+                self.recent_out_ring.remove(evk)
+            except ValueError:
+                pass
+        self.recent_out_ring.append(evk)
+        self.recent_out_set.add(evk)
+        # Enforce ring capacity
+        while len(self.recent_out_ring) > self.recent_out_ring.maxlen:
+            old = self.recent_out_ring.popleft()
+            self.recent_out_set.discard(old)
+        # No further action; admission/promotion handled on insert/hit.
 
 
 # Single policy instance reused across calls
-_policy = _ArcPolicy()
+_policy = _WTinyLFUPolicy()
 
 
 def evict(cache_snapshot, obj):
     """
     Choose eviction victim key for the incoming obj.
     """
     return _policy.choose_victim(cache_snapshot, obj)
 
 
 def update_after_hit(cache_snapshot, obj):
     """
     Update policy state after a cache hit on obj.
     """
     _policy.on_hit(cache_snapshot, obj)
 
 
 def update_after_insert(cache_snapshot, obj):
     """
     Update policy state after a new obj is inserted into the cache.
     """
     _policy.on_insert(cache_snapshot, obj)
 
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     """
     Update policy state after evicting evicted_obj to make room for obj.
     """
     _policy.on_evict(cache_snapshot, obj, evicted_obj)
-
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate