<NAME>
arc_tinylfu_sampling_eviction
</NAME>
<DESCRIPTION>
I introduce a frequency-aware enhancement to the existing ARC-like policy by integrating a lightweight TinyLFU-inspired frequency estimator with periodic decay and competitive tail sampling during eviction. The key idea is to choose the coldest victim by combining long-term frequency (with exponential aging) and recency, while still respecting ARCâ€™s adaptive bias between recency and frequency (via T1/T2 and parameter p).

Concretely:
- Maintain a small frequency map (freq) with periodic halving (ageing) and a per-key last access timestamp. These are updated on every hit and insert.
- During eviction, sample a few LRU-tail candidates from both T1 and T2 and pick the colder one by a lexicographic coldness score: first by low frequency, then by older recency. Add a small bias protecting T2 to preserve frequently used items.
- Keep the rest of ARC mechanisms (ghost lists B1/B2 and adaptive p) unchanged for robust scan resistance and phase adaptation.

This hybrid ARC + TinyLFU sampling improves victim selection, reduces pollution by low-frequency items, and protects frequently reused entries, lowering the cache miss rate across diverse workloads.
</DESCRIPTION>
<DIFF>
<<<<<<< SEARCH
"""Adaptive cache eviction using an ARC-like policy with ghost history.

Public API:
- evict(cache_snapshot, obj) -> key
- update_after_hit(cache_snapshot, obj)
- update_after_insert(cache_snapshot, obj)
- update_after_evict(cache_snapshot, obj, evicted_obj)
"""

from collections import OrderedDict

class _ArcPolicy:
    """
    ARC-like policy:
    - T1: recency list (seen once, resident)
    - T2: frequency list (seen >=2, resident)
    - B1: ghost of keys evicted from T1
    - B2: ghost of keys evicted from T2
    - p: target size for T1 (adaptive; 0..capacity)
    """

    __slots__ = (
        "T1", "T2", "B1", "B2",
        "p", "capacity", "_last_evicted_from"
    )

    def __init__(self):
        self.T1 = OrderedDict()
        self.T2 = OrderedDict()
        self.B1 = OrderedDict()
        self.B2 = OrderedDict()
        self.p = 0
        self.capacity = None
        self._last_evicted_from = None  # 'T1' or 'T2'

    # ---------- internal helpers ----------

    def _ensure_capacity(self, cap: int):
        # On first call or capacity change, reset safely.
        if self.capacity is None:
            self.capacity = max(int(cap), 1)
            return
        if self.capacity != cap:
            # Reset state to avoid inconsistencies if framework changes capacity.
            self.T1.clear(); self.T2.clear(); self.B1.clear(); self.B2.clear()
            self.p = 0
            self.capacity = max(int(cap), 1)

    def _is_in_cache(self, key: str, cache_snapshot) -> bool:
        return key in cache_snapshot.cache

    def _prune_stale_residents(self, cache_snapshot):
        # Drop keys that our policy still tracks but the cache no longer has.
        cache_keys = cache_snapshot.cache.keys()
        for k in list(self.T1.keys()):
            if k not in cache_keys:
                self.T1.pop(k, None)
        for k in list(self.T2.keys()):
            if k not in cache_keys:
                self.T2.pop(k, None)

    def _prune_ghosts(self):
        cap = self.capacity or 1
        # Bound each ghost list individually to capacity.
        while len(self.B1) > cap:
            self.B1.popitem(last=False)
        while len(self.B2) > cap:
            self.B2.popitem(last=False)

    def _touch_T1(self, key: str):
        # Place/move key to MRU end of T1
        self.T1[key] = None
        self.T1.move_to_end(key)

    def _touch_T2(self, key: str):
        # Place/move key to MRU end of T2
        self.T2[key] = None
        self.T2.move_to_end(key)

    def _move_T1_to_B1(self, key: str):
        self.T1.pop(key, None)
        self.B1[key] = None
        self.B1.move_to_end(key)

    def _move_T2_to_B2(self, key: str):
        self.T2.pop(key, None)
        self.B2[key] = None
        self.B2.move_to_end(key)

    # ---------- public hooks called by the cache framework ----------

    def choose_victim(self, cache_snapshot, new_obj) -> str:
        """
        ARC victim selection:
        Prefer evicting from T1 (recency) when T1 is large (>p) or incoming key
        has history in B2 and T1 == p; else from T2 (frequency).
        """
        self._ensure_capacity(cache_snapshot.capacity)
        self._prune_stale_residents(cache_snapshot)

        in_B2 = (new_obj.key in self.B2)

        # Decide which resident list to evict from
        choose_T1 = len(self.T1) > 0 and (len(self.T1) > self.p or (in_B2 and len(self.T1) == self.p))

        victim_key = None
        if choose_T1 and len(self.T1) > 0:
            victim_key = next(iter(self.T1))
            self._last_evicted_from = 'T1'
        elif len(self.T2) > 0:
            victim_key = next(iter(self.T2))
            self._last_evicted_from = 'T2'
        elif len(self.T1) > 0:
            victim_key = next(iter(self.T1))
            self._last_evicted_from = 'T1'
        else:
            # Fallback: unknown ordering, pick an arbitrary key from current cache.
            # This also handles situations where our state is cold but cache is warm.
            victim_key = next(iter(cache_snapshot.cache))
            self._last_evicted_from = 'T1'

        return victim_key

    def on_hit(self, cache_snapshot, obj):
        """Hit handling: promote to T2 if in T1; reorder within T2 if already there."""
        self._ensure_capacity(cache_snapshot.capacity)

        key = obj.key
        # Keep state robust to any desyncs:
        if key in self.T1:
            # Second touch: promote to T2
            self.T1.pop(key, None)
            self._touch_T2(key)
        elif key in self.T2:
            # Renew recency in T2
            self._touch_T2(key)
        else:
            # If our metadata missed this key, place it as frequent (it was a hit).
            self._touch_T2(key)

    def on_insert(self, cache_snapshot, obj):
        """
        Insert handling (called on miss after space made, if needed):
        - If key in B1: increase p (bias to recency) and insert into T2.
        - If key in B2: decrease p (bias to frequency) and insert into T2.
        - Else: insert into T1.
        """
        self._ensure_capacity(cache_snapshot.capacity)
        key = obj.key

        if key in self.B1:
            # Increase p toward recency
            delta = max(1, len(self.B2) // max(1, len(self.B1)))
            self.p = min(self.capacity, self.p + delta)
            self.B1.pop(key, None)
            self._touch_T2(key)
        elif key in self.B2:
            # Decrease p toward frequency
            delta = max(1, len(self.B1) // max(1, len(self.B2)))
            self.p = max(0, self.p - delta)
            self.B2.pop(key, None)
            self._touch_T2(key)
        else:
            # New key with no ghost history: start in T1
            self._touch_T1(key)

        # Ensure ghosts are bounded
        self._prune_ghosts()

    def on_evict(self, cache_snapshot, obj, evicted_obj):
        """
        Eviction handling: move the evicted resident to its corresponding ghost list.
        Maintain bounded ghost metadata and adapt later on insert.
        """
        self._ensure_capacity(cache_snapshot.capacity)
        evk = evicted_obj.key

        if evk in self.T1:
            self._move_T1_to_B1(evk)
        elif evk in self.T2:
            self._move_T2_to_B2(evk)
        else:
            # Fall back to our last decision if state was pruned.
            if self._last_evicted_from == 'T1':
                self.B1[evk] = None
                self.B1.move_to_end(evk)
            else:
                self.B2[evk] = None
                self.B2.move_to_end(evk)

        self._prune_ghosts()


# Single policy instance reused across calls
_policy = _ArcPolicy()


def evict(cache_snapshot, obj):
    """
    Choose eviction victim key for the incoming obj.
    """
    return _policy.choose_victim(cache_snapshot, obj)


def update_after_hit(cache_snapshot, obj):
    """
    Update policy state after a cache hit on obj.
    """
    _policy.on_hit(cache_snapshot, obj)


def update_after_insert(cache_snapshot, obj):
    """
    Update policy state after a new obj is inserted into the cache.
    """
    _policy.on_insert(cache_snapshot, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Update policy state after evicting evicted_obj to make room for obj.
    """
    _policy.on_evict(cache_snapshot, obj, evicted_obj)
=======
"""Adaptive cache eviction using an ARC-like policy with ghost history.

Public API:
- evict(cache_snapshot, obj) -> key
- update_after_hit(cache_snapshot, obj)
- update_after_insert(cache_snapshot, obj)
- update_after_evict(cache_snapshot, obj, evicted_obj)
"""

from collections import OrderedDict
from itertools import islice

class _ArcPolicy:
    """
    ARC-like policy augmented with TinyLFU-style frequency estimation and
    competitive tail sampling for victim selection.

    Lists:
    - T1: recency list (seen once, resident)
    - T2: frequency list (seen >=2, resident)
    - B1: ghost of keys evicted from T1
    - B2: ghost of keys evicted from T2

    Other state:
    - p: target size for T1 (adaptive; 0..capacity)
    - freq: approximate frequency counter with periodic decay
    - last_access_time: per-key last access timestamp (for recency tie-break)
    """

    __slots__ = (
        "T1", "T2", "B1", "B2",
        "p", "capacity", "_last_evicted_from",
        # TinyLFU-like components
        "freq", "last_access_time", "last_decay_tick", "decay_period",
        "k_sample"
    )

    def __init__(self):
        self.T1 = OrderedDict()
        self.T2 = OrderedDict()
        self.B1 = OrderedDict()
        self.B2 = OrderedDict()
        self.p = 0
        self.capacity = None
        self._last_evicted_from = None  # 'T1' or 'T2'

        # Frequency estimator with decay
        self.freq = {}
        self.last_access_time = {}
        self.last_decay_tick = 0
        self.decay_period = 256  # will be adjusted after first capacity seen
        self.k_sample = 4

    # ---------- internal helpers ----------

    def _ensure_capacity(self, cap: int):
        # On first call or capacity change, reset safely.
        if self.capacity is None:
            self.capacity = max(int(cap), 1)
            # Tune decay and sampling based on capacity (counts, not bytes here)
            self.decay_period = max(128, int(self.capacity) * 4)
            self.k_sample = max(2, min(12, self.capacity // 16 or 2))
            return
        if self.capacity != cap:
            # Reset state to avoid inconsistencies if framework changes capacity.
            self.T1.clear(); self.T2.clear(); self.B1.clear(); self.B2.clear()
            self.p = 0
            self.capacity = max(int(cap), 1)
            # Re-tune adaptive parameters
            self.decay_period = max(128, int(self.capacity) * 4)
            self.k_sample = max(2, min(12, self.capacity // 16 or 2))
            # Keep freq across capacity changes but decay it to soften carryover
            self._global_decay()

    def _maybe_decay(self, cache_snapshot):
        now = getattr(cache_snapshot, "access_count", 0) or 0
        if self.last_decay_tick == 0:
            self.last_decay_tick = now
            return
        if now - self.last_decay_tick >= self.decay_period:
            self._global_decay()
            self.last_decay_tick = now

    def _global_decay(self):
        # Halve all frequencies; drop zeros to bound memory.
        if not self.freq:
            return
        for k in list(self.freq.keys()):
            v = self.freq[k] >> 1
            if v:
                self.freq[k] = v
            else:
                self.freq.pop(k, None)

    def _prune_stale_residents(self, cache_snapshot):
        # Drop keys that our policy still tracks but the cache no longer has.
        cache_keys = cache_snapshot.cache.keys()
        for k in list(self.T1.keys()):
            if k not in cache_keys:
                self.T1.pop(k, None)
        for k in list(self.T2.keys()):
            if k not in cache_keys:
                self.T2.pop(k, None)

    def _prune_ghosts(self):
        cap = self.capacity or 1
        # Bound each ghost list individually to capacity.
        while len(self.B1) > cap:
            self.B1.popitem(last=False)
        while len(self.B2) > cap:
            self.B2.popitem(last=False)

    def _touch_T1(self, key: str):
        # Place/move key to MRU end of T1
        self.T1[key] = None
        self.T1.move_to_end(key)

    def _touch_T2(self, key: str):
        # Place/move key to MRU end of T2
        self.T2[key] = None
        self.T2.move_to_end(key)

    def _move_T1_to_B1(self, key: str):
        self.T1.pop(key, None)
        self.B1[key] = None
        self.B1.move_to_end(key)

    def _move_T2_to_B2(self, key: str):
        self.T2.pop(key, None)
        self.B2[key] = None
        self.B2.move_to_end(key)

    def _bump_freq(self, key: str):
        self.freq[key] = self.freq.get(key, 0) + 1
        # Bound frequency map size to avoid unbounded growth.
        # Keep a multiple of capacity (frequency info helps admission/eviction across phases).
        max_keys = max(1024, int((self.capacity or 1) * 16))
        if len(self.freq) > max_keys:
            # Drop the coldest quarter by rough thresholding (simple and cheap).
            # Compute median-ish threshold by sampling few keys.
            sample = list(islice(self.freq.items(), min(64, len(self.freq))))
            if sample:
                # pick 25th percentile approx
                vals = sorted(v for _, v in sample)
                thr = vals[max(0, len(vals)//4 - 1)]
                for k in list(self.freq.keys()):
                    if self.freq.get(k, 0) <= thr:
                        self.freq.pop(k, None)
                        if len(self.freq) <= max_keys:
                            break

    def _record_access_time(self, key: str, cache_snapshot):
        self.last_access_time[key] = getattr(cache_snapshot, "access_count", 0) or 0

    def _iter_head(self, od: OrderedDict, k: int):
        return list(islice(od.keys(), 0, k))

    def _coldness(self, key: str, now: int, bias_freq: int = 0):
        # Lower is colder (i.e., better eviction victim)
        f = self.freq.get(key, 0) + bias_freq
        age = now - self.last_access_time.get(key, 0)
        # Lexicographic: prefer lower frequency; then older (larger age)
        # We invert age to negative so that larger age becomes smaller tuple component.
        return (f, -age)

    # ---------- public hooks called by the cache framework ----------

    def choose_victim(self, cache_snapshot, new_obj) -> str:
        """
        ARC + frequency-aware victim selection:
        - Sample from the LRU tails of T1 and T2.
        - Use TinyLFU-estimated frequency (with decay) and recency to pick the colder candidate.
        - Protect T2 by adding a +1 bias to frequency when comparing.
        """
        self._ensure_capacity(cache_snapshot.capacity)
        self._prune_stale_residents(cache_snapshot)
        self._maybe_decay(cache_snapshot)

        now = getattr(cache_snapshot, "access_count", 0) or 0

        in_B2 = (new_obj.key in self.B2)
        choose_T1_arc = len(self.T1) > 0 and (len(self.T1) > self.p or (in_B2 and len(self.T1) == self.p))

        k = self.k_sample
        t1_candidates = self._iter_head(self.T1, min(k, len(self.T1)))
        t2_candidates = self._iter_head(self.T2, min(k, len(self.T2)))

        best_key = None
        best_score = None

        # Evaluate T1 candidates (no bias)
        for k1 in t1_candidates:
            score = self._coldness(k1, now, bias_freq=0)
            if best_score is None or score < best_score:
                best_score = score
                best_key = (k1, 'T1')

        # Evaluate T2 candidates with protective bias (+1 frequency)
        for k2 in t2_candidates:
            score = self._coldness(k2, now, bias_freq=1)
            if best_score is None or score < best_score:
                best_score = score
                best_key = (k2, 'T2')

        victim_key = None
        if best_key is not None:
            victim_key, origin = best_key
            self._last_evicted_from = origin
        else:
            # Fallback to ARC baseline decision if sampling had no candidates
            if choose_T1_arc and len(self.T1) > 0:
                victim_key = next(iter(self.T1))
                self._last_evicted_from = 'T1'
            elif len(self.T2) > 0:
                victim_key = next(iter(self.T2))
                self._last_evicted_from = 'T2'
            elif len(self.T1) > 0:
                victim_key = next(iter(self.T1))
                self._last_evicted_from = 'T1'
            else:
                # Unknown ordering, pick an arbitrary key from current cache.
                victim_key = next(iter(cache_snapshot.cache))
                self._last_evicted_from = 'T1'

        return victim_key

    def on_hit(self, cache_snapshot, obj):
        """Hit handling: promote to T2 if in T1; reorder within T2 if already there."""
        self._ensure_capacity(cache_snapshot.capacity)
        self._maybe_decay(cache_snapshot)

        key = obj.key
        # Update frequency and recency
        self._bump_freq(key)
        self._record_access_time(key, cache_snapshot)

        # Keep state robust to any desyncs:
        if key in self.T1:
            # Second touch: promote to T2
            self.T1.pop(key, None)
            self._touch_T2(key)
        elif key in self.T2:
            # Renew recency in T2
            self._touch_T2(key)
        else:
            # If our metadata missed this key, place it as frequent (it was a hit).
            self._touch_T2(key)

    def on_insert(self, cache_snapshot, obj):
        """
        Insert handling (called on miss after space made, if needed):
        - If key in B1: increase p (bias to recency) and insert into T2.
        - If key in B2: decrease p (bias to frequency) and insert into T2.
        - Else: insert into T1.
        Also update frequency counter with decay-aware bump.
        """
        self._ensure_capacity(cache_snapshot.capacity)
        self._maybe_decay(cache_snapshot)
        key = obj.key

        # Account the access for frequency tracking
        self._bump_freq(key)
        self._record_access_time(key, cache_snapshot)

        if key in self.B1:
            # Increase p toward recency
            delta = max(1, len(self.B2) // max(1, len(self.B1)))
            self.p = min(self.capacity, self.p + delta)
            self.B1.pop(key, None)
            self._touch_T2(key)
        elif key in self.B2:
            # Decrease p toward frequency
            delta = max(1, len(self.B1) // max(1, len(self.B2)))
            self.p = max(0, self.p - delta)
            self.B2.pop(key, None)
            self._touch_T2(key)
        else:
            # New key with no ghost history: start in T1
            self._touch_T1(key)

        # Ensure ghosts are bounded
        self._prune_ghosts()

    def on_evict(self, cache_snapshot, obj, evicted_obj):
        """
        Eviction handling: move the evicted resident to its corresponding ghost list.
        Maintain bounded ghost metadata and adapt later on insert.
        """
        self._ensure_capacity(cache_snapshot.capacity)
        self._maybe_decay(cache_snapshot)
        evk = evicted_obj.key

        if evk in self.T1:
            self._move_T1_to_B1(evk)
        elif evk in self.T2:
            self._move_T2_to_B2(evk)
        else:
            # Fall back to our last decision if state was pruned.
            if self._last_evicted_from == 'T1':
                self.B1[evk] = None
                self.B1.move_to_end(evk)
            else:
                self.B2[evk] = None
                self.B2.move_to_end(evk)

        # We can prune recency timestamps for evicted residents (keep freq for LFU history)
        self.last_access_time.pop(evk, None)
        self._prune_ghosts()


# Single policy instance reused across calls
_policy = _ArcPolicy()


def evict(cache_snapshot, obj):
    """
    Choose eviction victim key for the incoming obj.
    """
    return _policy.choose_victim(cache_snapshot, obj)


def update_after_hit(cache_snapshot, obj):
    """
    Update policy state after a cache hit on obj.
    """
    _policy.on_hit(cache_snapshot, obj)


def update_after_insert(cache_snapshot, obj):
    """
    Update policy state after a new obj is inserted into the cache.
    """
    _policy.on_insert(cache_snapshot, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Update policy state after evicting evicted_obj to make room for obj.
    """
    _policy.on_evict(cache_snapshot, obj, evicted_obj)
>>>>>>> REPLACE

</DIFF>