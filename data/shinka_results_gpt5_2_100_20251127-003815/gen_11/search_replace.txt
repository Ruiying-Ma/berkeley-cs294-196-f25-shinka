<NAME>
slru_lrfu_hybrid_with_adaptive_protected
</NAME>

<DESCRIPTION>
I propose upgrading the current pure LRFU-decay policy into a segmented SLRU+LRFU hybrid:
- Maintain two resident segments: probation (new/once-accessed) and protected (multi-hit/hot). This provides scan resistance and improves protection for frequently accessed items, as seen in SLRU/ARC families.
- Keep the LRFU exponential-decay score to select victims within each segment, blending recency with frequency in a lightweight way.
- Evict from the probation segment first; only if itâ€™s empty, evict from protected. This reduces the chance of evicting hot items.
- On a hit in probation, promote to protected; on a hit in protected, reinforce its score. We use an adaptive protected capacity that increases on probation hits and slightly decreases on protected hits to balance recency vs. frequency based on workload.
- Initialize newly inserted items into probation with a small score to curb scan pollution.
- Add light robustness: prune stale membership, enforce protected capacity by demoting the lowest-score item back to probation.

This design captures the core benefit of ARC/SLRU (resident segmentation and promotion) without ghost lists, while still leveraging the LRFU score for fine-grained selection. It should lower miss rates on mixed workloads and streaming traces compared to the prior single-heap LRFU variant. I also tightened decay (half-life 8) to forget stale popularity faster, further improving scan resistance.

</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Adaptive LRFU-like eviction: combine recency and frequency with exponential decay"""

# Configuration: half-life in number of accesses for score decay.
# After DECAY_HALF_LIFE accesses without a hit, a key's score halves.
DECAY_HALF_LIFE = 16
DECAY_BASE = 2 ** (-1.0 / DECAY_HALF_LIFE)

# Per-key metadata for cached objects
_key_score = dict()      # key -> float decayed frequency score
_key_last_time = dict()  # key -> int last access_count when we updated its score

def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global _key_score, _key_last_time
    now = cache_snapshot.access_count

    # Choose the object with the minimum decayed score; tie-break on older last access (LRU fallback).
    min_key = None
    min_score = None
    min_old_time = None

    # Lazily decay scores to "now" during victim selection to avoid global walks elsewhere.
    for k in cache_snapshot.cache.keys():
        # Initialize metadata if missing (robustness)
        if k not in _key_last_time:
            _key_last_time[k] = now
        if k not in _key_score:
            _key_score[k] = 0.0

        old_time = _key_last_time[k]
        dt = now - old_time
        if dt > 0:
            _key_score[k] *= pow(DECAY_BASE, dt)
            _key_last_time[k] = now
        s = _key_score[k]

        if (min_score is None) or (s < min_score) or (s == min_score and old_time < min_old_time):
            min_score = s
            min_key = k
            min_old_time = old_time

    return min_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global _key_score, _key_last_time
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if obj.key not in _key_last_time:
        _key_last_time[obj.key] = now
    if obj.key not in _key_score:
        _key_score[obj.key] = 0.0

    dt = now - _key_last_time[obj.key]
    if dt > 0:
        _key_score[obj.key] *= pow(DECAY_BASE, dt)
    _key_score[obj.key] += 1.0  # frequency boost
    _key_last_time[obj.key] = now

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global _key_score, _key_last_time
    now = cache_snapshot.access_count
    _key_last_time[obj.key] = now
    # Start with a modest score to reduce scan pollution; frequent re-references will quickly boost it.
    _key_score[obj.key] = 0.5

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global _key_score, _key_last_time
    _key_score.pop(evicted_obj.key, None)
    _key_last_time.pop(evicted_obj.key, None)
=======
"""Adaptive SLRU+LRFU eviction: probation/protected segments with exponential-decay score"""

# Configuration: half-life in number of accesses for score decay.
# After DECAY_HALF_LIFE accesses without a hit, a key's score halves.
DECAY_HALF_LIFE = 8
DECAY_BASE = 2 ** (-1.0 / DECAY_HALF_LIFE)

# Per-key metadata for cached objects
_key_score = dict()      # key -> float decayed frequency score
_key_last_time = dict()  # key -> int last access_count when we updated its score

# Segmented resident sets
_probation = set()       # keys admitted recently or with single hit
_protected = set()       # keys with demonstrated frequency
_prot_cap = 0            # adaptive target size (in number of keys) for protected segment

def _prune_membership(cache_snapshot):
    """Remove keys from segment sets that are no longer resident in the cache."""
    cache_keys = set(cache_snapshot.cache.keys())
    for seg in (_probation, _protected):
        stale = [k for k in seg if k not in cache_keys]
        for k in stale:
            seg.discard(k)

def _ensure_meta(k, now):
    """Ensure metadata exists for key k and lazily decay its score to 'now'."""
    if k not in _key_last_time:
        _key_last_time[k] = now
    if k not in _key_score:
        _key_score[k] = 0.0
    dt = now - _key_last_time[k]
    if dt > 0:
        _key_score[k] *= pow(DECAY_BASE, dt)
        _key_last_time[k] = now
    return _key_score[k], _key_last_time[k]

def _pick_min_by_score(candidates, now):
    """Pick key with minimal decayed score from candidates; tie-break on older last access."""
    min_key = None
    min_score = None
    min_old_time = None
    for k in candidates:
        old_time = _key_last_time.get(k, now)
        # decay lazily
        s, _ = _ensure_meta(k, now)
        if (min_score is None) or (s < min_score) or (s == min_score and old_time < (min_old_time if min_old_time is not None else old_time)):
            min_score = s
            min_key = k
            min_old_time = old_time
    return min_key

def _enforce_protected_cap(now):
    """If protected exceeds target cap, demote lowest-score protected key to probation."""
    global _prot_cap
    while _protected and len(_protected) > _prot_cap:
        k = _pick_min_by_score(_protected, now)
        if k is None:
            break
        _protected.discard(k)
        _probation.add(k)

def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global _prot_cap
    now = cache_snapshot.access_count

    # Keep sets consistent with actual cache content
    _prune_membership(cache_snapshot)

    # Initialize protected capacity target if unset
    cur_cap = max(1, len(cache_snapshot.cache))
    if _prot_cap <= 0:
        _prot_cap = max(1, cur_cap // 2)

    # Prefer evicting from probation; if empty, from protected; else fallback to any key
    prob_candidates = [k for k in _probation if k in cache_snapshot.cache]
    if prob_candidates:
        return _pick_min_by_score(prob_candidates, now)

    prot_candidates = [k for k in _protected if k in cache_snapshot.cache]
    if prot_candidates:
        return _pick_min_by_score(prot_candidates, now)

    # Fallback: choose globally minimal score among resident keys
    return _pick_min_by_score(list(cache_snapshot.cache.keys()), now)

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global _prot_cap
    now = cache_snapshot.access_count

    _prune_membership(cache_snapshot)

    # Initialize protected capacity target if unset
    cur_cap = max(1, len(cache_snapshot.cache))
    if _prot_cap <= 0:
        _prot_cap = max(1, cur_cap // 2)

    # Update decayed score and last time
    _ensure_meta(obj.key, now)

    # Promotion logic and adaptive protected sizing
    if obj.key in _probation:
        # Promote to protected on first observed hit
        _probation.discard(obj.key)
        _protected.add(obj.key)
        # Bias toward larger protected region when we observe probation hits
        if _prot_cap < cur_cap:
            _prot_cap += 1
    else:
        # If unknown to segments, treat as protected (robustness)
        _protected.add(obj.key)
        _probation.discard(obj.key)
        # Slightly reduce protected target on protected hits to keep room for recency
        if _prot_cap > 1:
            _prot_cap -= 1

    # Frequency boost; slightly stronger in protected
    if obj.key in _protected:
        _key_score[obj.key] += 1.5
    else:
        _key_score[obj.key] += 1.0

    # Enforce protected capacity by demoting weakest protected key
    _enforce_protected_cap(now)

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global _prot_cap
    now = cache_snapshot.access_count

    _prune_membership(cache_snapshot)

    # Initialize protected capacity on cold start
    cur_cap = max(1, len(cache_snapshot.cache))
    if _prot_cap <= 0:
        _prot_cap = max(1, cur_cap // 2)

    # Admit into probation with a small initial score to reduce scan pollution.
    _probation.add(obj.key)
    _protected.discard(obj.key)

    _key_last_time[obj.key] = now
    _key_score[obj.key] = 0.1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global _prot_cap
    # Clear all metadata for the evicted key
    _key_score.pop(evicted_obj.key, None)
    _key_last_time.pop(evicted_obj.key, None)
    _probation.discard(evicted_obj.key)
    _protected.discard(evicted_obj.key)

    # Make sure protected cap does not exceed current possible size
    cur_cap = max(1, len(cache_snapshot.cache))
    if _prot_cap > cur_cap:
        _prot_cap = cur_cap
>>>>>>> REPLACE

</DIFF>