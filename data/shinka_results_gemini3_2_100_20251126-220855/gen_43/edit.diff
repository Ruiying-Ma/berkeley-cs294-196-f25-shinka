--- a/original.py
+++ b/original.py
@@ -1,185 +1,199 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm combining ARC adaptive sizing with S3 probabilistic rescue"""
+"""Cache eviction algorithm: S3-FIFO with LIFO-Small, Ghost-Freq, and Aging"""
 from collections import OrderedDict
 import random
 
 # Global State
-m_t1 = OrderedDict() # T1: Recent cache entries (Probation)
-m_t2 = OrderedDict() # T2: Frequent cache entries (Protected)
-m_b1 = OrderedDict() # B1: Ghost entries evicted from T1
-m_b2 = OrderedDict() # B2: Ghost entries evicted from T2
-m_p = 0              # Target size for T1
-m_last_access_count = -1
+s3_small = OrderedDict() # Small/Probationary items (LIFO Eviction)
+s3_main = OrderedDict()  # Main/Protected items (LRU Eviction)
+s3_ghost = OrderedDict() # Ghost registry: key -> freq at eviction
+s3_freq = {}             # Frequency map: key -> count
+s3_config = {}           # Configuration parameters
+ops_count = 0
 
-def _check_reset(cache_snapshot):
-    """Resets global state if a new trace is detected based on access count rollback."""
-    global m_t1, m_t2, m_b1, m_b2, m_p, m_last_access_count
-    if cache_snapshot.access_count < m_last_access_count:
-        m_t1.clear()
-        m_t2.clear()
-        m_b1.clear()
-        m_b2.clear()
-        m_p = 0
-    m_last_access_count = cache_snapshot.access_count
+def _reset_state(cache_snapshot):
+    """Resets global state if a new trace is detected."""
+    global s3_small, s3_main, s3_ghost, s3_freq, s3_config, ops_count
+    # Detect reset via access count rollback
+    if cache_snapshot.access_count < ops_count:
+        s3_small.clear()
+        s3_main.clear()
+        s3_ghost.clear()
+        s3_freq.clear()
+        s3_config.clear()
+    ops_count = cache_snapshot.access_count
+    
+    # Initialize config once per trace
+    if not s3_config:
+        cap = cache_snapshot.capacity
+        s3_config['small_ratio'] = 0.1
+        # Jitter: Allow target to fluctuate significantly to enable rotation/adaptation
+        # Use 15% range to force frequent shifts between Main and Small eviction
+        s3_config['jitter_range'] = max(1, int(cap * 0.15)) 
+        # Aging interval: Decay frequencies every 'capacity' operations
+        s3_config['age_interval'] = cap
+        s3_config['next_age'] = cap
+
+def _perform_aging():
+    """Decay frequency counters to prioritize recent popularity."""
+    global s3_freq
+    for k in s3_freq:
+        s3_freq[k] >>= 1 # Integer divide by 2
 
 def evict(cache_snapshot, obj):
     '''
-    Hybrid ARC Eviction:
-    - Adaptive 'p' parameter from ARC.
-    - Randomized victim selection from T1 (from Inspiration).
-    - Probabilistic 'Lucky Save' from S3 to prevent thrashing in T1.
+    S3-FIFO Hardened Eviction Policy:
+    - Calculates a jittered target size for Small.
+    - If Small is over target (or Main empty): Evict from Small using LIFO (Tail).
+      - This protects the 'Head' of Small (older items) and effectively handles loops > capacity.
+    - Else: Evict from Main using LRU (Head).
     '''
-    _check_reset(cache_snapshot)
-    global m_p
-
-    key = obj.key
+    _reset_state(cache_snapshot)
+    global ops_count
+    
     capacity = cache_snapshot.capacity
-
-    # 1. Adapt p (target size of T1) based on hits in ghost lists
-    if key in m_b1:
-        delta = 1
-        if len(m_b1) > 0 and len(m_b2) > 0:
-            delta = max(1, len(m_b2) / len(m_b1))
-        m_p = min(capacity, m_p + delta)
-    elif key in m_b2:
-        delta = 1
-        if len(m_b1) > 0 and len(m_b2) > 0:
-            delta = max(1, len(m_b1) / len(m_b2))
-        m_p = max(0, m_p - delta)
-
-    # 2. Select victim source (T1 vs T2)
-    # Apply small jitter to p for boundary fuzzing (inspired by S3)
-    # This prevents the algorithm from getting stuck at a hard threshold
-    p_jitter = random.randint(-1, 1) if capacity > 10 else 0
-    target_t1 = max(0, min(capacity, m_p + p_jitter))
-
-    evict_t1 = False
-    if len(m_t1) > 0:
-        if len(m_t1) > target_t1:
-            evict_t1 = True
-        elif key in m_b2 and len(m_t1) >= int(target_t1):
-            evict_t1 = True
     
-    # If T2 is empty, we must evict from T1
-    if len(m_t2) == 0:
-        evict_t1 = True
-
-    # 3. Select specific victim
-    if evict_t1 and m_t1:
-        # Loop to allow for 'Lucky Save' retries
-        # Try up to 3 times to find a victim
+    # Periodic Frequency Aging
+    if ops_count >= s3_config.get('next_age', float('inf')):
+        _perform_aging()
+        s3_config['next_age'] = ops_count + s3_config['age_interval']
+    
+    # Dynamic Target for Small (Jitter prevents LIFO lock-up)
+    jitter = random.randint(-s3_config['jitter_range'], s3_config['jitter_range'])
+    target_small = max(1, int(capacity * s3_config['small_ratio']) + jitter)
+    
+    # 1. Check Small Queue (LIFO Eviction Strategy)
+    # Evict from Small if it exceeds target OR if Main is empty
+    if len(s3_small) > target_small or len(s3_main) == 0:
+        # LIFO: Pick from Tail (Most Recently Inserted)
+        # This acts as a filter for scans and loops.
+        
+        # Loop for "Lucky Save" (Anti-Thrashing)
         for _ in range(3):
-            # Randomized selection from bottom k (Scan resistance)
-            k = 5
-            candidates = []
-            it = iter(m_t1)
-            try:
-                for _ in range(k):
-                    candidates.append(next(it))
-            except StopIteration:
-                pass
+            if not s3_small: break
+            victim_key, _ = s3_small.popitem(last=True) # Pop Tail
             
-            victim = random.choice(candidates)
-            
-            # Lucky Save (from S3): Small chance to give second chance
-            # Helps retain items during transient scans/loops
-            if random.random() < 0.05: # 5% chance
-                m_t1.move_to_end(victim) # Move to MRU (Rescue)
-                # Victim is now at end, next loop will pick different candidates
-                continue 
+            # Lucky Save (1%): Move to Head (Safe Zone)
+            # Helps retain items that might be useful but got caught in LIFO churn
+            if random.random() < 0.01:
+                s3_small[victim_key] = 1
+                s3_small.move_to_end(victim_key, last=False) # Move to Head
+                continue
             else:
-                return victim
+                # Restore to Tail (at the end) to maintain state consistency
+                # The harness will evict this key, and update_after_evict will handle cleanup.
+                s3_small[victim_key] = 1
+                return victim_key
         
-        # Fallback if we looped out (pick LRU)
-        return next(iter(m_t1))
-    else:
-        # T2 Eviction: LRU
-        # T2 contains high-value items, LRU is usually optimal here
-        return next(iter(m_t2)) if m_t2 else next(iter(m_t1))
+        # Fallback if loop exhausted (return Tail)
+        if s3_small:
+            return next(reversed(s3_small))
+    
+    # 2. Check Main Queue (LRU Eviction Strategy)
+    # LRU is at Head of s3_main
+    if s3_main:
+        return next(iter(s3_main))
+    
+    # Emergency fallback
+    return next(iter(s3_small)) if s3_small else None
 
 def update_after_hit(cache_snapshot, obj):
     '''
-    On Cache Hit:
-    - If in T1, move to T2 (MRU).
-    - If in T2, move to MRU of T2.
+    On Hit:
+    - Main: Move to MRU (True LRU).
+    - Small: Increment Freq. Move to Head (Safety). Promote if Freq >= 2.
     '''
-    _check_reset(cache_snapshot)
+    _reset_state(cache_snapshot)
     key = obj.key
-
-    if key in m_t1:
-        del m_t1[key]
-        m_t2[key] = 1
-        m_t2.move_to_end(key)
-    elif key in m_t2:
-        m_t2.move_to_end(key)
+    
+    if key in s3_main:
+        s3_main.move_to_end(key)
+    elif key in s3_small:
+        f = s3_freq.get(key, 0) + 1
+        s3_freq[key] = f
+        
+        # Move to Head (Safety from LIFO eviction at Tail)
+        s3_small.move_to_end(key, last=False)
+        
+        # Promotion (Freq Gating: Requires 2 hits)
+        if f >= 2:
+            del s3_small[key]
+            s3_main[key] = 1
+            s3_main.move_to_end(key)
     else:
-        # Should be in cache, but if missing from metadata, add to T2 (self-healing)
-        m_t2[key] = 1
-        m_t2.move_to_end(key)
+        # Recovery
+        s3_main[key] = 1
+        s3_main.move_to_end(key)
 
 def update_after_insert(cache_snapshot, obj):
     '''
-    On Cache Insert (Miss):
-    - If previously in ghost B1/B2, promote to T2.
-    - Otherwise, insert into T1.
+    On Insert:
+    - Ghost: Restore Freq. Promote directly if Freq high.
+    - Else: Insert into Small at Tail (LIFO Danger Zone).
     '''
-    _check_reset(cache_snapshot)
+    _reset_state(cache_snapshot)
     key = obj.key
-
-    if key in m_b1:
-        del m_b1[key]
-        m_t2[key] = 1
-        m_t2.move_to_end(key)
-    elif key in m_b2:
-        del m_b2[key]
-        m_t2[key] = 1
-        m_t2.move_to_end(key)
+    
+    # Ghost Restoration
+    if key in s3_ghost:
+        f = s3_ghost[key]
+        del s3_ghost[key]
+        s3_freq[key] = f
+        # Ghost Rescue: if highly frequent, skip probation
+        if f >= 2:
+            s3_main[key] = 1
+            s3_main.move_to_end(key)
+            return
     else:
-        # Totally new item
-        m_t1[key] = 1
-        m_t1.move_to_end(key)
+        s3_freq[key] = 0
+        
+    # Insert into Small at Tail (LIFO position)
+    s3_small[key] = 1
+    s3_small.move_to_end(key) 
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     '''
     On Eviction:
-    - Move evicted key from T1/T2 to B1/B2.
-    - Maintain capacity of ghost lists.
+    - Remove from queues.
+    - Store frequency in Ghost map.
     '''
     key = evicted_obj.key
-    capacity = cache_snapshot.capacity
+    
+    if key in s3_small:
+        del s3_small[key]
+    elif key in s3_main:
+        del s3_main[key]
+        
+    # Store Ghost Freq
+    f = s3_freq.get(key, 0)
+    s3_ghost[key] = f
+    s3_ghost.move_to_end(key) # Mark as recent ghost
+    
+    # Cleanup Freq Map to save memory (we rely on ghost for history)
+    if key in s3_freq:
+        del s3_freq[key]
 
-    if key in m_t1:
-        del m_t1[key]
-        m_b1[key] = 1
-        m_b1.move_to_end(key)
-    elif key in m_t2:
-        del m_t2[key]
-        m_b2[key] = 1
-        m_b2.move_to_end(key)
-
-    # Restrict size of ghost lists to capacity
-    if len(m_b1) > capacity:
-        m_b1.popitem(last=False)
-    if len(m_b2) > capacity:
-        m_b2.popitem(last=False)
+    # Ghost Capacity Management (2x Cache for better history)
+    if len(s3_ghost) > 2 * cache_snapshot.capacity:
+        s3_ghost.popitem(last=False) # Remove oldest ghost
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate