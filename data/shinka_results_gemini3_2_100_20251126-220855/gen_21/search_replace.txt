The following changes are proposed to improve the cache eviction algorithm.

<NAME>
randomized_s3_fifo
</NAME>

<DESCRIPTION>
Implement S3-FIFO with a randomized eviction window.
This addresses the 0.0000 hit rate on Trace 14 (loop > cache size) by introducing probabilistic survival via a randomized selection of the eviction victim from the tail of the queues (Small or Main).
Standard S3-FIFO or LRU fails completely on strict loops larger than cache size due to synchronization. Randomization breaks this synchronization.
The algorithm maintains the robust S3-FIFO structure (Small/Main/Ghost queues) for other traces.
Logic:
- `evict`: Scans a small window (k=5) at the head of the target queue (Small or Main).
- Promotes any item in the window with hits (lazy promotion).
- If no hits in window, picks a random victim from the window.
- `update_after_evict`: Handles removal from internal queues and Ghost management.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
from collections import OrderedDict

# ARC Global State
arc_t1 = OrderedDict() # T1: Recent items
arc_t2 = OrderedDict() # T2: Frequent items
arc_b1 = OrderedDict() # B1: Ghost Recent
arc_b2 = OrderedDict() # B2: Ghost Frequent
arc_p = 0.0            # Adaptation parameter
arc_c = 0              # Cache capacity
m_last_access_count = -1

def _check_reset(cache_snapshot):
    """Resets global state if a new trace is detected."""
    global arc_t1, arc_t2, arc_b1, arc_b2, arc_p, arc_c, m_last_access_count
    if cache_snapshot.access_count < m_last_access_count:
        arc_t1.clear()
        arc_t2.clear()
        arc_b1.clear()
        arc_b2.clear()
        arc_p = 0.0
        arc_c = cache_snapshot.capacity
    m_last_access_count = cache_snapshot.access_count
    if arc_c == 0: arc_c = cache_snapshot.capacity

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Policy.
    Uses T1/T2 lists and B1/B2 ghost lists to adaptively manage cache content.
    '''
    _check_reset(cache_snapshot)
    global arc_p

    key = obj.key
    # Adaptation logic (ARC)
    if key in arc_b1:
        delta = 1
        if len(arc_b1) >= len(arc_b2):
            delta = 1
        else:
            delta = len(arc_b2) / len(arc_b1)
        arc_p = min(float(arc_c), arc_p + delta)
    elif key in arc_b2:
        delta = 1
        if len(arc_b2) >= len(arc_b1):
            delta = 1
        else:
            delta = len(arc_b1) / len(arc_b2)
        arc_p = max(0.0, arc_p - delta)

    # Determine victim based on ARC logic
    replace_t1 = False
    if len(arc_t1) > 0:
        if len(arc_t1) > arc_p:
            replace_t1 = True
        elif (key in arc_b1) and (len(arc_t1) == int(arc_p)):
             replace_t1 = True

    if replace_t1 and arc_t1:
        return next(iter(arc_t1)) # LRU of T1
    else:
        if arc_t2:
            return next(iter(arc_t2)) # LRU of T2
        return next(iter(arc_t1)) # Fallback

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Move to MRU of T2.
    '''
    _check_reset(cache_snapshot)
    key = obj.key
    if key in arc_t1:
        del arc_t1[key]
        arc_t2[key] = 1 # Insert MRU T2
    elif key in arc_t2:
        arc_t2.move_to_end(key) # Update MRU T2

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Move ghosts to T2, or new items to T1.
    '''
    _check_reset(cache_snapshot)
    key = obj.key

    if key in arc_b1:
        del arc_b1[key]
        arc_t2[key] = 1 # From Ghost T1 -> T2
    elif key in arc_b2:
        del arc_b2[key]
        arc_t2[key] = 1 # From Ghost T2 -> T2
    else:
        arc_t1[key] = 1 # New item -> T1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move to Ghost lists and manage Ghost capacity.
    '''
    key = evicted_obj.key

    if key in arc_t1:
        del arc_t1[key]
        arc_b1[key] = 1
    elif key in arc_t2:
        del arc_t2[key]
        arc_b2[key] = 1

    # Manage Ghost Capacity
    # Limit B1 and B2 size to ensure bounded memory usage
    target_ghost = arc_c

    while len(arc_b1) > target_ghost:
        arc_b1.popitem(last=False)
    while len(arc_b2) > target_ghost * 2:
        arc_b2.popitem(last=False)
=======
# EVOLVE-BLOCK-START
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
from collections import OrderedDict
import random

# S3-FIFO with Randomized Eviction Global State
s3_small = OrderedDict()
s3_main = OrderedDict()
s3_ghost = OrderedDict()
s3_freq = {}
m_last_access_count = -1

def _check_reset(cache_snapshot):
    """Resets global state if a new trace is detected."""
    global s3_small, s3_main, s3_ghost, s3_freq, m_last_access_count
    if cache_snapshot.access_count < m_last_access_count:
        s3_small.clear()
        s3_main.clear()
        s3_ghost.clear()
        s3_freq.clear()
    m_last_access_count = cache_snapshot.access_count

def evict(cache_snapshot, obj):
    '''
    S3-FIFO with Randomized Eviction.
    - S (Small) buffers new items. M (Main) holds frequent items.
    - Victim is chosen from a small window at the head of S or M.
    - Randomization helps with synchronized loops.
    '''
    _check_reset(cache_snapshot)
    capacity = cache_snapshot.capacity
    target_s_size = max(1, int(capacity * 0.1))

    # Window size for scan/randomization
    k_window = 5

    while True:
        # Determine which queue to look at
        if len(s3_small) > target_s_size or len(s3_main) == 0:
            queue = s3_small
            is_small = True
        else:
            queue = s3_main
            is_small = False

        if not queue:
            # Should generally not happen if cache is full
            return next(iter(cache_snapshot.cache)) if cache_snapshot.cache else None

        # Get first K candidates
        candidates = []
        iterator = iter(queue)
        for _ in range(k_window):
            try:
                candidates.append(next(iterator))
            except StopIteration:
                break

        # Check for promotions (Lazy Promotion)
        promoted = False

        for key in candidates:
            if s3_freq.get(key, 0) > 0:
                # Promotion: Move to Main (or reinsert in Main)
                if is_small:
                    del s3_small[key]
                    s3_main[key] = 1
                else:
                    del s3_main[key]
                    s3_main[key] = 1

                s3_freq[key] = 0 # Reset frequency
                promoted = True
                break

        if promoted:
            continue

        # No promotions in window -> Pick random victim
        victim_key = random.choice(candidates)
        return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency counter (capped).
    '''
    _check_reset(cache_snapshot)
    key = obj.key
    s3_freq[key] = min(3, s3_freq.get(key, 0) + 1)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - If in Ghost (G), insert to Main (M).
    - Else, insert to Small (S).
    '''
    _check_reset(cache_snapshot)
    key = obj.key

    if key in s3_ghost:
        # Recall from Ghost
        del s3_ghost[key]
        s3_main[key] = 1
        s3_freq[key] = 0
    else:
        # New Insert
        s3_small[key] = 1
        s3_freq[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Remove from queues and manage Ghost.
    '''
    key = evicted_obj.key

    if key in s3_small:
        del s3_small[key]
        # Evicted from S -> Add to G
        s3_ghost[key] = 1
    elif key in s3_main:
        del s3_main[key]
        # Evicted from M -> Discard (Standard S3-FIFO)

    if key in s3_freq:
        del s3_freq[key]

    # Limit Ghost size
    if len(s3_ghost) > cache_snapshot.capacity:
        s3_ghost.popitem(last=False)
>>>>>>> REPLACE

</DIFF>