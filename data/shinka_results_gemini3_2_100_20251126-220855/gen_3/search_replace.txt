<NAME>
implement_arc_algorithm
</NAME>

<DESCRIPTION>
I will replace the current eviction algorithm with an Adaptive Replacement Cache (ARC) implementation.
ARC dynamically balances between Recency (T1) and Frequency (T2) by adjusting a parameter `p` based on "ghost" hits (hits on recently evicted items).
- `evict`: Uses ARC logic to select a victim from T1 or T2 based on `p` and presence of the new object in ghost sets.
- `update_after_hit`: Moves items from T1 to T2 (frequency promotion) or updates position in T2.
- `update_after_insert`: Handles new items (to T1) or ghost hits (promote to T2).
- `update_after_evict`: Moves evicted items to ghost sets B1 or B2 and manages ghost capacity.
This approach captures both recency and frequency properties and scans, generally outperforming static 2Q or LRU.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

m_key_timestamp = dict()
m_protected_keys = set()
m_ghost_timestamp = dict()

def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global m_key_timestamp, m_protected_keys

    # Calculate current size of A1in (items in cache but not protected)
    a1in_keys = [k for k in cache_snapshot.cache if k not in m_protected_keys]
    current_a1in_size = len(a1in_keys)

    # Target size for A1in (Probation)
    target_a1in = cache_snapshot.capacity * 0.25

    victim_key = None
    candidates = []

    # Decide whether to evict from A1in or Am (Protected)
    # If A1in exceeds target, evict from A1in.
    # Also if Am is empty (all items are probationary), must evict from A1in.
    if current_a1in_size > target_a1in or current_a1in_size == len(cache_snapshot.cache):
        candidates = a1in_keys
    else:
        # Otherwise try to evict from Am
        candidates = [k for k in cache_snapshot.cache if k in m_protected_keys]
        # Fallback if candidates is empty (should be covered by condition above)
        if not candidates:
            candidates = a1in_keys

    # LRU eviction within the chosen segment
    victim_key = min(candidates, key=lambda k: m_key_timestamp.get(k, 0))
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp
    # Update access time (LRU)
    # In 2Q, hits in A1in do NOT promote to Am. They just refresh position in A1in.
    if obj.key in m_key_timestamp:
        m_key_timestamp[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_protected_keys, m_ghost_timestamp
    m_key_timestamp[obj.key] = cache_snapshot.access_count

    # Check if object was in ghost registry
    if obj.key in m_ghost_timestamp:
        # Promote to Protected (Am)
        m_protected_keys.add(obj.key)
        del m_ghost_timestamp[obj.key]
    else:
        # Insert into Probation (A1in)
        # Ensure it's not marked as protected (in case of re-use of key after long time)
        if obj.key in m_protected_keys:
            m_protected_keys.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp, m_protected_keys, m_ghost_timestamp
    if evicted_obj.key in m_key_timestamp:
        del m_key_timestamp[evicted_obj.key]

    if evicted_obj.key in m_protected_keys:
        m_protected_keys.remove(evicted_obj.key)
        # Evicted from Am: usually not added to ghost in standard 2Q
    else:
        # Evicted from A1in: add to ghost registry
        m_ghost_timestamp[evicted_obj.key] = cache_snapshot.access_count

        # Manage ghost size (limit to cache capacity)
        if len(m_ghost_timestamp) > cache_snapshot.capacity:
             # Remove oldest in ghost
             oldest_ghost = min(m_ghost_timestamp, key=m_ghost_timestamp.get)
             del m_ghost_timestamp[oldest_ghost]
=======
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

# ARC Global State
m_lru = {}    # key -> access_count
m_t1 = set()  # T1 keys (Recent)
m_t2 = set()  # T2 keys (Frequent)
m_b1 = set()  # B1 keys (Ghost Recent)
m_b2 = set()  # B2 keys (Ghost Frequent)
m_p = 0.0     # Adaptation parameter (Target T1 size)
m_last_access_count = 0

def check_reset(cache_snapshot):
    global m_last_access_count, m_lru, m_t1, m_t2, m_b1, m_b2, m_p
    current_count = cache_snapshot.access_count
    if current_count < m_last_access_count:
        m_lru.clear()
        m_t1.clear()
        m_t2.clear()
        m_b1.clear()
        m_b2.clear()
        m_p = 0.0
    m_last_access_count = current_count

def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    '''
    check_reset(cache_snapshot)
    global m_p, m_t1, m_t2, m_b1, m_b2, m_lru

    # ARC Adaptation: Adjust p based on ghost hits
    if obj.key in m_b1:
        delta = 1.0
        if len(m_b1) < len(m_b2):
            delta = float(len(m_b2)) / len(m_b1)
        m_p = min(float(cache_snapshot.capacity), m_p + delta)
    elif obj.key in m_b2:
        delta = 1.0
        if len(m_b2) < len(m_b1):
            delta = float(len(m_b1)) / len(m_b2)
        m_p = max(0.0, m_p - delta)

    # Filter sets to match actual cache (handle drift/init)
    t1_real = [k for k in m_t1 if k in cache_snapshot.cache]
    t2_real = [k for k in m_t2 if k in cache_snapshot.cache]

    # Fallback
    if not t1_real and not t2_real:
        t1_real = list(cache_snapshot.cache.keys())

    # ARC Replace Logic
    replace_t1 = False
    if len(t1_real) > 0 and len(t1_real) > m_p:
        replace_t1 = True
    elif len(t1_real) > 0 and (obj.key in m_b2) and (len(t1_real) == int(m_p)):
        replace_t1 = True

    victim_key = None
    if (replace_t1 or not t2_real) and t1_real:
        victim_key = min(t1_real, key=lambda k: m_lru.get(k, 0))
    else:
        # Evict from T2
        victim_key = min(t2_real, key=lambda k: m_lru.get(k, 0))

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update metadata after hit. Move to T2.
    '''
    check_reset(cache_snapshot)
    global m_lru, m_t1, m_t2, m_b1, m_b2
    m_lru[obj.key] = cache_snapshot.access_count

    if obj.key in m_t1:
        m_t1.remove(obj.key)
        m_t2.add(obj.key)
    # If already in T2, stays in T2.

def update_after_insert(cache_snapshot, obj):
    '''
    Update metadata after insert. Handle ghost hits.
    '''
    check_reset(cache_snapshot)
    global m_lru, m_t1, m_t2, m_b1, m_b2
    m_lru[obj.key] = cache_snapshot.access_count

    if obj.key in m_b1:
        m_b1.remove(obj.key)
        m_t2.add(obj.key)
    elif obj.key in m_b2:
        m_b2.remove(obj.key)
        m_t2.add(obj.key)
    else:
        # New object -> T1
        m_t1.add(obj.key)
        # Safety: ensure not in T2
        if obj.key in m_t2:
            m_t2.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Update metadata after eviction. Move to ghosts.
    '''
    check_reset(cache_snapshot)
    global m_t1, m_t2, m_b1, m_b2, m_lru

    if evicted_obj.key in m_t1:
        m_t1.remove(evicted_obj.key)
        m_b1.add(evicted_obj.key)
    elif evicted_obj.key in m_t2:
        m_t2.remove(evicted_obj.key)
        m_b2.add(evicted_obj.key)

    # Manage ghost size: limit total ghosts to capacity
    target_ghost = cache_snapshot.capacity
    if len(m_b1) + len(m_b2) > target_ghost:
        # Evict LRU from ghosts (B1 U B2)
        # Optimization: Scan only if needed.
        # Simple implementation: check all ghosts.
        # Since ghosts are not in cache, we use m_lru timestamps.
        # But iterating all ghosts might be O(N). N=capacity. Accepted.
        victim_ghost = None
        min_ts = float('inf')

        # Iterate B1
        for k in m_b1:
            ts = m_lru.get(k, 0)
            if ts < min_ts:
                min_ts = ts
                victim_ghost = k
        # Iterate B2
        for k in m_b2:
            ts = m_lru.get(k, 0)
            if ts < min_ts:
                min_ts = ts
                victim_ghost = k

        if victim_ghost:
            if victim_ghost in m_b1: m_b1.remove(victim_ghost)
            elif victim_ghost in m_b2: m_b2.remove(victim_ghost)
            if victim_ghost in m_lru: del m_lru[victim_ghost]
>>>>>>> REPLACE
</DIFF>