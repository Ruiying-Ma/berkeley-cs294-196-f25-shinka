<NAME>
init_adaptive_globals
</NAME>

<DESCRIPTION>
Update `_reset_state` to initialize `s3_config` with an eviction counter for garbage collection logic, in addition to the existing parameters.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def _reset_state(cache_snapshot):
    global s3_small, s3_main, s3_ghost, s3_freq, s3_config, last_op_count
    if cache_snapshot.access_count < last_op_count:
        s3_small.clear()
        s3_main.clear()
        s3_ghost.clear()
        s3_freq.clear()
        s3_config.clear()
    last_op_count = cache_snapshot.access_count

    if not s3_config:
        cap = cache_snapshot.capacity
        # Small queue size target (10%)
        s3_config['small_target'] = max(1, int(cap * 0.1))
        # Aging interval (once per capacity accesses)
        s3_config['aging_interval'] = cap

def _age_freqs():
=======
def _reset_state(cache_snapshot):
    global s3_small, s3_main, s3_ghost, s3_freq, s3_config, last_op_count
    if cache_snapshot.access_count < last_op_count:
        s3_small.clear()
        s3_main.clear()
        s3_ghost.clear()
        s3_freq.clear()
        s3_config.clear()
    last_op_count = cache_snapshot.access_count

    if not s3_config:
        cap = cache_snapshot.capacity
        # Small queue size target (starts at 10%)
        s3_config['small_target'] = max(1, int(cap * 0.1))
        # Aging interval (once per capacity accesses)
        s3_config['aging_interval'] = cap
        # Eviction counter for garbage collection
        s3_config['evict_count'] = 0

def _age_freqs():
>>>>>>> REPLACE

</DIFF>

<NAME>
evict_adaptive_cleaning
</NAME>

<DESCRIPTION>
Modify `evict` to include two new mechanisms:
1.  **Garbage Collection**: Periodically (10% of evictions from Small), check the head of the Small queue (oldest). If it has 0 frequency (unused since insertion), evict it immediately. This cleans out "cold" data that LIFO eviction would otherwise trap.
2.  **Adaptive Targeting**: Use the dynamic `small_target` from `s3_config`.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    S3-LIFO-LRU Eviction:
    - Ages frequencies periodically.
    - Favors evicting from Small (LIFO) to protect Main and filter scan/loops.
    - Promotes from Small to Main if freq > 1.
    - Fallback to Main (LRU).
    '''
    _reset_state(cache_snapshot)

    # Aging Logic
    if cache_snapshot.access_count % s3_config['aging_interval'] == 0:
        _age_freqs()

    target_small = s3_config['small_target']

    # 1. Try evicting from Small if it's over budget or if Main is empty
    # We use a loop to handle promotions
    while len(s3_small) > target_small or (not s3_main and s3_small):
        # LIFO Eviction: Inspect tail (newest)
        victim_key, _ = s3_small.popitem(last=True)

        # Check Promotion (freq > 1)
        if s3_freq.get(victim_key, 0) > 1:
            # Promote to Main (MRU)
            s3_main[victim_key] = 1
            s3_main.move_to_end(victim_key)
            # Remove from freq map (optional, or keep for history)
            # We keep it in s3_freq for continuity
        else:
            # Victim found. Put back for update_after_evict (or just return)
            # We must return a key that is present in the cache.
            # We popped it, so we must push it back to be consistent with external view
            s3_small[victim_key] = 1
            # move_to_end(last=True) is implied by assignment for new key
            return victim_key

    # 2. If Small is safe, evict from Main (LRU)
    if s3_main:
        return next(iter(s3_main))

    # Fallback (should be covered)
    if s3_small:
        return next(iter(s3_small))
    return None
=======
def evict(cache_snapshot, obj):
    '''
    Adaptive S3-LIFO-LRU Eviction:
    - Ages frequencies periodically.
    - Adaptive Small target size.
    - Small Queue: Mostly LIFO to filter scans, with periodic FIFO to clean garbage.
    - Main Queue: LRU.
    '''
    _reset_state(cache_snapshot)

    # Aging Logic
    if cache_snapshot.access_count % s3_config['aging_interval'] == 0:
        _age_freqs()

    target_small = s3_config.get('small_target', 10)

    # 1. Try evicting from Small if it's over budget or if Main is empty
    while len(s3_small) > target_small or (not s3_main and s3_small):
        # Garbage Collection: Occasional FIFO eviction (10% chance)
        s3_config['evict_count'] += 1
        if s3_config['evict_count'] % 10 == 0:
            head_key = next(iter(s3_small))
            # Only evict Head if it has 0 frequency (static garbage)
            if s3_freq.get(head_key, 0) == 0:
                return head_key

        # Standard LIFO Eviction: Inspect tail (newest)
        victim_key, _ = s3_small.popitem(last=True)

        # Check Promotion (freq > 1)
        if s3_freq.get(victim_key, 0) > 1:
            # Promote to Main (MRU)
            s3_main[victim_key] = 1
            s3_main.move_to_end(victim_key)
        else:
            # Victim found. Put back to maintain state until update_after_evict
            s3_small[victim_key] = 1
            return victim_key

    # 2. If Small is safe, evict from Main (LRU)
    if s3_main:
        return next(iter(s3_main))

    # Fallback
    if s3_small:
        return next(iter(s3_small))
    return None
>>>>>>> REPLACE

</DIFF>

<NAME>
update_insert_adaptive
</NAME>

<DESCRIPTION>
Update `update_after_insert` to implement Adaptive Sizing logic (Inspired by ARC).
- If a ghost hit occurs, check if it was previously in Main or Small (using the new tuple format).
- If it was from Main, shrink `small_target` (Main needs more space).
- If it was from Small, grow `small_target` (Small needs more space).
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - Ghost Hit: Restore freq, Promote to Main.
    - New: Insert to Small (Probation).
    '''
    _reset_state(cache_snapshot)
    key = obj.key

    if key in s3_ghost:
        # Restore frequency
        restored_freq = s3_ghost.pop(key)
        s3_freq[key] = restored_freq
        # Promote to Main immediately
        s3_main[key] = 1
        s3_main.move_to_end(key)
    else:
        # New Item -> Small
        s3_small[key] = 1
        s3_freq[key] = 0
=======
def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - Ghost Hit: Restore freq, Promote to Main, Adapt Target.
    - New: Insert to Small (Probation).
    '''
    _reset_state(cache_snapshot)
    key = obj.key

    if key in s3_ghost:
        # Restore frequency and check origin
        val = s3_ghost.pop(key)
        if isinstance(val, tuple):
            restored_freq, was_main = val
        else:
            restored_freq, was_main = val, False

        # Adapt Small Target
        # If ghost from Main, Main was too small -> Shrink Small target
        # If ghost from Small, Small was too small (LIFO evicted useful item) -> Grow Small target
        target = s3_config.get('small_target', 10)
        if was_main:
            target = max(1, target - 1)
        else:
            target = min(cache_snapshot.capacity - 1, target + 1)
        s3_config['small_target'] = target

        s3_freq[key] = restored_freq
        # Promote to Main immediately
        s3_main[key] = 1
        s3_main.move_to_end(key)
    else:
        # New Item -> Small
        s3_small[key] = 1
        s3_freq[key] = 0
>>>>>>> REPLACE

</DIFF>

<NAME>
update_evict_store_source
</NAME>

<DESCRIPTION>
Update `update_after_evict` to store the origin queue (`was_main`) in the ghost registry along with the frequency. This supports the adaptive sizing logic in `update_after_insert`.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Remove from queues.
    - Save to Ghost (with frequency).
    '''
    key = evicted_obj.key
    capacity = cache_snapshot.capacity

    if key in s3_small:
        del s3_small[key]
    elif key in s3_main:
        del s3_main[key]

    # Save to Ghost
    current_freq = s3_freq.get(key, 0)
    s3_ghost[key] = current_freq
    s3_ghost.move_to_end(key) # Mark as recent in Ghost

    # Clean up freq map
    if key in s3_freq:
        del s3_freq[key]

    # Maintain Ghost size
    if len(s3_ghost) > capacity:
        s3_ghost.popitem(last=False) # Remove oldest ghost
=======
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Remove from queues.
    - Save to Ghost (with frequency and origin).
    '''
    key = evicted_obj.key
    capacity = cache_snapshot.capacity

    was_main = False
    if key in s3_small:
        del s3_small[key]
    elif key in s3_main:
        was_main = True
        del s3_main[key]

    # Save to Ghost
    current_freq = s3_freq.get(key, 0)
    s3_ghost[key] = (current_freq, was_main)
    s3_ghost.move_to_end(key) # Mark as recent in Ghost

    # Clean up freq map
    if key in s3_freq:
        del s3_freq[key]

    # Maintain Ghost size
    if len(s3_ghost) > capacity:
        s3_ghost.popitem(last=False) # Remove oldest ghost
>>>>>>> REPLACE

</DIFF>