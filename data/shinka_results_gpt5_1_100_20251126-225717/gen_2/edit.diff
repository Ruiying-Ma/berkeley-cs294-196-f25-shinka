--- a/original.py
+++ b/original.py
@@ -1,150 +1,176 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
+"""Cache eviction algorithm using decayed LFU with ghost-admission bias."""
 
-# Legacy timestamp dictionary kept for compatibility; used as a general access ledger.
+import math
+
+# Compatibility ledger from legacy code (kept but not relied upon for policy)
 m_key_timestamp = dict()
 
-# Segmented LRU metadata: probation and protected segments (key -> last access time)
-_probation = dict()
-_protected = dict()
+# Per-key decayed frequency state
+_score = dict()          # key -> decayed frequency score (float)
+_last_update = dict()    # key -> last timestamp when score was updated (int)
 
-def _get_caps(cache_snapshot):
-    """Compute target sizes for probation and protected segments."""
-    total_cap = max(int(getattr(cache_snapshot, "capacity", 1)), 1)
-    # Favor protected segment to keep repeatedly used items
-    prot_cap = max(int(total_cap * 0.66), 1 if total_cap > 1 else 0)
-    prob_cap = max(total_cap - prot_cap, 1)
-    return prob_cap, prot_cap
+# Ghost history: tracks last-access time for recently evicted keys to bias re-admission
+_ghost_last_access = dict()  # key -> last access time recorded at eviction
 
-def _lru_key_in(seg_dict, cache_snapshot):
-    """Return the LRU key from seg_dict that is currently in the cache."""
-    min_key = None
-    min_ts = None
-    # Iterate only over keys that are in the current cache snapshot
-    cache_keys = cache_snapshot.cache.keys()
-    for k, ts in seg_dict.items():
-        if k in cache_keys:
-            if (min_ts is None) or (ts < min_ts):
-                min_ts = ts
-                min_key = k
-    return min_key
+
+def _half_life(cache_snapshot):
+    """
+    Choose a decay half-life scaled to cache capacity.
+    Larger caches get a longer memory; smaller caches emphasize recency more.
+    """
+    cap = max(int(getattr(cache_snapshot, "capacity", 1)), 1)
+    # Empirically effective: ~1.5x capacity accesses as half-life
+    return max(8, int(1.5 * cap))
+
+
+def _decay_factor(delta, hl):
+    if hl <= 0:
+        return 0.0
+    # Exponential decay to half every 'hl' accesses: 0.5 ** (delta/hl)
+    return math.pow(0.5, float(delta) / float(hl))
+
+
+def _current_score(key, now, hl):
+    s = _score.get(key, 0.0)
+    last = _last_update.get(key, now)
+    if s <= 0.0:
+        return 0.0
+    delta = now - last
+    if delta <= 0:
+        return s
+    return s * _decay_factor(delta, hl)
+
 
 def evict(cache_snapshot, obj):
     '''
     This function defines how the algorithm chooses the eviction victim.
     - Args:
         - `cache_snapshot`: A snapshot of the current cache state.
         - `obj`: The new object that needs to be inserted into the cache.
     - Return:
         - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
     '''
-    # Prefer evicting from the probation segment to protect re-referenced items.
-    candid_obj_key = _lru_key_in(_probation, cache_snapshot)
-    if candid_obj_key is None:
-        # Fall back to protected segment if probation has no candidates in cache.
-        candid_obj_key = _lru_key_in(_protected, cache_snapshot)
-    if candid_obj_key is None:
-        # Last-resort fallback: pick any key from the cache (should rarely happen).
+    now = cache_snapshot.access_count
+    hl = _half_life(cache_snapshot)
+
+    # Select the key with the minimal decayed LFU score; tie-break by oldest last_update (LRU-ish).
+    min_key = None
+    min_score = None
+    min_last = None
+
+    for k in cache_snapshot.cache.keys():
+        cs = _current_score(k, now, hl)
+        lu = _last_update.get(k, now)
+        if (min_score is None) or (cs < min_score) or (cs == min_score and lu < min_last):
+            min_key = k
+            min_score = cs
+            min_last = lu
+
+    # Fallback (should not happen, but keep safe)
+    if min_key is None:
         for k in cache_snapshot.cache:
-            candid_obj_key = k
+            min_key = k
             break
-    return candid_obj_key
+    return min_key
+
 
 def update_after_hit(cache_snapshot, obj):
     '''
-    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object accessed during the cache hit.
-    - Return: `None`
+    Update metadata immediately after a cache hit.
     '''
-    global m_key_timestamp, _probation, _protected
-    current_ts = cache_snapshot.access_count
+    global m_key_timestamp, _score, _last_update, _ghost_last_access
+
+    now = cache_snapshot.access_count
     k = obj.key
+    hl = _half_life(cache_snapshot)
 
-    # Maintain a general timestamp ledger for robustness.
-    m_key_timestamp[k] = current_ts
+    # Update decayed score lazily on access
+    cur = _current_score(k, now, hl)
+    _score[k] = cur + 1.0
+    _last_update[k] = now
 
-    if k in _probation:
-        # Promote to protected on second touch.
-        _probation.pop(k, None)
-        _protected[k] = current_ts
+    # Maintain general ledger and ghost recency (used for admission)
+    m_key_timestamp[k] = now
+    _ghost_last_access[k] = now  # keep last access fresh for this key
 
-        # If protected exceeds capacity, demote its LRU back to probation.
-        prob_cap, prot_cap = _get_caps(cache_snapshot)
-        if len(_protected) > prot_cap:
-            demote_key = _lru_key_in(_protected, cache_snapshot)
-            if demote_key is not None and demote_key != k:
-                demote_ts = _protected.pop(demote_key, current_ts)
-                # Keep original timestamp to maintain proper LRU ordering in probation.
-                _probation[demote_key] = demote_ts
-    elif k in _protected:
-        # Refresh recency within protected.
-        _protected[k] = current_ts
-    else:
-        # Metadata miss: treat as a re-reference and place in protected.
-        _protected[k] = current_ts
-        prob_cap, prot_cap = _get_caps(cache_snapshot)
-        if len(_protected) > prot_cap:
-            demote_key = _lru_key_in(_protected, cache_snapshot)
-            if demote_key is not None and demote_key != k:
-                demote_ts = _protected.pop(demote_key, current_ts)
-                _probation[demote_key] = demote_ts
 
 def update_after_insert(cache_snapshot, obj):
     '''
-    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object that was just inserted into the cache.
-    - Return: `None`
+    Update metadata immediately after inserting a new object into the cache.
     '''
-    global m_key_timestamp, _probation, _protected
-    current_ts = cache_snapshot.access_count
+    global m_key_timestamp, _score, _last_update, _ghost_last_access
+
+    now = cache_snapshot.access_count
     k = obj.key
+    hl = _half_life(cache_snapshot)
 
-    # Record in general ledger and place new entries in the probation segment.
-    m_key_timestamp[k] = current_ts
-    _probation[k] = current_ts
+    # Admission bias: if the key was seen recently in ghost, give it a stronger initial score.
+    # Otherwise, keep a conservative initial score to avoid pollution.
+    base = 0.3
+    last_ghost = _ghost_last_access.get(k)
+    if last_ghost is not None:
+        # Recently evicted and accessed within a half-life -> boost
+        if (now - last_ghost) <= hl:
+            base = 1.2
 
-    # Optional hygiene: if probation grows too large relative to capacity, no immediate action is required.
-    # Eviction will naturally prefer probation, which is desired.
+    _score[k] = base
+    _last_update[k] = now
+    m_key_timestamp[k] = now
+    # We do not remove from ghost here; admission bias can persist for a short time.
+
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     '''
-    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object to be inserted into the cache.
-        - `evicted_obj`: The object that was just evicted from the cache.
-    - Return: `None`
+    Update metadata immediately after evicting the victim.
     '''
-    global m_key_timestamp, _probation, _protected
-    # Remove evicted key from all metadata stores.
-    if evicted_obj is not None:
-        _probation.pop(evicted_obj.key, None)
-        _protected.pop(evicted_obj.key, None)
-        m_key_timestamp.pop(evicted_obj.key, None)
-    # Do not add obj here; it will be handled in update_after_insert.
+    global m_key_timestamp, _score, _last_update, _ghost_last_access
+
+    if evicted_obj is None:
+        return
+
+    ek = evicted_obj.key
+    # Record last access time of evicted key into ghost for admission decisions
+    last_seen = _last_update.get(ek, cache_snapshot.access_count)
+    _ghost_last_access[ek] = last_seen
+
+    # Remove from main metadata
+    _score.pop(ek, None)
+    _last_update.pop(ek, None)
+    m_key_timestamp.pop(ek, None)
+
+    # Bound ghost size to avoid unbounded growth: keep up to ~50x capacity recent ghosts
+    cap = max(int(getattr(cache_snapshot, "capacity", 1)), 1)
+    max_ghost = max(50 * cap, 100)  # ensure reasonable minimum
+    if len(_ghost_last_access) > max_ghost:
+        # Evict the stalest ghosts
+        # Remove approx 10% overflow to amortize cost
+        target = len(_ghost_last_access) - max_ghost
+        # Sort by last access ascending and drop 'target' items
+        # For performance, do a simple selection loop without full sort
+        # but given this triggers rarely, a sort is fine.
+        stale_items = sorted(_ghost_last_access.items(), key=lambda x: x[1])[:target]
+        for key, _ in stale_items:
+            _ghost_last_access.pop(key, None)
 
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate