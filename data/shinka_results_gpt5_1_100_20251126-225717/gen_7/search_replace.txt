<NAME>
slru_with_freq_tiebreak_and_ghost_bias
</NAME>

<DESCRIPTION>
I propose enhancing the existing decayed-LFU policy with an SLRU (segmented LRU) structure and refined admission bias. The key changes:

- Maintain two segments (probation and protected). New items enter probation; on hit they get promoted to protected. Eviction prefers probation first (LRU), then protected (frequency-aware tiebreaker).
- Use decayed LFU scores only inside tie-breaking and for protected segment victim selection, preserving long-term popularity while SLRU enforces recency structure.
- Enforce a protected segment size limit (~80% of capacity) via demotions (LRU) to probation ensuring frequent items are protected but we still allow new items to prove themselves.
- Ghost history continues to bias admission: items seen recently in ghost enter protected immediately.
- Robustness: prune/seed segment sets to remain consistent with actual cache contents; this handles cold start and any drift.
- Keep lazy exponential decay scoring for hits/inserts for better adaptivity across workloads.

This hybrid SLRU + decayed LFU approach improves resilience to scans while keeping frequently reused items resident, which should lower miss rates on mixed traces and protect heavy hitters compared to pure decayed LFU.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# Ghost history: tracks last-access time for recently evicted keys to bias re-admission
_ghost_last_access = dict()  # key -> last access time recorded at eviction
=======
# Ghost history: tracks last-access time for recently evicted keys to bias re-admission
_ghost_last_access = dict()  # key -> last access time recorded at eviction

# SLRU segments
_probation = set()
_protected = set()
_PROTECTED_RATIO = 0.8
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def _current_score(key, now, hl):
    s = _score.get(key, 0.0)
    last = _last_update.get(key, now)
    if s <= 0.0:
        return 0.0
    delta = now - last
    if delta <= 0:
        return s
    return s * _decay_factor(delta, hl)


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)

    # Select the key with the minimal decayed LFU score; tie-break by oldest last_update (LRU-ish).
    min_key = None
    min_score = None
    min_last = None

    for k in cache_snapshot.cache.keys():
        cs = _current_score(k, now, hl)
        lu = _last_update.get(k, now)
        if (min_score is None) or (cs < min_score) or (cs == min_score and lu < min_last):
            min_key = k
            min_score = cs
            min_last = lu

    # Fallback (should not happen, but keep safe)
    if min_key is None:
        for k in cache_snapshot.cache:
            min_key = k
            break
    return min_key
=======
def _current_score(key, now, hl):
    s = _score.get(key, 0.0)
    last = _last_update.get(key, now)
    if s <= 0.0:
        return 0.0
    delta = now - last
    if delta <= 0:
        return s
    return s * _decay_factor(delta, hl)


def _protected_limit(cache_snapshot):
    cap = max(int(getattr(cache_snapshot, "capacity", 1)), 1)
    # At least 1 item protected, up to 80% of capacity
    return max(1, int(_PROTECTED_RATIO * cap))


def _prune_and_seed_segments(cache_snapshot):
    """Keep SLRU segments consistent with actual cache keys and seed unknown keys into probation."""
    in_cache = set(cache_snapshot.cache.keys())
    # Drop any keys that are no longer in cache
    _probation.intersection_update(in_cache)
    _protected.intersection_update(in_cache)
    # Seed any missing cached keys into probation (e.g., after cold start)
    unknown = in_cache.difference(_probation).difference(_protected)
    if unknown:
        _probation.update(unknown)


def _enforce_protected_limit(cache_snapshot):
    """Demote oldest from protected until within limit."""
    limit = _protected_limit(cache_snapshot)
    if not _protected:
        return
    while len(_protected) > limit:
        # Demote LRU from protected to probation
        oldest = None
        oldest_ts = None
        for kk in _protected:
            ts = _last_update.get(kk, 0)
            if oldest_ts is None or ts < oldest_ts:
                oldest = kk
                oldest_ts = ts
        if oldest is None:
            break
        _protected.discard(oldest)
        _probation.add(oldest)


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)

    # Ensure segments reflect current cache contents
    _prune_and_seed_segments(cache_snapshot)

    in_cache = set(cache_snapshot.cache.keys())
    prob = _probation.intersection(in_cache)
    prot = _protected.intersection(in_cache)

    # Prefer evicting from probation (SLRU). Use LRU with frequency tiebreaker.
    if prob:
        victim = None
        best_tuple = None  # (last_update, decayed_score)
        for k in prob:
            lu = _last_update.get(k, 0)
            cs = _current_score(k, now, hl)
            tup = (lu, cs)
            if best_tuple is None or tup < best_tuple:
                best_tuple = tup
                victim = k
        return victim

    # If probation empty, evict from protected: pick lowest decayed score, tiebreak by oldest
    if prot:
        victim = None
        min_score = None
        min_last = None
        for k in prot:
            cs = _current_score(k, now, hl)
            lu = _last_update.get(k, 0)
            if (min_score is None) or (cs < min_score) or (cs == min_score and lu < min_last):
                victim = k
                min_score = cs
                min_last = lu
        return victim

    # Fallback (should not happen): pick any key in cache
    for k in cache_snapshot.cache:
        return k
    return None
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    Update metadata immediately after a cache hit.
    '''
    global m_key_timestamp, _score, _last_update, _ghost_last_access

    now = cache_snapshot.access_count
    k = obj.key
    hl = _half_life(cache_snapshot)

    # Update decayed score lazily on access
    cur = _current_score(k, now, hl)
    _score[k] = cur + 1.0
    _last_update[k] = now

    # Maintain general ledger and ghost recency (used for admission)
    m_key_timestamp[k] = now
    _ghost_last_access[k] = now  # keep last access fresh for this key
=======
def update_after_hit(cache_snapshot, obj):
    '''
    Update metadata immediately after a cache hit.
    '''
    global m_key_timestamp, _score, _last_update, _ghost_last_access, _probation, _protected

    now = cache_snapshot.access_count
    k = obj.key
    hl = _half_life(cache_snapshot)

    # Ensure segments reflect current cache contents
    _prune_and_seed_segments(cache_snapshot)

    # Update decayed score lazily on access
    cur = _current_score(k, now, hl)
    _score[k] = cur + 1.0
    _last_update[k] = now

    # Promotion: if hit in probation, move to protected
    if k in _probation:
        _probation.discard(k)
        _protected.add(k)
    elif k not in _protected:
        # If key was unknown to segments but in cache, start in probation then promote on first hit
        _probation.add(k)
        _probation.discard(k)
        _protected.add(k)

    # Enforce protected size limit via demotion (LRU)
    _enforce_protected_limit(cache_snapshot)

    # Maintain general ledger and ghost recency (used for admission)
    m_key_timestamp[k] = now
    _ghost_last_access[k] = now  # keep last access fresh for this key
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    Update metadata immediately after inserting a new object into the cache.
    '''
    global m_key_timestamp, _score, _last_update, _ghost_last_access

    now = cache_snapshot.access_count
    k = obj.key
    hl = _half_life(cache_snapshot)

    # Admission bias: if the key was seen recently in ghost, give it a stronger initial score.
    # Otherwise, keep a conservative initial score to avoid pollution.
    base = 0.3
    last_ghost = _ghost_last_access.get(k)
    if last_ghost is not None:
        # Recently evicted and accessed within a half-life -> boost
        if (now - last_ghost) <= hl:
            base = 1.2

    _score[k] = base
    _last_update[k] = now
    m_key_timestamp[k] = now
    # We do not remove from ghost here; admission bias can persist for a short time.
=======
def update_after_insert(cache_snapshot, obj):
    '''
    Update metadata immediately after inserting a new object into the cache.
    '''
    global m_key_timestamp, _score, _last_update, _ghost_last_access, _probation, _protected

    now = cache_snapshot.access_count
    k = obj.key
    hl = _half_life(cache_snapshot)

    # Admission bias: if the key was seen recently in ghost, insert directly into protected.
    base = 0.3
    place_in_protected = False
    last_ghost = _ghost_last_access.get(k)
    if last_ghost is not None and (now - last_ghost) <= hl:
        base = 1.2
        place_in_protected = True

    _score[k] = base
    _last_update[k] = now
    m_key_timestamp[k] = now

    # Place new object into SLRU segments
    _prune_and_seed_segments(cache_snapshot)
    if place_in_protected:
        _protected.add(k)
        _probation.discard(k)
        _enforce_protected_limit(cache_snapshot)
    else:
        _probation.add(k)
        _protected.discard(k)
    # Keep ghost record; don't delete so brief reinsertions can still get bias.
>>>>>>> REPLACE

</DIFF>

<DIFF>
<<<<<<< SEARCH
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Update metadata immediately after evicting the victim.
    '''
    global m_key_timestamp, _score, _last_update, _ghost_last_access

    if evicted_obj is None:
        return

    ek = evicted_obj.key
    # Record last access time of evicted key into ghost for admission decisions
    last_seen = _last_update.get(ek, cache_snapshot.access_count)
    _ghost_last_access[ek] = last_seen

    # Remove from main metadata
    _score.pop(ek, None)
    _last_update.pop(ek, None)
    m_key_timestamp.pop(ek, None)

    # Bound ghost size to avoid unbounded growth: keep up to ~50x capacity recent ghosts
    cap = max(int(getattr(cache_snapshot, "capacity", 1)), 1)
    max_ghost = max(50 * cap, 100)  # ensure reasonable minimum
    if len(_ghost_last_access) > max_ghost:
        # Evict the stalest ghosts
        # Remove approx 10% overflow to amortize cost
        target = len(_ghost_last_access) - max_ghost
        # Sort by last access ascending and drop 'target' items
        # For performance, do a simple selection loop without full sort
        # but given this triggers rarely, a sort is fine.
        stale_items = sorted(_ghost_last_access.items(), key=lambda x: x[1])[:target]
        for key, _ in stale_items:
            _ghost_last_access.pop(key, None)
=======
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Update metadata immediately after evicting the victim.
    '''
    global m_key_timestamp, _score, _last_update, _ghost_last_access, _probation, _protected

    if evicted_obj is None:
        return

    ek = evicted_obj.key
    # Record last access time of evicted key into ghost for admission decisions
    last_seen = _last_update.get(ek, cache_snapshot.access_count)
    _ghost_last_access[ek] = last_seen

    # Remove from main metadata and SLRU segments
    _score.pop(ek, None)
    _last_update.pop(ek, None)
    m_key_timestamp.pop(ek, None)
    _probation.discard(ek)
    _protected.discard(ek)

    # Bound ghost size to avoid unbounded growth: keep up to ~50x capacity recent ghosts
    cap = max(int(getattr(cache_snapshot, "capacity", 1)), 1)
    max_ghost = max(50 * cap, 100)  # ensure reasonable minimum
    if len(_ghost_last_access) > max_ghost:
        # Evict the stalest ghosts
        target = len(_ghost_last_access) - max_ghost
        stale_items = sorted(_ghost_last_access.items(), key=lambda x: x[1])[:target]
        for key, _ in stale_items:
            _ghost_last_access.pop(key, None)
>>>>>>> REPLACE

</DIFF>