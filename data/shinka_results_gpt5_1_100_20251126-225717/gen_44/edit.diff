--- a/original.py
+++ b/original.py
@@ -1,280 +1,373 @@
 # EVOLVE-BLOCK-START
-"""Adaptive ARC-like cache eviction with SLRU segments and ghost history"""
+"""Scan-aware ARC+SLRU with freshness-aware ghosts, momentum p-updates, and aging LRFU sampling"""
 
 from collections import OrderedDict
 
-# Segment structures
-_T1_probation = OrderedDict()   # keys in cache, 1st-touch (recency-biased)
-_T2_protected = OrderedDict()   # keys in cache, 2nd+ touch (frequency-biased)
-
-# Ghost history (recently evicted keys) for adaptation
-_B1_ghost = OrderedDict()       # evicted from T1
-_B2_ghost = OrderedDict()       # evicted from T2
-
-# Adaptive target size for probation (ARC's p). Float to allow smooth adjust.
+# Segments (resident)
+_T1_probation = OrderedDict()   # first-touch, recency-biased
+_T2_protected = OrderedDict()   # multi-touch, frequency-biased
+
+# Ghost histories (evicted keys) store eviction timestamps for freshness
+_B1_ghost = OrderedDict()       # from T1: key -> evict_ts
+_B2_ghost = OrderedDict()       # from T2: key -> evict_ts
+
+# ARC's adaptive target (float) for T1 size
 _p_target = 0.0
-
-# Estimated capacity (number of objects). Initialize lazily.
 _cap_est = 0
 
-# Fallback LRU timestamps if metadata desync occurs
-m_key_timestamp = dict()
-
-# Lightweight per-key frequency counter (hit count)
-_freq = dict()  # key -> int
-
-# Admission guard based on last victim "strength"
+# Fallback timestamp ledger and lightweight frequency
+m_key_timestamp = dict()        # key -> last access time (for tie-breaking)
+_freq = dict()                  # key -> small counter (saturating)
+_last_age_tick = 0
+
+# Admission guard based on last victim strength and a short scan window
 _last_victim_strength = 0.0
-_VICTIM_GUARD_THRESH = 2.0  # if last victim was strong, down-seed next newcomer
-
-# Eviction sampling (number of LRU candidates to compare by frequency)
-_T1_SAMPLE = 2
-_T2_SAMPLE = 3
-
-# Tunable parameters
-_P_INIT_RATIO = 0.3  # initial share for probation (T1)
-# On ghost hits, adjust p by this delta rule (ARC-like):
-# delta = ratio of other ghost to this ghost (float), clamped and then p is clamped to [0, cap]
+_VICTIM_GUARD_THRESH = 2.0
+_guard_until = 0
+
+# Scan detection and momentum for p updates
+_hit_ewma = 0.0
+_ins_ewma = 0.0
+_EWMA_ALPHA = 0.05
+_scan_until = 0
+_p_momentum = 0.0
+_p_last_update_tick = 0
+
+# Tunables
+_P_INIT_RATIO = 0.30             # initial share for T1
+_FREQ_MAX = 7                    # 3-bit saturating counter
+_FRESH_WINDOW_RATIO = 0.5        # ghost freshness window = 0.5 * cap
+_SCAN_TRIGGER_INS = 0.7          # insert EWMA threshold
+_SCAN_TRIGGER_HIT = 0.15         # hit EWMA threshold
+_SCAN_WINDOW_MULT = 1.0          # scan window length ~= cap accesses
 
 def _ensure_capacity(cache_snapshot):
-    """Initialize or update capacity estimate and clamp p."""
+    """Initialize capacity and clamp p within [0, cap]."""
     global _cap_est, _p_target
     cap = getattr(cache_snapshot, "capacity", None)
-    # Some runners define capacity as number of objects; if absent, infer
     if isinstance(cap, int) and cap > 0:
         _cap_est = cap
     else:
         _cap_est = max(_cap_est, len(cache_snapshot.cache))
     if _cap_est <= 0:
         _cap_est = max(1, len(cache_snapshot.cache))
-    # Initialize p if never set (zero and empty metadata)
     if _p_target == 0.0 and not _T1_probation and not _T2_protected and not _B1_ghost and not _B2_ghost:
-        _p_target = max(0.0, min(float(_cap_est), float(_cap_est) * _P_INIT_RATIO))
-    # Clamp p
+        _p_target = min(float(_cap_est), max(0.0, float(_cap_est) * _P_INIT_RATIO))
     if _p_target < 0.0:
         _p_target = 0.0
     if _p_target > float(_cap_est):
         _p_target = float(_cap_est)
 
 def _ghost_trim():
-    """Limit ghost lists to capacity each (ARC-style bound)."""
-    global _B1_ghost, _B2_ghost
-    # Trim oldest entries beyond capacity
+    """Bound ghosts by capacity."""
     while len(_B1_ghost) > _cap_est:
         _B1_ghost.popitem(last=False)
     while len(_B2_ghost) > _cap_est:
         _B2_ghost.popitem(last=False)
 
+def _maybe_age(cache_snapshot):
+    """Periodically age frequencies to avoid stale bias."""
+    global _last_age_tick
+    _ensure_capacity(cache_snapshot)
+    now = cache_snapshot.access_count
+    if now - _last_age_tick >= max(1, _cap_est):
+        for k in list(_freq.keys()):
+            newf = _freq.get(k, 0) // 2
+            if newf <= 0:
+                _freq.pop(k, None)
+            else:
+                _freq[k] = newf
+        _last_age_tick = now
+
+def _update_activity(is_hit, cache_snapshot):
+    """Track recent hit/miss behavior and activate scan window if needed."""
+    global _hit_ewma, _ins_ewma, _scan_until
+    alpha = _EWMA_ALPHA
+    _hit_ewma = (1.0 - alpha) * _hit_ewma + alpha * (1.0 if is_hit else 0.0)
+    _ins_ewma = (1.0 - alpha) * _ins_ewma + alpha * (0.0 if is_hit else 1.0)
+    if (_ins_ewma > _SCAN_TRIGGER_INS) and (_hit_ewma < _SCAN_TRIGGER_HIT):
+        _scan_until = cache_snapshot.access_count + int(max(1, _SCAN_WINDOW_MULT * _cap_est))
+
+def _adjust_p(sign, step, now, freshness_scale=1.0):
+    """Momentum-based adjustment of ARC's p with clamping."""
+    global _p_target, _p_momentum, _p_last_update_tick
+    # Scale step by freshness and bound to 0.25*cap to avoid wild swings
+    bounded = min(max(1.0, float(step) * float(freshness_scale)), max(1.0, 0.25 * float(_cap_est)))
+    _p_momentum = 0.5 * _p_momentum + float(sign) * bounded
+    _p_target += _p_momentum
+    if _p_target < 0.0:
+        _p_target = 0.0
+        _p_momentum = 0.0
+    if _p_target > float(_cap_est):
+        _p_target = float(_cap_est)
+        _p_momentum = 0.0
+    _p_last_update_tick = now
+
 def _fallback_choose(cache_snapshot):
-    """Fallback victim: global LRU by timestamp among cached keys."""
-    # Prefer the minimum timestamp; if unknown, pick arbitrary
+    """LRU fallback using timestamps."""
     keys = list(cache_snapshot.cache.keys())
     if not keys:
         return None
-    # Filter to known timestamps
     known = [(k, m_key_timestamp.get(k, None)) for k in keys]
     known_ts = [x for x in known if x[1] is not None]
     if known_ts:
-        k = min(known_ts, key=lambda kv: kv[1])[0]
-        return k
+        return min(known_ts, key=lambda kv: kv[1])[0]
     return keys[0]
 
+def _lru_iter(od):
+    """Iterate keys from LRU to MRU for an OrderedDict."""
+    for k in od.keys():
+        yield k
+
+def _score_key(k):
+    """Compute victim score: lower is better (less frequent, older)."""
+    return (_freq.get(k, 0), m_key_timestamp.get(k, 0))
+
+def _pick_from(od, sample_n, cache_snapshot):
+    """Pick victim from first few LRU entries by (freq asc, timestamp asc)."""
+    if not od:
+        return None
+    cnt = 0
+    best_k = None
+    best_sc = None
+    for k in _lru_iter(od):
+        if k not in cache_snapshot.cache:
+            continue
+        sc = _score_key(k)
+        if best_sc is None or sc < best_sc:
+            best_sc = sc
+            best_k = k
+        cnt += 1
+        if cnt >= sample_n:
+            break
+    return best_k
+
+def _demote_protected_if_needed(cache_snapshot, avoid_key=None):
+    """Keep T2 size within ARC target by demoting LRU entries to T1 MRU."""
+    _ensure_capacity(cache_snapshot)
+    t1_target = int(round(_p_target))
+    t2_target = max(_cap_est - t1_target, 0)
+    while len(_T2_protected) > t2_target:
+        # LRU of T2
+        lru = None
+        for k in _lru_iter(_T2_protected):
+            if k != avoid_key and k in cache_snapshot.cache:
+                lru = k
+                break
+        if lru is None:
+            break
+        _T2_protected.pop(lru, None)
+        _T1_probation[lru] = True  # demoted MRU in T1
+
 def evict(cache_snapshot, obj):
     '''
-    Choose victim key using adaptive ARC-like policy with small frequency-aware sampling.
-    REPLACE(x): if |T1|>=1 and ((x in B2 and |T1| == p) or |T1| > p) -> evict from T1 else from T2.
-    Within the chosen segment, pick the lowest-frequency among a few LRU candidates.
+    Evict using ARC replace with dynamic sampling and scan bias:
+    - Prefer T1 when |T1| > p or when upcoming key is in B2 and |T1| == p.
+    - During scan window, always prefer T1 if non-empty.
+    - Within the chosen segment, sample a few LRU entries and pick (freq asc, timestamp asc).
     '''
     _ensure_capacity(cache_snapshot)
 
     t1_size = len(_T1_probation)
     t2_size = len(_T2_protected)
     x_in_b2 = (obj is not None) and (obj.key in _B2_ghost)
-    p_int = int(round(_p_target))
-    choose_t1 = (t1_size >= 1) and ((x_in_b2 and t1_size == p_int) or (t1_size > _p_target))
-
-    def _pick_from(od, sample_n):
-        # Sample the first few LRU keys and pick by (freq asc, timestamp asc)
-        if not od:
-            return None
-        it = iter(od.keys())
-        candidates = []
-        for _ in range(min(sample_n, len(od))):
-            try:
-                k = next(it)
-            except StopIteration:
-                break
-            candidates.append(k)
-        if not candidates:
-            return None
-        def score(k):
-            # Lower freq better; older (smaller timestamp) better
-            return (_freq.get(k, 0), m_key_timestamp.get(k, 0))
-        return min(candidates, key=score)
+    choose_t1 = (t1_size >= 1) and ((x_in_b2 and t1_size == int(round(_p_target))) or (t1_size > _p_target))
+
+    # Scan bias: keep evictions in probation when scanning
+    if cache_snapshot.access_count <= _scan_until and t1_size > 0:
+        choose_t1 = True
+
+    # Adaptive sampling sizes based on pressure and scan
+    cap = max(1, _cap_est)
+    t1_pressure = (t1_size > _p_target + 0.1 * cap) or (cache_snapshot.access_count <= _scan_until)
+    t2_pressure = (t2_size > (cap - int(round(_p_target)))) or False
+
+    T1_SAMPLE = 1 if t1_pressure else 2
+    if cache_snapshot.access_count <= _scan_until:
+        T1_SAMPLE = 1
+    T2_SAMPLE = 5 if t2_pressure else 3
+    # If the overall recent hit rate is low, reduce T2 sampling slightly
+    if _hit_ewma < 0.2:
+        T2_SAMPLE = max(2, T2_SAMPLE - 1)
 
     victim_key = None
     if choose_t1 and t1_size > 0:
-        victim_key = _pick_from(_T1_probation, _T1_SAMPLE)
-        if victim_key not in cache_snapshot.cache:
-            victim_key = None
+        victim_key = _pick_from(_T1_probation, T1_SAMPLE, cache_snapshot)
     if victim_key is None and t2_size > 0:
-        victim_key = _pick_from(_T2_protected, _T2_SAMPLE)
-        if victim_key not in cache_snapshot.cache:
-            victim_key = None
+        victim_key = _pick_from(_T2_protected, T2_SAMPLE, cache_snapshot)
     if victim_key is None and t1_size > 0:
-        victim_key = _pick_from(_T1_probation, _T1_SAMPLE)
-        if victim_key not in cache_snapshot.cache:
-            victim_key = None
+        victim_key = _pick_from(_T1_probation, T1_SAMPLE, cache_snapshot)
     if victim_key is None:
-        # Fallback to global LRU if metadata desync
         victim_key = _fallback_choose(cache_snapshot)
     return victim_key
 
 def update_after_hit(cache_snapshot, obj):
     '''
-    Update metadata after cache hit.
-    - If hit in probation (T1), promote to protected (T2).
-    - If hit in protected, refresh recency.
-    - Maintain fallback timestamp map and per-key frequency.
-    '''
-    _ensure_capacity(cache_snapshot)
+    On hit:
+    - Update EWMA and age frequencies.
+    - Increment frequency (saturating).
+    - In scan mode: require two touches in T1 before promotion (first hit -> T1 MRU).
+    - Otherwise: first hit in T1 promotes to T2.
+    - Keep T2 within its ARC target via demotion.
+    - Remove any ghost entries for this key.
+    '''
+    _ensure_capacity(cache_snapshot)
+    _maybe_age(cache_snapshot)
+    _update_activity(True, cache_snapshot)
+
     key = obj.key
-    # Update fallback LRU timestamp
-    m_key_timestamp[key] = cache_snapshot.access_count
-    # Increment frequency counter
-    _freq[key] = _freq.get(key, 0) + 1
-
-    # If the key exists in our segments, update positions
+    now = cache_snapshot.access_count
+    m_key_timestamp[key] = now
+    _freq[key] = min(_FREQ_MAX, _freq.get(key, 0) + 1)
+
+    in_scan = now <= _scan_until
+
     if key in _T2_protected:
-        # Refresh to MRU
         _T2_protected.move_to_end(key, last=True)
     elif key in _T1_probation:
-        # Promote from probation to protected
-        _T1_probation.pop(key, None)
-        _T2_protected[key] = True  # insert as MRU
+        if in_scan and _freq.get(key, 0) < 2:
+            # First re-touch during scan: keep in T1 and move to MRU
+            _T1_probation.move_to_end(key, last=True)
+        else:
+            # Promote to protected
+            _T1_probation.pop(key, None)
+            _T2_protected[key] = True
     else:
-        # Metadata miss: cache has it but we don't; treat as frequent and add to protected
+        # Metadata miss: treat as hot and place in T2
         _T2_protected[key] = True
-    # Touch ghosts cleanup if any stale
+
+    _demote_protected_if_needed(cache_snapshot, avoid_key=key)
+
+    # Ghost cleanup
     if key in _B1_ghost:
         _B1_ghost.pop(key, None)
     if key in _B2_ghost:
         _B2_ghost.pop(key, None)
     _ghost_trim()
 
 def update_after_insert(cache_snapshot, obj):
     '''
-    Update metadata on insertion (cache miss path).
-    - If the key is in ghost lists, adjust p (ARC adaptation) and insert into protected.
-    - Otherwise insert into probation as MRU, unless guarded by a strong last victim (insert at LRU).
-    - Maintain fallback timestamp map and seed frequency.
-    '''
-    _ensure_capacity(cache_snapshot)
+    On miss and insert:
+    - Update EWMA and age frequencies.
+    - If key in ghosts: momentum-adjust p; fresh ghosts re-admit to T2 (seed freq), stale to T1.
+    - Else: insert to T1; during guard/scan, place at T1 LRU; also gently lower p in scan to bias T1 evictions.
+    '''
+    _ensure_capacity(cache_snapshot)
+    _maybe_age(cache_snapshot)
+    _update_activity(False, cache_snapshot)
+
     key = obj.key
-    m_key_timestamp[key] = cache_snapshot.access_count
+    now = cache_snapshot.access_count
+    m_key_timestamp[key] = now
 
     in_b1 = key in _B1_ghost
     in_b2 = key in _B2_ghost
 
+    fresh_window = max(1, int(_FRESH_WINDOW_RATIO * _cap_est))
+
     if in_b1 or in_b2:
-        # ARC adaptation of p (smooth float-based steps)
-        global _p_target
+        # Compute step based on opposing ghost sizes
         if in_b1:
-            # Favor recency: increase p
-            inc = max(1.0, float(len(_B2_ghost)) / max(1.0, float(len(_B1_ghost))))
-            _p_target = min(float(_cap_est), _p_target + float(inc))
+            step = max(1.0, float(len(_B2_ghost)) / max(1.0, float(len(_B1_ghost))))
+            ev_ts = _B1_ghost.get(key, None)
+            age = (now - ev_ts) if isinstance(ev_ts, int) else (fresh_window + 1)
+            fresh = age <= fresh_window
+            _adjust_p(+1, step, now, freshness_scale=(1.2 if fresh else 1.0))
             _B1_ghost.pop(key, None)
+            if fresh:
+                # Admit to T2 as recently valuable
+                _T2_protected[key] = True
+                _freq[key] = max(3, min(_FREQ_MAX, _freq.get(key, 0) + 2))
+                _demote_protected_if_needed(cache_snapshot, avoid_key=key)
+            else:
+                _T1_probation[key] = True
+                _freq[key] = _freq.get(key, 0)
         else:
-            # Favor frequency: decrease p
-            dec = max(1.0, float(len(_B1_ghost)) / max(1.0, float(len(_B2_ghost))))
-            _p_target = max(0.0, _p_target - float(dec))
+            step = max(1.0, float(len(_B1_ghost)) / max(1.0, float(len(_B2_ghost))))
+            ev_ts = _B2_ghost.get(key, None)
+            age = (now - ev_ts) if isinstance(ev_ts, int) else (fresh_window + 1)
+            fresh = age <= fresh_window
+            _adjust_p(-1, step, now, freshness_scale=(1.2 if fresh else 1.0))
             _B2_ghost.pop(key, None)
-        # Insert into protected (seen before, effectively 2nd touch)
-        if key in _T1_probation:
-            _T1_probation.pop(key, None)
-        _T2_protected[key] = True
-        # Seed frequency as at least 2 for re-referenced keys
-        _freq[key] = max(_freq.get(key, 0) + 1, 2)
+            if fresh:
+                _T2_protected[key] = True
+                _freq[key] = max(3, min(_FREQ_MAX, _freq.get(key, 0) + 2))
+                _demote_protected_if_needed(cache_snapshot, avoid_key=key)
+            else:
+                _T1_probation[key] = True
+                _freq[key] = _freq.get(key, 0)
     else:
-        # New to cache and ghosts: insert into probation (T1)
-        if key in _T2_protected:
-            # Rare desync; ensure consistency (shouldn't happen on miss)
-            _T2_protected.move_to_end(key, last=True)
-        else:
-            _T1_probation[key] = True
-            # Admission guard: if last victim was strong, place newcomer at LRU so it's evicted quickly
-            if _last_victim_strength >= _VICTIM_GUARD_THRESH:
-                _T1_probation.move_to_end(key, last=False)
-        # Seed minimal frequency for new items
+        # New key: insert into T1
+        _T1_probation[key] = True
         _freq[key] = _freq.get(key, 0)
+        # Guard and scan handling: bias newcomer colder
+        if (_last_victim_strength >= _VICTIM_GUARD_THRESH) or (now <= _scan_until):
+            _T1_probation.move_to_end(key, last=False)
+            # During scan, gently lower p to keep pressure in T1
+            if now <= _scan_until:
+                _adjust_p(-1, max(1.0, 0.1 * float(_cap_est)), now)
 
     # Avoid duplicates across structures
     if key in _T1_probation and key in _T2_protected:
         _T1_probation.pop(key, None)
     if key in _B1_ghost:
         _B1_ghost.pop(key, None)
     if key in _B2_ghost:
         _B2_ghost.pop(key, None)
     _ghost_trim()
 
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     '''
-    Update metadata after eviction.
-    - Remove victim from its resident segment.
-    - Add to corresponding ghost list (B1 if from probation, B2 if from protected).
-    - Track last victim strength for admission guard.
-    - Trim ghost lists to capacity and clean timestamps/frequency.
-    '''
-    _ensure_capacity(cache_snapshot)
-    victim_key = evicted_obj.key
-
-    was_t1 = victim_key in _T1_probation
-    was_t2 = victim_key in _T2_protected
-
-    # Track strength of the evicted item before removing counters
-    base_strength = float(_freq.get(victim_key, 0))
-    if was_t2:
-        base_strength += 2.0  # extra credit for protected residency
-    global _last_victim_strength
-    _last_victim_strength = base_strength
-
-    # Remove from resident segments and add to ghosts
+    After eviction:
+    - Remove from resident segment and put into the appropriate ghost with timestamp.
+    - Track victim strength and set a short admission guard when a strong T2 victim is evicted.
+    - Clean frequency and timestamp entries.
+    '''
+    _ensure_capacity(cache_snapshot)
+    key = evicted_obj.key
+    now = cache_snapshot.access_count
+
+    was_t1 = key in _T1_probation
+    was_t2 = key in _T2_protected
+
+    fval = _freq.get(key, 0)
+    strength = float(fval) + (2.0 if was_t2 else 0.0)
+    global _last_victim_strength, _guard_until
+    _last_victim_strength = strength
+
     if was_t1:
-        _T1_probation.pop(victim_key, None)
-        _B1_ghost[victim_key] = True  # insert as MRU
+        _T1_probation.pop(key, None)
+        _B1_ghost[key] = now
     elif was_t2:
-        _T2_protected.pop(victim_key, None)
-        _B2_ghost[victim_key] = True  # insert as MRU
+        _T2_protected.pop(key, None)
+        _B2_ghost[key] = now
+        if fval >= 2:
+            _guard_until = now + max(1, _cap_est // 2)
     else:
-        # Unknown location; put in B1 by default
-        _B1_ghost[victim_key] = True
-
-    # Remove fallback timestamp and frequency for evicted key
-    if victim_key in m_key_timestamp:
-        m_key_timestamp.pop(victim_key, None)
-    if victim_key in _freq:
-        _freq.pop(victim_key, None)
-
-    # If inserting obj is accidentally in ghosts, let insert handle cleanup next
+        # Unknown residency; default to B1 ghost
+        _B1_ghost[key] = now
+
+    m_key_timestamp.pop(key, None)
+    _freq.pop(key, None)
     _ghost_trim()
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate