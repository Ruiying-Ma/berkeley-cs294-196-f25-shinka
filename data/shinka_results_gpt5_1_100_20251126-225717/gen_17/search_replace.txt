<NAME>
arc_srlu_tiny_guard_sampling
</NAME>

<DESCRIPTION>
I enhance the existing ARC-like SLRU policy by adding three synergistic improvements:

1) Tiny admission guard using last victim strength: After evicting a strong (frequently used) item, the next brand‑new object is inserted at the LRU end of probation so it will be evicted quickly unless it proves itself. This reduces pollution from one-timers without bypassing insertion.

2) Lightweight per-key frequency counters: We maintain a simple hit counter per key (no global decay). On eviction, instead of selecting the strict LRU in a segment, we sample a few oldest candidates (2 from probation, 3 from protected) and pick the one with the lowest frequency, tiebroken by age. This balances recency and frequency with minimal overhead.

3) Closer adherence to ARC’s REPLACE(x) rule: The eviction segment (T1 vs T2) now depends on whether the incoming key is in B2 and the current size of T1 relative to p. This improves adaptation between recency and frequency across workloads.

Additionally, I make p-adaptation steps float-based and smoother (ratio instead of integer floor) to avoid overshooting. All changes are localized to the allowed functions and supporting globals. This should reduce miss rates especially under mixed and scan-heavy workloads while preserving strong performance on skewed traces.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Adaptive ARC-like cache eviction with SLRU segments and ghost history"""

from collections import OrderedDict

# Segment structures
_T1_probation = OrderedDict()   # keys in cache, 1st-touch (recency-biased)
_T2_protected = OrderedDict()   # keys in cache, 2nd+ touch (frequency-biased)

# Ghost history (recently evicted keys) for adaptation
_B1_ghost = OrderedDict()       # evicted from T1
_B2_ghost = OrderedDict()       # evicted from T2

# Adaptive target size for probation (ARC's p). Float to allow smooth adjust.
_p_target = 0.0

# Estimated capacity (number of objects). Initialize lazily.
_cap_est = 0

# Fallback LRU timestamps if metadata desync occurs
m_key_timestamp = dict()

# Tunable parameters
_P_INIT_RATIO = 0.3  # initial share for probation (T1)
# On ghost hits, adjust p by this delta rule (ARC-like):
# delta = max(1, len(other_ghost) // max(1, len(this_ghost)))
=======
"""Adaptive ARC-like cache eviction with SLRU segments and ghost history"""

from collections import OrderedDict

# Segment structures
_T1_probation = OrderedDict()   # keys in cache, 1st-touch (recency-biased)
_T2_protected = OrderedDict()   # keys in cache, 2nd+ touch (frequency-biased)

# Ghost history (recently evicted keys) for adaptation
_B1_ghost = OrderedDict()       # evicted from T1
_B2_ghost = OrderedDict()       # evicted from T2

# Adaptive target size for probation (ARC's p). Float to allow smooth adjust.
_p_target = 0.0

# Estimated capacity (number of objects). Initialize lazily.
_cap_est = 0

# Fallback LRU timestamps if metadata desync occurs
m_key_timestamp = dict()

# Lightweight per-key frequency counter (hit count)
_freq = dict()  # key -> int

# Admission guard based on last victim "strength"
_last_victim_strength = 0.0
_VICTIM_GUARD_THRESH = 2.0  # if last victim was strong, down-seed next newcomer

# Eviction sampling (number of LRU candidates to compare by frequency)
_T1_SAMPLE = 2
_T2_SAMPLE = 3

# Tunable parameters
_P_INIT_RATIO = 0.3  # initial share for probation (T1)
# On ghost hits, adjust p by this delta rule (ARC-like):
# delta = ratio of other ghost to this ghost (float), clamped and then p is clamped to [0, cap]
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def evict(cache_snapshot, obj):
    '''
    Choose victim key using adaptive ARC-like policy.
    Prefer evicting from probation (T1) when it exceeds target p;
    otherwise evict from protected (T2). Fall back to global LRU.
    '''
    _ensure_capacity(cache_snapshot)

    # If metadata perfectly mirrors cache, choose from segments.
    t1_size = len(_T1_probation)
    t2_size = len(_T2_protected)

    # Primary ARC eviction choice
    # Evict from T1 if it is "too big" relative to target p
    # Otherwise evict from T2 (if non-empty), else from T1, else fallback.
    victim_key = None
    t1_over = t1_size > max(1, int(_p_target))
    if t1_size > 0 and (t1_over or t2_size == 0):
        # LRU from T1
        victim_key = next(iter(_T1_probation.keys()))
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None and t2_size > 0:
        # LRU from T2
        victim_key = next(iter(_T2_protected.keys()))
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None and t1_size > 0:
        victim_key = next(iter(_T1_probation.keys()))
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None:
        # Fallback to global LRU if metadata desync
        victim_key = _fallback_choose(cache_snapshot)
    return victim_key
=======
def evict(cache_snapshot, obj):
    '''
    Choose victim key using adaptive ARC-like policy with small frequency-aware sampling.
    REPLACE(x): if |T1|>=1 and ((x in B2 and |T1| == p) or |T1| > p) -> evict from T1 else from T2.
    Within the chosen segment, pick the lowest-frequency among a few LRU candidates.
    '''
    _ensure_capacity(cache_snapshot)

    t1_size = len(_T1_probation)
    t2_size = len(_T2_protected)
    x_in_b2 = (obj is not None) and (obj.key in _B2_ghost)
    p_int = int(round(_p_target))
    choose_t1 = (t1_size >= 1) and ((x_in_b2 and t1_size == p_int) or (t1_size > _p_target))

    def _pick_from(od, sample_n):
        # Sample the first few LRU keys and pick by (freq asc, timestamp asc)
        if not od:
            return None
        it = iter(od.keys())
        candidates = []
        for _ in range(min(sample_n, len(od))):
            try:
                k = next(it)
            except StopIteration:
                break
            candidates.append(k)
        if not candidates:
            return None
        def score(k):
            # Lower freq better; older (smaller timestamp) better
            return (_freq.get(k, 0), m_key_timestamp.get(k, 0))
        return min(candidates, key=score)

    victim_key = None
    if choose_t1 and t1_size > 0:
        victim_key = _pick_from(_T1_probation, _T1_SAMPLE)
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None and t2_size > 0:
        victim_key = _pick_from(_T2_protected, _T2_SAMPLE)
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None and t1_size > 0:
        victim_key = _pick_from(_T1_probation, _T1_SAMPLE)
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None:
        # Fallback to global LRU if metadata desync
        victim_key = _fallback_choose(cache_snapshot)
    return victim_key
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_hit(cache_snapshot, obj):
    '''
    Update metadata after cache hit.
    - If hit in probation (T1), promote to protected (T2).
    - If hit in protected, refresh recency.
    - Maintain fallback timestamp map.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    # Update fallback LRU timestamp
    m_key_timestamp[key] = cache_snapshot.access_count

    # If the key exists in our segments, update positions
    if key in _T2_protected:
        # Refresh to MRU
        _T2_protected.move_to_end(key, last=True)
    elif key in _T1_probation:
        # Promote from probation to protected
        _T1_probation.pop(key, None)
        _T2_protected[key] = True  # insert as MRU
    else:
        # Metadata miss: cache has it but we don't; treat as frequent and add to protected
        _T2_protected[key] = True
    # Touch ghosts cleanup if any stale
    if key in _B1_ghost:
        _B1_ghost.pop(key, None)
    if key in _B2_ghost:
        _B2_ghost.pop(key, None)
    _ghost_trim()
=======
def update_after_hit(cache_snapshot, obj):
    '''
    Update metadata after cache hit.
    - If hit in probation (T1), promote to protected (T2).
    - If hit in protected, refresh recency.
    - Maintain fallback timestamp map and per-key frequency.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    # Update fallback LRU timestamp
    m_key_timestamp[key] = cache_snapshot.access_count
    # Increment frequency counter
    _freq[key] = _freq.get(key, 0) + 1

    # If the key exists in our segments, update positions
    if key in _T2_protected:
        # Refresh to MRU
        _T2_protected.move_to_end(key, last=True)
    elif key in _T1_probation:
        # Promote from probation to protected
        _T1_probation.pop(key, None)
        _T2_protected[key] = True  # insert as MRU
    else:
        # Metadata miss: cache has it but we don't; treat as frequent and add to protected
        _T2_protected[key] = True
    # Touch ghosts cleanup if any stale
    if key in _B1_ghost:
        _B1_ghost.pop(key, None)
    if key in _B2_ghost:
        _B2_ghost.pop(key, None)
    _ghost_trim()
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_insert(cache_snapshot, obj):
    '''
    Update metadata on insertion (cache miss path).
    - If the key is in ghost lists, adjust p (ARC adaptation) and insert into protected.
    - Otherwise insert into probation as MRU.
    - Maintain fallback timestamp map.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    m_key_timestamp[key] = cache_snapshot.access_count

    in_b1 = key in _B1_ghost
    in_b2 = key in _B2_ghost

    if in_b1 or in_b2:
        # ARC adaptation of p
        global _p_target
        if in_b1:
            # Favor recency: increase p
            inc = max(1, len(_B2_ghost) // max(1, len(_B1_ghost)))
            _p_target = min(float(_cap_est), _p_target + float(inc))
            _B1_ghost.pop(key, None)
        else:
            # Favor frequency: decrease p
            dec = max(1, len(_B1_ghost) // max(1, len(_B2_ghost)))
            _p_target = max(0.0, _p_target - float(dec))
            _B2_ghost.pop(key, None)
        # Insert into protected (seen before, effectively 2nd touch)
        if key in _T1_probation:
            _T1_probation.pop(key, None)
        _T2_protected[key] = True
    else:
        # New to cache and ghosts: insert into probation (T1)
        if key in _T2_protected:
            # Rare desync; ensure consistency (shouldn't happen on miss)
            _T2_protected.move_to_end(key, last=True)
        else:
            _T1_probation[key] = True

    # Avoid duplicates across structures
    if key in _T1_probation and key in _T2_protected:
        _T1_probation.pop(key, None)
    if key in _B1_ghost:
        _B1_ghost.pop(key, None)
    if key in _B2_ghost:
        _B2_ghost.pop(key, None)
    _ghost_trim()
=======
def update_after_insert(cache_snapshot, obj):
    '''
    Update metadata on insertion (cache miss path).
    - If the key is in ghost lists, adjust p (ARC adaptation) and insert into protected.
    - Otherwise insert into probation as MRU, unless guarded by a strong last victim (insert at LRU).
    - Maintain fallback timestamp map and seed frequency.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    m_key_timestamp[key] = cache_snapshot.access_count

    in_b1 = key in _B1_ghost
    in_b2 = key in _B2_ghost

    if in_b1 or in_b2:
        # ARC adaptation of p (smooth float-based steps)
        global _p_target
        if in_b1:
            # Favor recency: increase p
            inc = max(1.0, float(len(_B2_ghost)) / max(1.0, float(len(_B1_ghost))))
            _p_target = min(float(_cap_est), _p_target + float(inc))
            _B1_ghost.pop(key, None)
        else:
            # Favor frequency: decrease p
            dec = max(1.0, float(len(_B1_ghost)) / max(1.0, float(len(_B2_ghost))))
            _p_target = max(0.0, _p_target - float(dec))
            _B2_ghost.pop(key, None)
        # Insert into protected (seen before, effectively 2nd touch)
        if key in _T1_probation:
            _T1_probation.pop(key, None)
        _T2_protected[key] = True
        # Seed frequency as at least 2 for re-referenced keys
        _freq[key] = max(_freq.get(key, 0) + 1, 2)
    else:
        # New to cache and ghosts: insert into probation (T1)
        if key in _T2_protected:
            # Rare desync; ensure consistency (shouldn't happen on miss)
            _T2_protected.move_to_end(key, last=True)
        else:
            _T1_probation[key] = True
            # Admission guard: if last victim was strong, place newcomer at LRU so it's evicted quickly
            if _last_victim_strength >= _VICTIM_GUARD_THRESH:
                _T1_probation.move_to_end(key, last=False)
        # Seed minimal frequency for new items
        _freq[key] = _freq.get(key, 0)

    # Avoid duplicates across structures
    if key in _T1_probation and key in _T2_protected:
        _T1_probation.pop(key, None)
    if key in _B1_ghost:
        _B1_ghost.pop(key, None)
    if key in _B2_ghost:
        _B2_ghost.pop(key, None)
    _ghost_trim()
>>>>>>> REPLACE

</DIFF>
<DIFF>
<<<<<<< SEARCH
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Update metadata after eviction.
    - Remove victim from its resident segment.
    - Add to corresponding ghost list (B1 if from probation, B2 if from protected).
    - Trim ghost lists to capacity.
    - Maintain fallback timestamp map.
    '''
    _ensure_capacity(cache_snapshot)
    victim_key = evicted_obj.key

    was_t1 = victim_key in _T1_probation
    was_t2 = victim_key in _T2_protected

    # Remove from resident segments
    if was_t1:
        _T1_probation.pop(victim_key, None)
        _B1_ghost[victim_key] = True  # insert as MRU
    elif was_t2:
        _T2_protected.pop(victim_key, None)
        _B2_ghost[victim_key] = True  # insert as MRU
    else:
        # Unknown location; put in B1 by default
        _B1_ghost[victim_key] = True

    # Remove fallback timestamp for evicted key
    if victim_key in m_key_timestamp:
        m_key_timestamp.pop(victim_key, None)

    # If inserting obj is accidentally in ghosts, let insert handle cleanup next
    _ghost_trim()
=======
def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Update metadata after eviction.
    - Remove victim from its resident segment.
    - Add to corresponding ghost list (B1 if from probation, B2 if from protected).
    - Track last victim strength for admission guard.
    - Trim ghost lists to capacity and clean timestamps/frequency.
    '''
    _ensure_capacity(cache_snapshot)
    victim_key = evicted_obj.key

    was_t1 = victim_key in _T1_probation
    was_t2 = victim_key in _T2_protected

    # Track strength of the evicted item before removing counters
    base_strength = float(_freq.get(victim_key, 0))
    if was_t2:
        base_strength += 2.0  # extra credit for protected residency
    global _last_victim_strength
    _last_victim_strength = base_strength

    # Remove from resident segments and add to ghosts
    if was_t1:
        _T1_probation.pop(victim_key, None)
        _B1_ghost[victim_key] = True  # insert as MRU
    elif was_t2:
        _T2_protected.pop(victim_key, None)
        _B2_ghost[victim_key] = True  # insert as MRU
    else:
        # Unknown location; put in B1 by default
        _B1_ghost[victim_key] = True

    # Remove fallback timestamp and frequency for evicted key
    if victim_key in m_key_timestamp:
        m_key_timestamp.pop(victim_key, None)
    if victim_key in _freq:
        _freq.pop(victim_key, None)

    # If inserting obj is accidentally in ghosts, let insert handle cleanup next
    _ghost_trim()
>>>>>>> REPLACE

</DIFF>