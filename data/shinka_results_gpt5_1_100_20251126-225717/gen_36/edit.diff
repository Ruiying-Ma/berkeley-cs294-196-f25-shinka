--- a/original.py
+++ b/original.py
@@ -1,296 +1,445 @@
 # EVOLVE-BLOCK-START
-"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""
+"""ARC+SLRU with TinyLFU aging, scan detection and momentum-adapted p"""
 
 from collections import OrderedDict
 
-# Legacy timestamp dictionary kept for compatibility; used as a general access ledger.
+# Resident segments: store last access timestamp for LRU
+_T1_probation = OrderedDict()   # key -> last_ts
+_T2_protected = OrderedDict()   # key -> last_ts
+
+# Ghost histories store eviction timestamp for freshness-aware adaptation
+_B1_ghost = OrderedDict()       # key -> evict_ts (from T1)
+_B2_ghost = OrderedDict()       # key -> evict_ts (from T2)
+
+# Adaptive target size for probation (ARC's p) with momentum
+_p_target = 0.0
+_p_mom = 0.0
+
+# Capacity estimate
+_cap_est = 0
+
+# Global timestamp ledger (for fallback and tie-breakers)
 m_key_timestamp = dict()
 
-# Segmented LRU metadata: probation and protected segments (key -> last access time)
-_probation = dict()
-_protected = dict()
-
-# ARC-style ghost histories (recently evicted):
-_B1_ghost = OrderedDict()  # evicted from probation
-_B2_ghost = OrderedDict()  # evicted from protected
-
-# Adaptive target for probation share (ARC's p) and capacity estimate
-_p_target = 0.0
-_cap_est = 0
-
-# Lightweight per-key frequency with periodic aging
-_freq = dict()           # key -> small integer frequency
-_last_age_tick = 0       # last access_count when we aged
+# TinyLFU-like lightweight frequency with periodic aging (3-bit saturating)
+_freq = dict()        # key -> [0..7]
+_last_age_tick = 0
+
+# Scan detection window
+_W = 0
+_win_total = 0
+_win_hits = 0
+_win_inserts = 0
+_scan_mode = False
+_scan_until = 0
+_scan_last_decay_tick = 0
+
+# Promotion tracking window (for dynamic sampling of T2)
+_prom_count = 0
+_prom_window_start = 0
+_prom_rate = 0.0
+
+# Two-touch gating during scan
+_t1_touch_count = dict()
 
 # Short-lived guard to resist scans after evicting strong protected entries
-_guard_until = 0         # access_count until which we bias new insertions as cold
+_guard_until = 0
+
+# Tunables
+_P_INIT_RATIO = 0.30
+_MOM_DECAY = 0.5
+_MAX_P_STEP_FRAC = 0.25
+_FRESH_GHOST_AGE_FRAC = 0.5
+_FRESH_W = 1.5
+_STALE_W = 1.0
+_SCAN_INS_THRESH = 0.7
+_SCAN_HIT_THRESH = 0.15
+
 
 def _ensure_capacity(cache_snapshot):
-    """Initialize/refresh capacity estimate and clamp p within [0, cap]."""
-    global _cap_est, _p_target
+    """Init capacity, p and window sizes; clamp p to [0, cap]."""
+    global _cap_est, _p_target, _W, _prom_window_start
     cap = getattr(cache_snapshot, "capacity", None)
     if isinstance(cap, int) and cap > 0:
         _cap_est = cap
     if _cap_est <= 0:
         _cap_est = max(1, len(cache_snapshot.cache))
-    # Initialize p only once when metadata is empty
-    if _p_target == 0.0 and not _probation and not _protected and not _B1_ghost and not _B2_ghost:
-        # Start with modest probation share
-        _p_target = min(float(_cap_est), max(0.0, float(_cap_est) * 0.33))
-    # Clamp p
+    if _p_target == 0.0 and not _T1_probation and not _T2_protected and not _B1_ghost and not _B2_ghost:
+        _p_target = min(float(_cap_est), max(0.0, float(_cap_est) * _P_INIT_RATIO))
+        _W = max(50, _cap_est)
+        _prom_window_start = getattr(cache_snapshot, "access_count", 0)
     if _p_target < 0.0:
         _p_target = 0.0
     if _p_target > float(_cap_est):
         _p_target = float(_cap_est)
 
+
 def _ghost_trim():
-    """Bound ghost lists by capacity."""
+    """Bound each ghost list to at most cap elements."""
     while len(_B1_ghost) > _cap_est:
         _B1_ghost.popitem(last=False)
     while len(_B2_ghost) > _cap_est:
         _B2_ghost.popitem(last=False)
 
-def _get_caps(cache_snapshot):
-    """Compute static target sizes for probation and protected segments (legacy helper)."""
-    total_cap = max(int(getattr(cache_snapshot, "capacity", 1)), 1)
-    # Favor protected segment to keep repeatedly used items
-    prot_cap = max(int(total_cap * 0.66), 1 if total_cap > 1 else 0)
-    prob_cap = max(total_cap - prot_cap, 1)
-    return prob_cap, prot_cap
-
-def _get_targets(cache_snapshot):
-    """Compute ARC targets from p: T1 target = round(p), T2 target = cap - T1 target."""
-    _ensure_capacity(cache_snapshot)
-    t1_target = int(round(_p_target))
-    t2_target = max(_cap_est - t1_target, 0)
-    return t1_target, t2_target
-
-def _lru_key_in(seg_dict, cache_snapshot):
-    """Return the LRU key from seg_dict that is currently in the cache."""
-    min_key = None
-    min_ts = None
-    # Iterate only over keys that are in the current cache snapshot
-    cache_keys = cache_snapshot.cache.keys()
-    for k, ts in seg_dict.items():
-        if k in cache_keys:
-            if (min_ts is None) or (ts < min_ts):
-                min_ts = ts
-                min_key = k
-    return min_key
-
-def _min_by_freq_ts(seg_dict, cache_snapshot):
-    """Pick a victim by (frequency asc, timestamp asc) among keys present in cache."""
-    best_k = None
-    best_score = None
-    cache_keys = cache_snapshot.cache.keys()
-    for k, ts in seg_dict.items():
-        if k in cache_keys:
-            f = _freq.get(k, 0)
-            score = (f, ts)
-            if best_score is None or score < best_score:
-                best_score = score
-                best_k = k
-    return best_k
 
 def _maybe_age(cache_snapshot):
-    """Periodically age frequencies to avoid stale bias."""
+    """Halve frequencies every capacity accesses."""
     global _last_age_tick
     _ensure_capacity(cache_snapshot)
     now = cache_snapshot.access_count
     if now - _last_age_tick >= max(1, _cap_est):
-        # Halve all frequencies (simple exponential decay)
         for k in list(_freq.keys()):
             newf = _freq.get(k, 0) // 2
             if newf <= 0:
                 _freq.pop(k, None)
             else:
                 _freq[k] = newf
         _last_age_tick = now
 
+
+def _bump_freq(k):
+    """Increment saturating 3-bit counter [0..7]."""
+    _freq[k] = min(_freq.get(k, 0) + 1, 7)
+
+
+def _lru_key_in(seg_dict, cache_snapshot):
+    """Return LRU key by smallest timestamp among keys in cache."""
+    min_key, min_ts = None, None
+    ckeys = cache_snapshot.cache.keys()
+    for k, ts in seg_dict.items():
+        if k in ckeys:
+            if min_ts is None or ts < min_ts:
+                min_key, min_ts = k, ts
+    return min_key
+
+
+def _pick_from(seg_dict, cache_snapshot, sample_n):
+    """Sample first sample_n LRU candidates and pick by (freq asc, ts asc)."""
+    if not seg_dict:
+        return None
+    ckeys = cache_snapshot.cache.keys()
+    picked = []
+    for k in seg_dict.keys():
+        if k in ckeys:
+            picked.append(k)
+            if len(picked) >= sample_n:
+                break
+    if not picked:
+        return None
+    def score(k):
+        return (_freq.get(k, 0), seg_dict.get(k, m_key_timestamp.get(k, 0)))
+    return min(picked, key=score)
+
+
+def _get_targets(cache_snapshot):
+    """From p, compute target sizes for T1 and T2 in objects."""
+    _ensure_capacity(cache_snapshot)
+    t1_target = int(round(_p_target))
+    t2_target = max(_cap_est - t1_target, 0)
+    return t1_target, t2_target
+
+
+def _enforce_t2_target(cache_snapshot, avoid_key=None):
+    """Demote T2 LRU -> T1 until T2 size <= target."""
+    _, t2_target = _get_targets(cache_snapshot)
+    while len(_T2_protected) > t2_target:
+        demote_key = _lru_key_in(_T2_protected, cache_snapshot)
+        if demote_key is None:
+            break
+        if avoid_key is not None and demote_key == avoid_key:
+            # Make avoid_key newest to expose next LRU
+            _T2_protected[avoid_key] = cache_snapshot.access_count
+            demote_key = _lru_key_in(_T2_protected, cache_snapshot)
+            if demote_key is None or demote_key == avoid_key:
+                break
+        demote_ts = _T2_protected.pop(demote_key, None)
+        if demote_ts is None:
+            demote_ts = cache_snapshot.access_count
+        _T1_probation[demote_key] = demote_ts
+        _t1_touch_count.pop(demote_key, None)
+
+
+def _update_windows_on_access(cache_snapshot, is_hit=None, is_insert=None):
+    """Update sliding windows for scan detection and promotion-rate tracking."""
+    global _win_total, _win_hits, _win_inserts, _scan_mode, _scan_until, _scan_last_decay_tick
+    global _prom_window_start, _prom_rate, _p_target
+
+    now = cache_snapshot.access_count
+    _ensure_capacity(cache_snapshot)
+
+    # Update scan window counters
+    _win_total += 1
+    if is_hit:
+        _win_hits += 1
+    if is_insert:
+        _win_inserts += 1
+
+    # Periodically evaluate scan mode every W accesses
+    if _W <= 0:
+        w = max(50, _cap_est)
+    else:
+        w = _W
+
+    if _win_total >= w:
+        hit_rate = _win_hits / max(1, _win_total)
+        insert_rate = _win_inserts / max(1, _win_total)
+        if (insert_rate > _SCAN_INS_THRESH and hit_rate < _SCAN_HIT_THRESH):
+            _scan_mode = True
+            _scan_until = now + w
+            _scan_last_decay_tick = now
+        # reset window
+        _win_total = _win_hits = _win_inserts = 0
+
+    # Auto-exit scan mode after its time window
+    if _scan_mode and now >= _scan_until:
+        _scan_mode = False
+
+    # While in scan mode, gently lower p periodically
+    if _scan_mode and (now - _scan_last_decay_tick >= 50):
+        step = 1.5 * max(1.0, float(len(_B1_ghost)) / max(1.0, float(len(_B2_ghost))))
+        _p_target = max(0.0, _p_target - step)
+        _scan_last_decay_tick = now
+
+    # Update promotion-rate window
+    wp = max(1, _cap_est // 2)
+    if now - _prom_window_start >= wp:
+        # use rate observed so far
+        _prom_rate = float(_prom_count) / float(now - _prom_window_start)
+        _prom_window_start = now
+        # decay promotions rather than hard reset to preserve momentum
+        # keep at most one carried over promotion
+        carried = 1 if _prom_count > 0 else 0
+        # reset with carryover to smooth
+        globals()['_prom_count'] = carried
+
+
+def _adjust_p_with_momentum(cache_snapshot, direction, freshness_weight):
+    """Apply ARC-like p update with momentum, clamped to capacity."""
+    global _p_target, _p_mom
+    _ensure_capacity(cache_snapshot)
+    # step based on ghost imbalance
+    if direction > 0:
+        # increase p (favor recency)
+        base = float(len(_B2_ghost)) / max(1.0, float(len(_B1_ghost)))
+    else:
+        # decrease p (favor frequency)
+        base = float(len(_B1_ghost)) / max(1.0, float(len(_B2_ghost)))
+    base = max(1.0, base)
+    max_step = _MAX_P_STEP_FRAC * float(_cap_est)
+    step = min(base * freshness_weight, max_step)
+    _p_mom = _MOM_DECAY * _p_mom + direction * step
+    _p_target = max(0.0, min(float(_cap_est), _p_target + _p_mom))
+
+
 def evict(cache_snapshot, obj):
     '''
-    This function defines how the algorithm chooses the eviction victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The new object that needs to be inserted into the cache.
-    - Return:
-        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
-    '''
-    _ensure_capacity(cache_snapshot)
-
-    # ARC REPLACE(x): if |T1|>=1 and ((x in B2 and |T1| == p) or |T1| > p) -> evict from T1 else from T2
-    t1_size = len(_probation)
+    Choose victim per ARC REPLACE(x), then pick within segment by (freq asc, ts asc)
+    using a small number of LRU candidates (sampling).
+    '''
+    _ensure_capacity(cache_snapshot)
+
+    t1_size = len(_T1_probation)
     x_in_b2 = (obj is not None) and (obj.key in _B2_ghost)
     p_int = int(round(_p_target))
     choose_t1 = (t1_size >= 1) and ((x_in_b2 and t1_size == p_int) or (t1_size > _p_target))
 
-    # Frequency-aware candidate selection within the chosen segment
-    candid_obj_key = None
+    # Dynamic sampling sizes
+    # T1 sampling smaller in scan or when T1 is over pressure
+    t1_over = t1_size > (_p_target + 0.1 * _cap_est)
+    T1_SAMPLE = 1 if (_scan_mode or t1_over) else 2
+
+    # T2 sampling depends on crowdedness and recent promotion/hit rates
+    _, t2_target = _get_targets(cache_snapshot)
+    t2_crowded = len(_T2_protected) > max(0, t2_target)
+    # recent hit rate approx from window
+    recent_hit_rate = (_win_hits / max(1, _win_total)) if _win_total > 0 else 0.5
+    if t2_crowded and _prom_rate > 0.2:
+        T2_SAMPLE = 5
+    elif recent_hit_rate < 0.2:
+        T2_SAMPLE = 2
+    else:
+        T2_SAMPLE = 3
+
+    victim_key = None
     if choose_t1:
-        candid_obj_key = _min_by_freq_ts(_probation, cache_snapshot)
-    if candid_obj_key is None:
-        candid_obj_key = _min_by_freq_ts(_protected, cache_snapshot)
-    if candid_obj_key is None:
-        # Last-resort fallback: pick any key from the cache (should rarely happen).
-        for k in cache_snapshot.cache:
-            candid_obj_key = k
-            break
-    return candid_obj_key
+        victim_key = _pick_from(_T1_probation, cache_snapshot, T1_SAMPLE)
+    if victim_key is None:
+        victim_key = _pick_from(_T2_protected, cache_snapshot, T2_SAMPLE)
+    if victim_key is None:
+        victim_key = _pick_from(_T1_probation, cache_snapshot, max(1, T1_SAMPLE))
+    if victim_key is None:
+        # Fallback: global LRU
+        victim_key = _lru_key_in(m_key_timestamp, cache_snapshot) or next(iter(cache_snapshot.cache.keys()), None)
+    return victim_key
+
 
 def update_after_hit(cache_snapshot, obj):
     '''
-    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object accessed during the cache hit.
-    - Return: `None`
-    '''
-    global m_key_timestamp, _probation, _protected, _freq
+    On hit: bump frequency, update timestamp.
+    - If in T1: promote to T2 unless in scan gating (require 2 touches), else refresh in T2.
+    - Demote T2 to enforce target.
+    - Clear ghosts for this key.
+    '''
+    global _prom_count
     _ensure_capacity(cache_snapshot)
     _maybe_age(cache_snapshot)
-    current_ts = cache_snapshot.access_count
+
+    now = cache_snapshot.access_count
     k = obj.key
 
-    # Maintain a general timestamp ledger for robustness.
-    m_key_timestamp[k] = current_ts
-
-    # Increment lightweight frequency counter
-    _freq[k] = _freq.get(k, 0) + 1
-
-    if k in _probation:
-        # Promote to protected on second touch.
-        _probation.pop(k, None)
-        _protected[k] = current_ts
-    elif k in _protected:
-        # Refresh recency within protected.
-        _protected[k] = current_ts
-    else:
-        # Metadata miss: treat as a re-reference and place in protected.
-        _protected[k] = current_ts
-
-    # Respect ARC target for protected by demoting its LRU if needed.
-    _, t2_target = _get_targets(cache_snapshot)
-    if len(_protected) > t2_target:
-        demote_key = _lru_key_in(_protected, cache_snapshot)
-        if demote_key is not None and demote_key != k:
-            demote_ts = _protected.pop(demote_key)
-            _probation[demote_key] = demote_ts
-
-    # If present in ghosts, clear and bound ghost history
+    # Record windows
+    _update_windows_on_access(cache_snapshot, is_hit=True, is_insert=False)
+
+    # Timestamp + frequency
+    m_key_timestamp[k] = now
+    _bump_freq(k)
+
+    if k in _T2_protected:
+        _T2_protected[k] = now
+    elif k in _T1_probation:
+        # Scan gating: require two touches before promotion
+        if _scan_mode:
+            cnt = _t1_touch_count.get(k, 0) + 1
+            _t1_touch_count[k] = cnt
+            if cnt >= 2:
+                _T1_probation.pop(k, None)
+                _T2_protected[k] = now
+                _t1_touch_count.pop(k, None)
+                _prom_count += 1
+            else:
+                # refresh recency in T1
+                _T1_probation[k] = now
+        else:
+            _T1_probation.pop(k, None)
+            _T2_protected[k] = now
+            _t1_touch_count.pop(k, None)
+            _prom_count += 1
+    else:
+        # Metadata miss: treat as protected
+        _T2_protected[k] = now
+
+    _enforce_t2_target(cache_snapshot, avoid_key=k)
+
+    # Ghost cleanup
     if k in _B1_ghost:
         _B1_ghost.pop(k, None)
     if k in _B2_ghost:
         _B2_ghost.pop(k, None)
     _ghost_trim()
 
+
 def update_after_insert(cache_snapshot, obj):
     '''
-    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object that was just inserted into the cache.
-    - Return: `None`
-    '''
-    global m_key_timestamp, _probation, _protected, _p_target, _freq, _guard_until
+    On miss/insert:
+    - If in ghosts: adapt p with momentum; fresh ghosts -> protected, stale -> probation.
+    - Otherwise: insert into T1; if guard or scan mode -> insert cold (older ts).
+    - Demote T2 to enforce target.
+    '''
     _ensure_capacity(cache_snapshot)
     _maybe_age(cache_snapshot)
-    current_ts = cache_snapshot.access_count
+
+    now = cache_snapshot.access_count
     k = obj.key
 
-    # Record in general ledger
-    m_key_timestamp[k] = current_ts
+    # Record windows
+    _update_windows_on_access(cache_snapshot, is_hit=False, is_insert=True)
+
+    m_key_timestamp[k] = now
+    # Minimal seed frequency for new items remains whatever is in map (default 0)
 
     in_b1 = k in _B1_ghost
     in_b2 = k in _B2_ghost
 
     if in_b1 or in_b2:
-        # ARC adaptation of p and re-admit into protected
+        # Freshness-aware and momentum ARC p update
         if in_b1:
-            inc = max(1.0, float(len(_B2_ghost)) / max(1.0, float(len(_B1_ghost))))
-            _p_target = min(float(_cap_est), _p_target + inc)
+            age = now - (_B1_ghost.get(k) or now)
+            fresh = age <= int(_FRESH_GHOST_AGE_FRAC * max(1, _cap_est))
+            _adjust_p_with_momentum(cache_snapshot, direction=+1, freshness_weight=(_FRESH_W if fresh else _STALE_W))
             _B1_ghost.pop(k, None)
+            if fresh:
+                _T2_protected[k] = now
+                _freq[k] = max(_freq.get(k, 0), 2)
+            else:
+                _T1_probation[k] = now
+                _t1_touch_count[k] = 0
         else:
-            dec = max(1.0, float(len(_B1_ghost)) / max(1.0, float(len(_B2_ghost))))
-            _p_target = max(0.0, _p_target - dec)
+            age = now - (_B2_ghost.get(k) or now)
+            fresh = age <= int(_FRESH_GHOST_AGE_FRAC * max(1, _cap_est))
+            _adjust_p_with_momentum(cache_snapshot, direction=-1, freshness_weight=(_FRESH_W if fresh else _STALE_W))
             _B2_ghost.pop(k, None)
-        _protected[k] = current_ts
-        # Seed frequency as reused
-        _freq[k] = max(_freq.get(k, 0) + 1, 2)
-
-        # Keep protected within its target by demoting its LRU if necessary
-        _, t2_target = _get_targets(cache_snapshot)
-        if len(_protected) > t2_target:
-            demote_key = _lru_key_in(_protected, cache_snapshot)
-            if demote_key is not None and demote_key != k:
-                demote_ts = _protected.pop(demote_key)
-                _probation[demote_key] = demote_ts
-    else:
-        # New to cache and ghosts: insert into probation
-        # If we recently had to evict a strong protected item, bias this newcomer as cold.
-        if current_ts <= _guard_until:
-            _probation[k] = current_ts - max(1, _cap_est)  # artificially older to prefer early eviction
+            if fresh:
+                _T2_protected[k] = now
+                _freq[k] = max(_freq.get(k, 0), 3)
+            else:
+                _T1_probation[k] = now
+                _t1_touch_count[k] = 0
+
+        _enforce_t2_target(cache_snapshot, avoid_key=k)
+    else:
+        # New key admission into T1
+        if now <= _guard_until or _scan_mode:
+            # Insert cold by giving an artificially old timestamp
+            _T1_probation[k] = now - max(1, _cap_est)
         else:
-            _probation[k] = current_ts
-        # Seed minimal frequency for new items
-        _freq[k] = _freq.get(k, 0)
+            _T1_probation[k] = now
+        _t1_touch_count[k] = 0
 
     _ghost_trim()
 
+
 def update_after_evict(cache_snapshot, obj, evicted_obj):
     '''
-    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
-    - Args:
-        - `cache_snapshot`: A snapshot of the current cache state.
-        - `obj`: The object to be inserted into the cache.
-        - `evicted_obj`: The object that was just evicted from the cache.
-    - Return: `None`
-    '''
-    global m_key_timestamp, _probation, _protected, _freq, _guard_until
-    _ensure_capacity(cache_snapshot)
-    # Remove evicted key from segments and add to appropriate ghost history.
-    if evicted_obj is not None:
-        k = evicted_obj.key
-        was_t1 = k in _probation
-        was_t2 = k in _protected
-        fval = _freq.get(k, 0)
-
-        _probation.pop(k, None)
-        _protected.pop(k, None)
-        m_key_timestamp.pop(k, None)
-        _freq.pop(k, None)
-
-        if was_t1:
-            _B1_ghost[k] = True  # insert as MRU in ghost
-        elif was_t2:
-            _B2_ghost[k] = True
-            # If we had to evict a strong protected item, enable a short guard window.
-            if fval >= 2:
-                _guard_until = cache_snapshot.access_count + max(1, _cap_est // 2)
-        else:
-            # Unknown residency; default to B1
-            _B1_ghost[k] = True
-
-        _ghost_trim()
-    # Do not add obj here; it will be handled in update_after_insert.
-
+    After eviction:
+    - Remove from resident segment.
+    - Add to corresponding ghost list with timestamp.
+    - If evicted strong protected, open a guard window to resist scans.
+    '''
+    _ensure_capacity(cache_snapshot)
+    now = cache_snapshot.access_count
+    if evicted_obj is None:
+        return
+
+    k = evicted_obj.key
+    was_t1 = k in _T1_probation
+    was_t2 = k in _T2_protected
+    fval = _freq.get(k, 0)
+
+    _T1_probation.pop(k, None)
+    _T2_protected.pop(k, None)
+    _t1_touch_count.pop(k, None)
+    m_key_timestamp.pop(k, None)
+    _freq.pop(k, None)
+
+    if was_t1:
+        _B1_ghost[k] = now
+    elif was_t2:
+        _B2_ghost[k] = now
+        # Guard window when evicting strong protected
+        if fval >= 2:
+            globals()['_guard_until'] = now + max(1, _cap_est // 2)
+    else:
+        _B1_ghost[k] = now
+
+    _ghost_trim()
 # EVOLVE-BLOCK-END
 
 # This part remains fixed (not evolved)
 def run_caching(trace_path: str, copy_code_dst: str):
     """Run the caching algorithm on a trace"""
     import os
     with open(os.path.abspath(__file__), 'r', encoding="utf-8") as f:
         code_str = f.read()
     with open(os.path.join(copy_code_dst), 'w') as f:
         f.write(code_str)
     from cache_utils import Cache, CacheConfig, CacheObj, Trace
     trace = Trace(trace_path=trace_path)
     cache_capacity = max(int(trace.get_ndv() * 0.1), 1)
     cache = Cache(CacheConfig(cache_capacity))
     for entry in trace.entries:
         obj = CacheObj(key=str(entry.key))
         cache.get(obj)
     with open(copy_code_dst, 'w') as f:
         f.write("")
     hit_rate = round(cache.hit_count / cache.access_count, 6)
     return hit_rate