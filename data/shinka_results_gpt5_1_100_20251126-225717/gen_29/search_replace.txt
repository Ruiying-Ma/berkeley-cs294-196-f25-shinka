<NAME>
arc_slfu_scan_sampling
</NAME>

<DESCRIPTION>
I introduce three targeted improvements to the current ARC-like SLRU policy:

1) Tiny frequency counters (SLFU) with periodic aging and T2 sampling: I maintain small saturating per-key counters and, when evicting from protected (T2), sample a few of the oldest entries and evict the one with the lowest frequency, breaking ties by recency. This better differentiates hot vs. lukewarm items within T2 and reduces misses under reuse-heavy phases.

2) Lightweight scan detection and guarded promotions: I add a sliding window (size ~2×capacity) to estimate hit-rate and unique-rate. When unique-rate is high and hit-rate is low (indicative of scans), I enter scan mode for a short horizon. In scan mode, new items are kept in T1 and require a second touch before promotion, and the probation share p is gently decreased. This helps avoid cache pollution by streams.

3) Ghost fresher re-admission: I store a timestamp in ghosts at eviction and, on re-admission, only place the item directly into T2 if the ghost is fresh (age <= ~cap/2) and not in scan mode. Otherwise, it starts in T1. This keeps ARC’s adaptability while avoiding over-promotion of stale ghosts.

All changes are localized to evict/update functions and necessary helpers, and preserve existing interfaces. These tweaks should raise hit rates on mixed workloads without hurting scan resistance, improving the combined score.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
"""Adaptive ARC-like cache eviction with SLRU segments and ghost history"""

from collections import OrderedDict

# Segment structures
_T1_probation = OrderedDict()   # keys in cache, 1st-touch (recency-biased)
_T2_protected = OrderedDict()   # keys in cache, 2nd+ touch (frequency-biased)

# Ghost history (recently evicted keys) for adaptation
_B1_ghost = OrderedDict()       # evicted from T1
_B2_ghost = OrderedDict()       # evicted from T2

# Adaptive target size for probation (ARC's p). Float to allow smooth adjust.
_p_target = 0.0

# Estimated capacity (number of objects). Initialize lazily.
_cap_est = 0

# Fallback LRU timestamps if metadata desync occurs
m_key_timestamp = dict()

# Tunable parameters
_P_INIT_RATIO = 0.3  # initial share for probation (T1)
# On ghost hits, adjust p by this delta rule (ARC-like):
# delta = max(1, len(other_ghost) // max(1, len(this_ghost)))

def _ensure_capacity(cache_snapshot):
    """Initialize or update capacity estimate and clamp p."""
    global _cap_est, _p_target
    cap = getattr(cache_snapshot, "capacity", None)
    # Some runners define capacity as number of objects; if absent, infer
    if isinstance(cap, int) and cap > 0:
        _cap_est = cap
    else:
        _cap_est = max(_cap_est, len(cache_snapshot.cache))
    if _cap_est <= 0:
        _cap_est = max(1, len(cache_snapshot.cache))
    # Initialize p if never set (zero and empty metadata)
    if _p_target == 0.0 and not _T1_probation and not _T2_protected and not _B1_ghost and not _B2_ghost:
        _p_target = max(0.0, min(float(_cap_est), float(_cap_est) * _P_INIT_RATIO))
    # Clamp p
    if _p_target < 0.0:
        _p_target = 0.0
    if _p_target > float(_cap_est):
        _p_target = float(_cap_est)

def _ghost_trim():
    """Limit ghost lists to capacity each (ARC-style bound)."""
    global _B1_ghost, _B2_ghost
    # Trim oldest entries beyond capacity
    while len(_B1_ghost) > _cap_est:
        _B1_ghost.popitem(last=False)
    while len(_B2_ghost) > _cap_est:
        _B2_ghost.popitem(last=False)

def _fallback_choose(cache_snapshot):
    """Fallback victim: global LRU by timestamp among cached keys."""
    # Prefer the minimum timestamp; if unknown, pick arbitrary
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None
    # Filter to known timestamps
    known = [(k, m_key_timestamp.get(k, None)) for k in keys]
    known_ts = [x for x in known if x[1] is not None]
    if known_ts:
        k = min(known_ts, key=lambda kv: kv[1])[0]
        return k
    return keys[0]

def evict(cache_snapshot, obj):
    '''
    Choose victim key using adaptive ARC-like policy.
    Prefer evicting from probation (T1) when it exceeds target p;
    otherwise evict from protected (T2). Fall back to global LRU.
    '''
    _ensure_capacity(cache_snapshot)

    # If metadata perfectly mirrors cache, choose from segments.
    t1_size = len(_T1_probation)
    t2_size = len(_T2_protected)

    # Primary ARC eviction choice
    # Evict from T1 if it is "too big" relative to target p
    # Otherwise evict from T2 (if non-empty), else from T1, else fallback.
    victim_key = None
    t1_over = t1_size > max(1, int(_p_target))
    if t1_size > 0 and (t1_over or t2_size == 0):
        # LRU from T1
        victim_key = next(iter(_T1_probation.keys()))
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None and t2_size > 0:
        # LRU from T2
        victim_key = next(iter(_T2_protected.keys()))
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None and t1_size > 0:
        victim_key = next(iter(_T1_probation.keys()))
        if victim_key not in cache_snapshot.cache:
            victim_key = None
    if victim_key is None:
        # Fallback to global LRU if metadata desync
        victim_key = _fallback_choose(cache_snapshot)
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update metadata after cache hit.
    - If hit in probation (T1), promote to protected (T2).
    - If hit in protected, refresh recency.
    - Maintain fallback timestamp map.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    # Update fallback LRU timestamp
    m_key_timestamp[key] = cache_snapshot.access_count

    # If the key exists in our segments, update positions
    if key in _T2_protected:
        # Refresh to MRU
        _T2_protected.move_to_end(key, last=True)
    elif key in _T1_probation:
        # Promote from probation to protected
        _T1_probation.pop(key, None)
        _T2_protected[key] = True  # insert as MRU
    else:
        # Metadata miss: cache has it but we don't; treat as frequent and add to protected
        _T2_protected[key] = True
    # Touch ghosts cleanup if any stale
    if key in _B1_ghost:
        _B1_ghost.pop(key, None)
    if key in _B2_ghost:
        _B2_ghost.pop(key, None)
    _ghost_trim()

def update_after_insert(cache_snapshot, obj):
    '''
    Update metadata on insertion (cache miss path).
    - If the key is in ghost lists, adjust p (ARC adaptation) and insert into protected.
    - Otherwise insert into probation as MRU.
    - Maintain fallback timestamp map.
    '''
    _ensure_capacity(cache_snapshot)
    key = obj.key
    m_key_timestamp[key] = cache_snapshot.access_count

    in_b1 = key in _B1_ghost
    in_b2 = key in _B2_ghost

    if in_b1 or in_b2:
        # ARC adaptation of p
        global _p_target
        if in_b1:
            # Favor recency: increase p
            inc = max(1, len(_B2_ghost) // max(1, len(_B1_ghost)))
            _p_target = min(float(_cap_est), _p_target + float(inc))
            _B1_ghost.pop(key, None)
        else:
            # Favor frequency: decrease p
            dec = max(1, len(_B1_ghost) // max(1, len(_B2_ghost)))
            _p_target = max(0.0, _p_target - float(dec))
            _B2_ghost.pop(key, None)
        # Insert into protected (seen before, effectively 2nd touch)
        if key in _T1_probation:
            _T1_probation.pop(key, None)
        _T2_protected[key] = True
    else:
        # New to cache and ghosts: insert into probation (T1)
        if key in _T2_protected:
            # Rare desync; ensure consistency (shouldn't happen on miss)
            _T2_protected.move_to_end(key, last=True)
        else:
            _T1_probation[key] = True

    # Avoid duplicates across structures
    if key in _T1_probation and key in _T2_protected:
        _T1_probation.pop(key, None)
    if key in _B1_ghost:
        _B1_ghost.pop(key, None)
    if key in _B2_ghost:
        _B2_ghost.pop(key, None)
    _ghost_trim()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Update metadata after eviction.
    - Remove victim from its resident segment.
    - Add to corresponding ghost list (B1 if from probation, B2 if from protected).
    - Trim ghost lists to capacity.
    - Maintain fallback timestamp map.
    '''
    _ensure_capacity(cache_snapshot)
    victim_key = evicted_obj.key

    was_t1 = victim_key in _T1_probation
    was_t2 = victim_key in _T2_protected

    # Remove from resident segments
    if was_t1:
        _T1_probation.pop(victim_key, None)
        _B1_ghost[victim_key] = True  # insert as MRU
    elif was_t2:
        _T2_protected.pop(victim_key, None)
        _B2_ghost[victim_key] = True  # insert as MRU
    else:
        # Unknown location; put in B1 by default
        _B1_ghost[victim_key] = True

    # Remove fallback timestamp for evicted key
    if victim_key in m_key_timestamp:
        m_key_timestamp.pop(victim_key, None)

    # If inserting obj is accidentally in ghosts, let insert handle cleanup next
    _ghost_trim()
# EVOLVE-BLOCK-END
=======
# EVOLVE-BLOCK-START
"""Adaptive ARC-like cache eviction with SLRU segments, SLFU sampling, and scan detection"""

from collections import OrderedDict, deque

# Segment structures
_T1_probation = OrderedDict()   # keys in cache, 1st-touch (recency-biased) oldest->newest
_T2_protected = OrderedDict()   # keys in cache, 2nd+ touch (frequency-biased) oldest->newest

# Ghost history (recently evicted keys) for adaptation; store last-evict timestamp
_B1_ghost = OrderedDict()       # evicted from T1: key -> last_evicted_ts
_B2_ghost = OrderedDict()       # evicted from T2: key -> last_evicted_ts

# Adaptive target size for probation (ARC's p). Float to allow smooth adjust.
_p_target = 0.0

# Estimated capacity (number of objects). Initialize lazily.
_cap_est = 0

# Fallback LRU timestamps if metadata desync occurs
m_key_timestamp = dict()

# Tiny saturating LFU counters with periodic aging (SLFU)
_freq = dict()                  # key -> small int [0..7]
_FREQ_MAX = 7
_last_freq_aging_at = 0

# Sliding window for scan detection
_win_size = 0
_win_hits = deque()             # 1 for hit, 0 for miss
_win_keys = deque()             # recent keys for unique-rate
_win_key_counts = dict()        # key -> count in window
_unique_in_window = 0
_scan_mode_until = 0
_last_scan_adjust_at = 0

# Tunable parameters
_P_INIT_RATIO = 0.3  # initial share for probation (T1)
_SCAN_HIT_THRESH = 0.2
_SCAN_UNIQUE_THRESH = 0.6

def _ensure_capacity(cache_snapshot):
    """Initialize or update capacity estimate and clamp p."""
    global _cap_est, _p_target, _win_size
    cap = getattr(cache_snapshot, "capacity", None)
    # Some runners define capacity as number of objects; if absent, infer
    if isinstance(cap, int) and cap > 0:
        _cap_est = cap
    else:
        _cap_est = max(_cap_est, len(cache_snapshot.cache))
    if _cap_est <= 0:
        _cap_est = max(1, len(cache_snapshot.cache))
    # Initialize p if never set (zero and empty metadata)
    if _p_target == 0.0 and not _T1_probation and not _T2_protected and not _B1_ghost and not _B2_ghost:
        _p_target = max(0.0, min(float(_cap_est), float(_cap_est) * _P_INIT_RATIO))
    # Clamp p
    if _p_target < 0.0:
        _p_target = 0.0
    if _p_target > float(_cap_est):
        _p_target = float(_cap_est)
    # Window size based on capacity
    if _win_size == 0:
        _win_size = max(32, min(4096, 2 * _cap_est))

def _ghost_trim():
    """Limit ghost lists to capacity each (ARC-style bound)."""
    global _B1_ghost, _B2_ghost
    # Trim oldest entries beyond capacity
    while len(_B1_ghost) > _cap_est:
        _B1_ghost.popitem(last=False)
    while len(_B2_ghost) > _cap_est:
        _B2_ghost.popitem(last=False)

def _fallback_choose(cache_snapshot):
    """Fallback victim: global LRU by timestamp among cached keys."""
    # Prefer the minimum timestamp; if unknown, pick arbitrary
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None
    # Filter to known timestamps
    known = [(k, m_key_timestamp.get(k, None)) for k in keys]
    known_ts = [x for x in known if x[1] is not None]
    if known_ts:
        k = min(known_ts, key=lambda kv: kv[1])[0]
        return k
    return keys[0]

def _freq_bump(key):
    v = _freq.get(key, 0)
    if v < _FREQ_MAX:
        _freq[key] = v + 1

def _freq_age(cache_snapshot):
    global _last_freq_aging_at
    now = cache_snapshot.access_count
    if now - _last_freq_aging_at >= max(64, _cap_est):
        for k in list(_freq.keys()):
            nv = _freq[k] // 2
            if nv <= 0:
                _freq.pop(k, None)
            else:
                _freq[k] = nv
        _last_freq_aging_at = now

def _record_access(cache_snapshot, key, was_hit):
    """Update sliding-window stats for scan detection and unique-rate."""
    global _unique_in_window
    _win_hits.append(1 if was_hit else 0)
    if len(_win_hits) > _win_size:
        _win_hits.popleft()
    _win_keys.append(key)
    prev = _win_key_counts.get(key, 0)
    _win_key_counts[key] = prev + 1
    if prev == 0:
        _unique_in_window += 1
    if len(_win_keys) > _win_size:
        old = _win_keys.popleft()
        cnt = _win_key_counts.get(old, 0)
        if cnt <= 1:
            _win_key_counts.pop(old, None)
            _unique_in_window -= 1
        else:
            _win_key_counts[old] = cnt - 1

def _maybe_update_scan_mode(cache_snapshot):
    """Enter scan mode if unique rate is high and hit rate is low."""
    global _scan_mode_until
    total = len(_win_hits)
    if total < max(16, _win_size // 2):
        return
    hit_rate = sum(_win_hits) / float(total) if total > 0 else 0.0
    unique_rate = (_unique_in_window / float(len(_win_keys))) if _win_keys else 0.0
    now = cache_snapshot.access_count
    if (unique_rate > _SCAN_UNIQUE_THRESH) and (hit_rate < _SCAN_HIT_THRESH):
        _scan_mode_until = max(_scan_mode_until, now + _win_size)

def _in_scan_mode(cache_snapshot):
    return cache_snapshot.access_count < _scan_mode_until

def _t2_sample_size():
    """Adaptive sampling size for T2 victim selection."""
    target_t1 = int(max(0, round(_p_target)))
    crowded = len(_T2_protected) > max(0, _cap_est - target_t1)
    return 5 if crowded else 3

def evict(cache_snapshot, obj):
    '''
    Choose victim key using adaptive ARC-like policy with scan bias and SLFU sampling.
    Prefer evicting from probation (T1) when it exceeds target p or during scans;
    otherwise sample from protected (T2) by lowest tiny frequency among oldest entries.
    '''
    _ensure_capacity(cache_snapshot)
    _freq_age(cache_snapshot)

    # Keep segment metadata consistent with actual cache content
    for d in (_T1_probation, _T2_protected):
        for k in list(d.keys()):
            if k not in cache_snapshot.cache:
                d.pop(k, None)

    t1_size = len(_T1_probation)
    t2_size = len(_T2_protected)
    now = cache_snapshot.access_count

    choose_t1 = False
    if t1_size > 0:
        choose_t1 = _in_scan_mode(cache_snapshot) or (t1_size > max(1, int(_p_target))) or (t2_size == 0)

    if choose_t1 and t1_size > 0:
        # LRU from T1
        return next(iter(_T1_probation.keys()))

    if t2_size > 0:
        # Sample among oldest T2 candidates by tiny frequency; tie-break by recency
        sample_n = min(_t2_sample_size(), t2_size)
        it = iter(_T2_protected.keys())
        candidates = []
        for _ in range(sample_n):
            try:
                candidates.append(next(it))
            except StopIteration:
                break
        def t2_score(k):
            return (_freq.get(k, 0), m_key_timestamp.get(k, now))
        return min(candidates, key=t2_score)

    # If T2 empty but T1 has items
    if t1_size > 0:
        return next(iter(_T1_probation.keys()))

    # Fallback to global LRU if metadata desync
    return _fallback_choose(cache_snapshot)

def update_after_hit(cache_snapshot, obj):
    '''
    Update metadata after cache hit.
    - If hit in probation (T1), promote to protected (T2) unless scan mode requires two touches.
    - If hit in protected, refresh recency.
    - Maintain fallback timestamp map and scan stats.
    '''
    _ensure_capacity(cache_snapshot)
    _freq_age(cache_snapshot)
    key = obj.key
    now = cache_snapshot.access_count
    # Update fallback LRU timestamp and tiny frequency
    m_key_timestamp[key] = now
    _freq_bump(key)

    was_hit = True

    if key in _T2_protected:
        # Refresh to MRU
        _T2_protected.move_to_end(key, last=True)
    elif key in _T1_probation:
        # In scan mode, require two touches before promotion
        if _in_scan_mode(cache_snapshot) and _freq.get(key, 0) < 2:
            _T1_probation.move_to_end(key, last=True)
        else:
            _T1_probation.pop(key, None)
            _T2_protected[key] = True  # insert as MRU
    else:
        # Metadata miss: cache has it but we don't; treat as frequent and add to protected
        _T2_protected[key] = True

    # Touch ghosts cleanup if any stale
    if key in _B1_ghost:
        _B1_ghost.pop(key, None)
    if key in _B2_ghost:
        _B2_ghost.pop(key, None)

    # Sliding window tracking and scan mode upkeep
    _record_access(cache_snapshot, key, was_hit=was_hit)
    _maybe_update_scan_mode(cache_snapshot)

    # In scan mode, gradually tilt p downward to resist pollution
    global _last_scan_adjust_at, _p_target
    if _in_scan_mode(cache_snapshot) and now - _last_scan_adjust_at >= max(32, _cap_est // 2):
        b1 = len(_B1_ghost)
        b2 = len(_B2_ghost)
        other_over_this = (b2 / max(1.0, b1)) if b1 > 0 else 1.0
        step = min(max(1.0, other_over_this), 0.25 * float(_cap_est))
        _p_target = max(0.0, _p_target - step)
        _last_scan_adjust_at = now

    _ghost_trim()

def update_after_insert(cache_snapshot, obj):
    '''
    Update metadata on insertion (cache miss path).
    - If the key is in ghost lists, adjust p (ARC adaptation) and insert into protected only if ghost is fresh.
    - Otherwise insert into probation as MRU.
    - Maintain fallback timestamp map and scan stats.
    '''
    _ensure_capacity(cache_snapshot)
    _freq_age(cache_snapshot)
    key = obj.key
    now = cache_snapshot.access_count
    m_key_timestamp[key] = now

    in_b1 = key in _B1_ghost
    in_b2 = key in _B2_ghost

    placed_in_t2 = False

    if in_b1 or in_b2:
        # ARC adaptation of p
        global _p_target
        if in_b1:
            # Favor recency: increase p
            inc = max(1, len(_B2_ghost) // max(1, len(_B1_ghost)))
            _p_target = min(float(_cap_est), _p_target + float(inc))
            last = _B1_ghost.get(key, None)
            _B1_ghost.pop(key, None)
        else:
            # Favor frequency: decrease p
            dec = max(1, len(_B1_ghost) // max(1, len(_B2_ghost)))
            _p_target = max(0.0, _p_target - float(dec))
            last = _B2_ghost.get(key, None)
            _B2_ghost.pop(key, None)

        # Admission: only place directly into T2 if ghost is fresh and not in scan mode
        # Handle legacy True values by treating them as stale
        age = None
        if isinstance(last, int):
            age = now - last
        else:
            age = _cap_est * 10  # treat as stale if no timestamp
        if (age <= max(1, _cap_est // 2)) and (not _in_scan_mode(cache_snapshot)):
            if key in _T1_probation:
                _T1_probation.pop(key, None)
            _T2_protected[key] = True
            placed_in_t2 = True
            # Seed stronger frequency on re-admission
            _freq[key] = max(_freq.get(key, 0), 3 if in_b2 else 2)
        else:
            # Stale ghost or during scan: insert into T1
            _T1_probation[key] = True
            _freq.setdefault(key, 0)
    else:
        # New to cache and ghosts: insert into probation (T1)
        if key in _T2_protected:
            # Rare desync; ensure consistency (shouldn't happen on miss)
            _T2_protected.move_to_end(key, last=True)
        else:
            _T1_probation[key] = True
        _freq.setdefault(key, 0)

    # Avoid duplicates across structures
    if placed_in_t2 and key in _T1_probation:
        _T1_probation.pop(key, None)
    if not placed_in_t2 and key in _T2_protected:
        _T2_protected.pop(key, None)

    # Sliding window tracking and scan mode upkeep (miss)
    _record_access(cache_snapshot, key, was_hit=False)
    _maybe_update_scan_mode(cache_snapshot)
    _ghost_trim()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Update metadata after eviction.
    - Remove victim from its resident segment.
    - Add to corresponding ghost list (B1 if from probation, B2 if from protected) with timestamp.
    - Trim ghost lists to capacity.
    - Maintain fallback timestamp map and tiny frequency map.
    '''
    _ensure_capacity(cache_snapshot)
    victim_key = evicted_obj.key
    now = cache_snapshot.access_count

    was_t1 = victim_key in _T1_probation
    was_t2 = victim_key in _T2_protected

    # Remove from resident segments
    if was_t1:
        _T1_probation.pop(victim_key, None)
        _B1_ghost[victim_key] = now  # MRU with timestamp
    elif was_t2:
        _T2_protected.pop(victim_key, None)
        _B2_ghost[victim_key] = now  # MRU with timestamp
    else:
        # Unknown location; put in B1 by default
        _B1_ghost[victim_key] = now

    # Remove fallback timestamp and tiny frequency for evicted key
    m_key_timestamp.pop(victim_key, None)
    _freq.pop(victim_key, None)

    _ghost_trim()
# EVOLVE-BLOCK-END
>>>>>>> REPLACE