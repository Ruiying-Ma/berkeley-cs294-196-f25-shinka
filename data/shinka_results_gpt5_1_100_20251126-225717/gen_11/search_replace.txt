<NAME>
arc_lrfu_hybrid_with_decay
</NAME>

<DESCRIPTION>
I propose upgrading the current ARC-like SLRU policy with a lightweight decayed frequency component and smarter victim selection. The changes:

1) Add a decayed frequency counter (TinyLFU-style but simple): m_freq tracks per-key frequency, decayed every ~2Ã—capacity accesses to keep counts bounded and adaptive to phase changes. Hits increment frequency, inserts seed a small base frequency.

2) Admission seeding via ghosts and previous strength: When a key is reinserted after eviction, it gets a higher initial frequency if it previously belonged to protected or had higher frequency at eviction (stored in m_ghost_strength). This boosts true working-set items and prevents scan pollution.

3) ARC-like eviction source selection combined with LRFU-style victim choice: Instead of always evicting from probation first, choose between probation and protected based on target sizes (ARC logic), then select the victim with the minimum hybrid priority score S = freq - age/Cap (age = now - last_access_ts). This balances recency and frequency, evicting the coldest item in the chosen segment. Fallback selects globally if a segment is empty.

4) Maintain metadata consistency: Clean m_freq for non-resident keys; ghosts are bounded and we trim ghost_strength together.

This hybrid ARC + decayed LFU scoring improves scan resistance and protects frequently reused items, typically reducing miss rates across mixed workloads without heavy data structures or high overhead.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

# Adaptive Segmented LRU with ARC-like ghost feedback.
# Live segments:
#  - Probation (recently inserted, not yet proven)
#  - Protected (proven frequent/reused)
# Ghost segments (no data, just metadata of evicted keys):
#  - m_ghost_probation (recent recency victims)
#  - m_ghost_protected (recent frequency victims)
# Target size of protected adapts based on ghost hits.
m_ts = dict()                  # key -> last access timestamp
m_probation = set()            # probation segment membership
m_protected = set()            # protected segment membership
m_ghost_probation = dict()     # key -> timestamp (ghost of probation)
m_ghost_protected = dict()     # key -> timestamp (ghost of protected)
m_target_protected = None      # target number of protected entries
m_last_capacity = None         # remember capacity to re-init target if it changes


def _init_targets(cache_snapshot):
    global m_target_protected, m_last_capacity
    cap = cache_snapshot.capacity or max(len(cache_snapshot.cache), 1)
    if m_target_protected is None or m_last_capacity != cap:
        # Start balanced
        m_target_protected = max(1, int(cap * 0.5))
        m_last_capacity = cap


def _oldest_key(candidates):
    # Return the key with the smallest timestamp among candidates
    return min(candidates, key=lambda k: m_ts.get(k, -1))


def _trim_ghosts(capacity):
    # Bound ghost lists to capacity (ARC heuristic)
    global m_ghost_probation, m_ghost_protected
    def trim(ghost):
        if len(ghost) <= capacity:
            return
        over = len(ghost) - capacity
        for _ in range(over):
            kmin = min(ghost, key=lambda k: ghost[k])
            ghost.pop(kmin, None)
    trim(m_ghost_probation)
    trim(m_ghost_protected)


def _enforce_protected_quota():
    # Demote LRU from protected to probation until target is met
    global m_probation, m_protected
    while m_target_protected is not None and len(m_protected) > m_target_protected:
        demote_key = _oldest_key(m_protected)
        m_protected.discard(demote_key)
        m_probation.add(demote_key)


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global m_ts, m_probation, m_protected
    _init_targets(cache_snapshot)

    keys_in_cache = set(cache_snapshot.cache.keys())

    # Keep metadata consistent with actual cache content
    if m_probation:
        m_probation.intersection_update(keys_in_cache)
    if m_protected:
        m_protected.intersection_update(keys_in_cache)
    if m_ts:
        for k in list(m_ts.keys()):
            if k not in keys_in_cache:
                m_ts.pop(k, None)
                m_probation.discard(k)
                m_protected.discard(k)

    probation_candidates = m_probation & keys_in_cache
    protected_candidates = m_protected & keys_in_cache

    # Prefer evicting from probationary segment to avoid polluting protected items
    if probation_candidates:
        return _oldest_key(probation_candidates)
    if protected_candidates:
        return _oldest_key(protected_candidates)

    # Fallback: evict the globally oldest if segmentation hasn't been set yet
    if keys_in_cache:
        return _oldest_key(keys_in_cache)
    return None


def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_ts, m_probation, m_protected, m_target_protected
    _init_targets(cache_snapshot)
    now = cache_snapshot.access_count
    key = obj.key

    # Ensure timestamp exists and update recency
    m_ts[key] = now

    if key in m_probation:
        # Promote on first reuse
        m_probation.discard(key)
        m_protected.add(key)
        # Slightly increase protected target on successful promotion (favor frequency)
        cap = m_last_capacity or max(len(cache_snapshot.cache), 1)
        delta = 1  # conservative step to avoid oscillation
        m_target_protected = min(cap, max(1, m_target_protected + delta))
    elif key not in m_protected:
        # If metadata was missing, treat as protected to avoid premature eviction
        m_protected.add(key)

    # Enforce protected quota by demoting its LRU if needed
    _enforce_protected_quota()


def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_ts, m_probation, m_protected, m_ghost_probation, m_ghost_protected, m_target_protected
    _init_targets(cache_snapshot)
    now = cache_snapshot.access_count
    key = obj.key
    cap = m_last_capacity or max(len(cache_snapshot.cache), 1)

    # ARC-like adaptation based on ghost hits:
    # - If miss corresponds to probation ghost, favor recency (shrink protected target)
    # - If miss corresponds to protected ghost, favor frequency (grow protected target)
    step = max(1, cap // 32)
    if key in m_ghost_probation:
        m_target_protected = max(1, m_target_protected - step)
        m_ghost_probation.pop(key, None)
    elif key in m_ghost_protected:
        m_target_protected = min(cap, m_target_protected + step)
        m_ghost_protected.pop(key, None)

    # Insert starts in probation
    m_ts[key] = now
    m_protected.discard(key)
    m_probation.add(key)

    # Respect current target by demoting protected LRU if over target
    _enforce_protected_quota()

    # Keep ghost lists bounded
    _trim_ghosts(cap)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_ts, m_probation, m_protected, m_ghost_probation, m_ghost_protected
    _init_targets(cache_snapshot)
    evk = evicted_obj.key
    now = cache_snapshot.access_count
    cap = m_last_capacity or max(len(cache_snapshot.cache), 1)

    # Determine segment before removal
    was_protected = evk in m_protected
    was_probation = evk in m_probation

    # Remove all metadata for the evicted object
    m_ts.pop(evk, None)
    m_probation.discard(evk)
    m_protected.discard(evk)

    # Record into appropriate ghost list (ARC feedback)
    if was_protected:
        m_ghost_protected[evk] = now
    else:
        # If unknown or probation, treat as probation ghost
        m_ghost_probation[evk] = now

    # Trim ghosts to capacity
    _trim_ghosts(cap)
=======
"""Cache eviction algorithm for optimizing hit rates across multiple workloads"""

# Adaptive Segmented LRU with ARC-like ghost feedback + lightweight decayed LFU.
# Live segments:
#  - Probation (recently inserted, not yet proven)
#  - Protected (proven frequent/reused)
# Ghost segments (no data, just metadata of evicted keys):
#  - m_ghost_probation (recent recency victims)
#  - m_ghost_protected (recent frequency victims)
# Target size of protected adapts based on ghost hits.
# Additionally, a decayed frequency (m_freq) informs scoring at eviction time.

m_ts = dict()                  # key -> last access timestamp
m_probation = set()            # probation segment membership
m_protected = set()            # protected segment membership
m_ghost_probation = dict()     # key -> timestamp (ghost of probation)
m_ghost_protected = dict()     # key -> timestamp (ghost of protected)
m_target_protected = None      # target number of protected entries
m_last_capacity = None         # remember capacity to re-init target if it changes
m_freq = dict()                # key -> decayed frequency score
m_last_decay_access = 0        # last access when decay applied
m_decay_interval = None        # accesses between global frequency decay
m_ghost_strength = dict()      # key -> last known frequency at eviction (for seeding)


def _init_targets(cache_snapshot):
    global m_target_protected, m_last_capacity, m_decay_interval
    cap = cache_snapshot.capacity or max(len(cache_snapshot.cache), 1)
    if m_target_protected is None or m_last_capacity != cap:
        # Start balanced
        m_target_protected = max(1, int(cap * 0.5))
        m_last_capacity = cap
        # Recompute decay interval when capacity changes
        m_decay_interval = max(100, cap * 2)


def _oldest_key(candidates):
    # Return the key with the smallest timestamp among candidates
    return min(candidates, key=lambda k: m_ts.get(k, -1))


def _trim_ghosts(capacity):
    # Bound ghost lists to capacity (ARC heuristic). Also trim stored strengths.
    global m_ghost_probation, m_ghost_protected, m_ghost_strength
    def trim(ghost):
        if len(ghost) <= capacity:
            return
        over = len(ghost) - capacity
        for _ in range(over):
            kmin = min(ghost, key=lambda k: ghost[k])
            ghost.pop(kmin, None)
            m_ghost_strength.pop(kmin, None)
    trim(m_ghost_probation)
    trim(m_ghost_protected)


def _enforce_protected_quota():
    # Demote LRU from protected to probation until target is met
    global m_probation, m_protected
    while m_target_protected is not None and len(m_protected) > m_target_protected:
        demote_key = _oldest_key(m_protected)
        m_protected.discard(demote_key)
        m_probation.add(demote_key)


def _maybe_decay(cache_snapshot):
    # Apply exponential decay to frequency counters periodically to adapt to phase changes
    global m_last_decay_access, m_decay_interval, m_freq
    now = cache_snapshot.access_count
    if m_decay_interval is None:
        return
    if now - m_last_decay_access >= m_decay_interval:
        for k in list(m_freq.keys()):
            m_freq[k] *= 0.5
            if m_freq[k] < 1e-3:
                m_freq.pop(k, None)
        m_last_decay_access = now


def _priority(key, now, cap):
    # Higher is better to keep; eviction chooses minimum.
    # LRFU-like: freq - age / cap, where age = now - last access.
    age = now - m_ts.get(key, now)
    freq = m_freq.get(key, 0.0)
    lam = 1.0 / max(1, cap)
    return freq - lam * age


def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    global m_ts, m_probation, m_protected, m_freq
    _init_targets(cache_snapshot)

    keys_in_cache = set(cache_snapshot.cache.keys())

    # Keep metadata consistent with actual cache content
    if m_probation:
        m_probation.intersection_update(keys_in_cache)
    if m_protected:
        m_protected.intersection_update(keys_in_cache)
    if m_ts:
        for k in list(m_ts.keys()):
            if k not in keys_in_cache:
                m_ts.pop(k, None)
                m_probation.discard(k)
                m_protected.discard(k)
    if m_freq:
        for k in list(m_freq.keys()):
            if k not in keys_in_cache:
                m_freq.pop(k, None)

    probation_candidates = m_probation & keys_in_cache
    protected_candidates = m_protected & keys_in_cache

    # ARC-like choice of source segment based on target sizes
    cap = m_last_capacity or max(len(cache_snapshot.cache), 1)
    now = cache_snapshot.access_count
    target_probation = max(0, cap - (m_target_protected or 0))
    choose_probation = bool(probation_candidates) and (len(probation_candidates) > target_probation or not protected_candidates)

    if choose_probation and probation_candidates:
        # Evict the coldest in probation by hybrid score
        return min(probation_candidates, key=lambda k: (_priority(k, now, cap), m_ts.get(k, -1)))
    if protected_candidates:
        # Evict the coldest in protected if probation is within target
        return min(protected_candidates, key=lambda k: (_priority(k, now, cap), m_ts.get(k, -1)))

    # Fallback: evict the globally coldest if segmentation hasn't been set yet
    if keys_in_cache:
        return min(keys_in_cache, key=lambda k: (_priority(k, now, cap), m_ts.get(k, -1)))
    return None


def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_ts, m_probation, m_protected, m_target_protected, m_freq
    _init_targets(cache_snapshot)
    _maybe_decay(cache_snapshot)
    now = cache_snapshot.access_count
    key = obj.key

    # Ensure timestamp exists and update recency
    m_ts[key] = now
    # Increase decayed frequency on hit
    m_freq[key] = m_freq.get(key, 0.0) + 1.0

    if key in m_probation:
        # Promote on first reuse
        m_probation.discard(key)
        m_protected.add(key)
        # Slightly increase protected target on successful promotion (favor frequency)
        cap = m_last_capacity or max(len(cache_snapshot.cache), 1)
        delta = 1  # conservative step to avoid oscillation
        m_target_protected = min(cap, max(1, m_target_protected + delta))
    elif key not in m_protected:
        # If metadata was missing, treat as protected to avoid premature eviction
        m_protected.add(key)

    # Enforce protected quota by demoting its LRU if needed
    _enforce_protected_quota()


def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_ts, m_probation, m_protected, m_ghost_probation, m_ghost_protected, m_target_protected, m_freq, m_ghost_strength
    _init_targets(cache_snapshot)
    _maybe_decay(cache_snapshot)
    now = cache_snapshot.access_count
    key = obj.key
    cap = m_last_capacity or max(len(cache_snapshot.cache), 1)

    # Capture ghost presence/strength before we mutate
    was_ghost_prob = key in m_ghost_probation
    was_ghost_prot = key in m_ghost_protected
    prev_strength = m_ghost_strength.pop(key, 0.0)

    # ARC-like adaptation based on ghost hits:
    # - If miss corresponds to probation ghost, favor recency (shrink protected target)
    # - If miss corresponds to protected ghost, favor frequency (grow protected target)
    step = max(1, cap // 32)
    if was_ghost_prob:
        m_target_protected = max(1, m_target_protected - step)
        m_ghost_probation.pop(key, None)
    elif was_ghost_prot:
        m_target_protected = min(cap, m_target_protected + step)
        m_ghost_protected.pop(key, None)

    # Insert starts in probation
    m_ts[key] = now
    m_protected.discard(key)
    m_probation.add(key)

    # Seed a small initial frequency; boost if recent ghost or remembered strength
    base = 0.1
    if was_ghost_prot:
        base = max(base, 1.5)
    elif was_ghost_prob:
        base = max(base, 0.6)
    if prev_strength > 0:
        base = max(base, prev_strength * 0.7)
    m_freq[key] = base

    # Respect current target by demoting protected LRU if over target
    _enforce_protected_quota()

    # Keep ghost lists bounded
    _trim_ghosts(cap)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_ts, m_probation, m_protected, m_ghost_probation, m_ghost_protected, m_freq, m_ghost_strength
    _init_targets(cache_snapshot)
    _maybe_decay(cache_snapshot)
    evk = evicted_obj.key
    now = cache_snapshot.access_count
    cap = m_last_capacity or max(len(cache_snapshot.cache), 1)

    # Determine segment before removal
    was_protected = evk in m_protected
    was_probation = evk in m_probation

    # Capture and remove all metadata for the evicted object
    last_strength = m_freq.pop(evk, 0.0)
    m_ts.pop(evk, None)
    m_probation.discard(evk)
    m_protected.discard(evk)

    # Record into appropriate ghost list (ARC feedback) and remember strength
    if was_protected:
        m_ghost_protected[evk] = now
    else:
        # If unknown or probation, treat as probation ghost
        m_ghost_probation[evk] = now
    if last_strength > 0:
        m_ghost_strength[evk] = last_strength

    # Trim ghosts to capacity
    _trim_ghosts(cap)
>>>>>>> REPLACE

</DIFF>